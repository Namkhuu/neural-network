{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "658f3459",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Model \n",
    "from keras.layers import Input, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6190b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def system_dynamics(x_t, u_t):\n",
    "    \"\"\"\n",
    "    System dynamics: x_t+1 = x_t + 0.5*u_t\n",
    "    \"\"\"\n",
    "    return x_t + 0.5 * u_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6c31fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10, Total Reward: 60293.19\n",
      "Episode 20, Total Reward: 11.37\n",
      "Episode 30, Total Reward: 10.84\n",
      "Episode 40, Total Reward: 41.27\n",
      "Episode 50, Total Reward: 0.13\n",
      "Episode 60, Total Reward: 5.55\n",
      "Episode 70, Total Reward: 30.37\n",
      "Episode 80, Total Reward: 10.83\n",
      "Episode 90, Total Reward: 1003231379456.00\n",
      "Episode 100, Total Reward: nan\n",
      "Episode 110, Total Reward: nan\n",
      "Episode 120, Total Reward: nan\n"
     ]
    }
   ],
   "source": [
    "# --- Minimal Actor-Critic for Linear System ---\n",
    "\n",
    "# 1. Define small actor and critic networks\n",
    "actor_in = Input(shape=(1,))\n",
    "actor_h = Dense(32, activation='relu')(actor_in)\n",
    "actor_out = Dense(1, activation='linear')(actor_h)\n",
    "actor = Model(actor_in, actor_out)\n",
    "actor_optimizer = tf.keras.optimizers.Adam(1e-3)\n",
    "\n",
    "critic_in = Input(shape=(2,))  # [x, u]\n",
    "critic_h = Dense(32, activation='relu')(critic_in)\n",
    "critic_out = Dense(1, activation='linear')(critic_h)\n",
    "critic = Model(critic_in, critic_out)\n",
    "critic_optimizer = tf.keras.optimizers.Adam(1e-3)\n",
    "\n",
    "# 2. Training loop\n",
    "n_episodes = 200\n",
    "n_steps = 20\n",
    "gamma = 1.0\n",
    "\n",
    "history = []\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    x = np.random.uniform(-5, 5)  # random initial state\n",
    "    episode_reward = 0\n",
    "    for t in range(n_steps):\n",
    "        x_tensor = tf.convert_to_tensor([[x]], dtype=tf.float32)\n",
    "        with tf.GradientTape() as tape_actor, tf.GradientTape() as tape_critic:\n",
    "            u = actor(x_tensor)\n",
    "            u_np = u.numpy()[0, 0]\n",
    "            x_next = system_dynamics(x, u_np)\n",
    "            r = x**2 + 0.001 * u_np**2\n",
    "\n",
    "            # Critic Q(s,a)\n",
    "            q_pred = critic(tf.convert_to_tensor([[x, u_np]], dtype=tf.float32))\n",
    "            # Target: r + gamma * Q(s', a')\n",
    "            u_next = actor(tf.convert_to_tensor([[x_next]], dtype=tf.float32))\n",
    "            q_next = critic(tf.convert_to_tensor([[x_next, u_next.numpy()[0,0]]], dtype=tf.float32))\n",
    "            q_target = r + q_next\n",
    "\n",
    "            # Critic loss (MSE)\n",
    "            critic_loss = tf.reduce_mean((q_pred - q_target)**2)\n",
    "\n",
    "        # Critic update\n",
    "        critic_grads = tape_critic.gradient(critic_loss, critic.trainable_variables)\n",
    "        critic_optimizer.apply_gradients(zip(critic_grads, critic.trainable_variables))\n",
    "\n",
    "        # Actor loss: maximize Q, so minimize -Q\n",
    "        with tf.GradientTape() as tape_actor:\n",
    "            u = actor(x_tensor)\n",
    "            q_val = critic(tf.concat([x_tensor, u], axis=1))\n",
    "            actor_loss = -tf.reduce_mean(q_val)\n",
    "        actor_grads = tape_actor.gradient(actor_loss, actor.trainable_variables)\n",
    "        actor_optimizer.apply_gradients(zip(actor_grads, actor.trainable_variables))\n",
    "\n",
    "        x = x_next\n",
    "        episode_reward += r\n",
    "\n",
    "    history.append(episode_reward)\n",
    "    if (episode+1) % 10 == 0:\n",
    "        print(f\"Episode {episode+1}, Total Reward: {episode_reward:.2f}\")\n",
    "\n",
    "# 3. Plot learning curve\n",
    "plt.plot(history)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Cost')\n",
    "plt.title('Actor-Critic Learning Curve')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# 4. Visualize learned policy vs analytical\n",
    "x_test = np.linspace(-5, 5, 100).reshape(-1, 1)\n",
    "u_actor = actor.predict(x_test)\n",
    "u_analytical = -2 * x_test\n",
    "plt.plot(x_test, u_actor, label='Learned Policy (Actor NN)')\n",
    "plt.plot(x_test, u_analytical, '--', label='Analytical Policy')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('u')\n",
    "plt.legend()\n",
    "plt.title('Learned Policy vs Analytical')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neural_network",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
