{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b8ceea56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Lambda\n",
    "from keras.layers import Concatenate\n",
    "from collections import deque\n",
    "import random\n",
    "from quanser.hardware import HIL \n",
    "from pal.products.qube import QubeServo3\n",
    "from pal.utilities.math import SignalGenerator, ddt_filter\n",
    "from threading import Thread\n",
    "import time\n",
    "import signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8674aa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # board.close()\n",
    "# # Open connection to QUBE\n",
    "# # board = HIL(\"qube_servo3_usb\", \"0\")\n",
    "\n",
    "# encoder_channels = np.array([0, 1], dtype=np.uint32)\n",
    "# motor_channels = np.array([0], dtype=np.uint32)\n",
    "# counts = np.zeros(2, dtype=np.int32)\n",
    "\n",
    "# ENCODER_RES = 2048\n",
    "# ARM_RAD_PER_COUNT = 2*np.pi / ENCODER_RES\n",
    "# PEND_RAD_PER_COUNT = 2*np.pi / ENCODER_RES\n",
    "\n",
    "# dt = 0.01  # 10 ms\n",
    "# theta_arm_prev  = counts[0] * ARM_RAD_PER_COUNT\n",
    "# theta_pend_prev = counts[1] * PEND_RAD_PER_COUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "49e7c226",
   "metadata": {},
   "outputs": [],
   "source": [
    "global KILL_THREAD\n",
    "KILL_THREAD = False\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=100000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    def store(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = map(np.array, zip(*batch))\n",
    "        return (np.array(states, dtype=np.float32),\n",
    "            np.array(actions, dtype=np.float32),\n",
    "            np.array(rewards, dtype=np.float32),\n",
    "            np.array(next_states, dtype=np.float32),\n",
    "            np.array(dones, dtype=np.float32))\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "def soft_update(target_weights, online_weights, tau=0.005):\n",
    "    for (target, online) in zip(target_weights, online_weights):\n",
    "        target.assign(target * (1 - tau) + online * tau) \n",
    "\n",
    "def sig_handler(*args): \n",
    "    global KILL_THREAD\n",
    "    KILL_THREAD = True\n",
    "\n",
    "signal.signal(signal.SIGINT, sig_handler)\n",
    "\n",
    "replay_buffer = ReplayBuffer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f8509c67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x1b6491fcb50>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_size = 4\n",
    "action_size = 1\n",
    "gamma = 0.99 # discount rate\n",
    "learning_rate = 0.001 # learning rate\n",
    "\n",
    "# Define the actor model\n",
    "states_inputs = Input(shape=(state_size,))\n",
    "dense = Dense(400, activation='relu')(states_inputs)\n",
    "dense = Dense(300, activation='relu')(dense)\n",
    "outputs = Dense(action_size, activation='tanh')(dense)\n",
    "outputs = keras.layers.Lambda(lambda x: x * 2.0)(outputs)  # Scale action to [-2, 2]\n",
    "actor_model = Model(inputs=states_inputs, outputs=outputs)\n",
    "\n",
    "# Critic 1\n",
    "state_input1 = Input(shape=(state_size,))\n",
    "action_input1 = Input(shape=(action_size,))\n",
    "concat1 = Concatenate()([state_input1, action_input1])\n",
    "dense1 = Dense(128, activation='relu')(concat1)\n",
    "dense1 = Dense(128, activation='relu')(dense1)\n",
    "output1 = Dense(1)(dense1)\n",
    "critic_model1 = Model([state_input1, action_input1], output1)\n",
    "\n",
    "# Critic 2\n",
    "state_input2 = Input(shape=(state_size,))\n",
    "action_input2 = Input(shape=(action_size,))\n",
    "concat2 = Concatenate()([state_input2, action_input2])\n",
    "dense2 = Dense(128, activation='relu')(concat2)\n",
    "dense2 = Dense(128, activation='relu')(dense2)\n",
    "output2 = Dense(1)(dense2)\n",
    "critic_model2 = Model([state_input2, action_input2], output2)\n",
    "\n",
    "try:\n",
    "    actor_model.load_weights('saves/quanser/actor_model.weights.h5')\n",
    "    critic_model1.load_weights('saves/quanser/critic_model1.weights.h5')\n",
    "    critic_model2.load_weights('saves/quanser/critic_model2.weights.h5')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "actor_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "critic_optimizer1 = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "critic_optimizer2 = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "target_actor = keras.models.clone_model(actor_model)\n",
    "target_actor.set_weights(actor_model.get_weights())\n",
    "\n",
    "target_critic1 = keras.models.clone_model(critic_model1)\n",
    "target_critic1.set_weights(critic_model1.get_weights())\n",
    "target_critic2 = keras.models.clone_model(critic_model2)\n",
    "target_critic2.set_weights(critic_model2.get_weights())\n",
    "\n",
    "ckpt = tf.train.Checkpoint(actor_optimizer=actor_optimizer,\n",
    "                           critic_optimizer1=critic_optimizer1, \n",
    "                           critic_optimizer2=critic_optimizer2)\n",
    "\n",
    "# Restore the latest checkpoint with optimizer states\n",
    "ckpt.restore(tf.train.latest_checkpoint(\"saves/quanser/optimizers_ckpt\")).expect_partial()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "844cf3fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta: 0.000, Theta dot: 0.000, Alpha: -1.000, Alpha dot: 0.000 Reward: -1.000\n",
      "Theta: 0.000, Theta dot: 0.000, Alpha: -1.000, Alpha dot: 0.000 Reward: -1.000\n",
      "Theta: 0.000, Theta dot: 0.000, Alpha: -1.000, Alpha dot: 0.000 Reward: -1.000\n",
      "Theta: 0.000, Theta dot: 0.000, Alpha: -1.000, Alpha dot: 0.000 Reward: -1.000\n",
      "Theta: 0.000, Theta dot: 0.000, Alpha: -1.000, Alpha dot: 0.000 Reward: -1.000\n",
      "Theta: 0.000, Theta dot: 0.000, Alpha: -1.000, Alpha dot: 0.000 Reward: -1.000\n",
      "Theta: 0.273, Theta dot: 13.002, Alpha: -0.994, Alpha dot: 10.320 Reward: -0.998\n",
      "Theta: 0.169, Theta dot: 6.797, Alpha: 0.509, Alpha dot: 189.453 Reward: -3.848\n",
      "Theta: 0.202, Theta dot: 7.757, Alpha: 0.998, Alpha dot: 243.699 Reward: -6.935\n",
      "Theta: 0.267, Theta dot: 10.086, Alpha: 0.996, Alpha dot: 213.056 Reward: -5.531\n",
      "Theta: 0.328, Theta dot: 12.047, Alpha: 0.997, Alpha dot: 173.482 Reward: -4.003\n",
      "Theta: 0.307, Theta dot: 9.877, Alpha: 0.938, Alpha dot: 102.614 Reward: -1.934\n",
      "Theta: 0.239, Theta dot: 5.722, Alpha: -0.990, Alpha dot: -157.017 Reward: -3.447\n",
      "Theta: 0.117, Theta dot: -0.666, Alpha: -0.949, Alpha dot: -111.734 Reward: -2.148\n",
      "Theta: 0.383, Theta dot: 12.107, Alpha: -0.840, Alpha dot: -68.549 Reward: -1.175\n",
      "Theta: 0.307, Theta dot: 7.302, Alpha: -0.862, Alpha dot: -59.990 Reward: -1.104\n",
      "Theta: 0.095, Theta dot: -3.474, Alpha: -0.991, Alpha dot: -109.605 Reward: -2.183\n",
      "Theta: 0.175, Theta dot: 0.655, Alpha: -0.935, Alpha dot: -44.494 Reward: -1.073\n",
      "Theta: 0.267, Theta dot: 4.976, Alpha: -0.855, Alpha dot: -19.670 Reward: -0.769\n",
      "Theta: 0.261, Theta dot: 4.210, Alpha: 1.000, Alpha dot: 217.071 Reward: -5.711\n",
      "Theta: 0.245, Theta dot: 3.078, Alpha: 1.000, Alpha dot: 180.951 Reward: -4.274\n",
      "Theta: 0.245, Theta dot: 2.785, Alpha: 1.000, Alpha dot: 147.214 Reward: -3.167\n",
      "Theta: 0.285, Theta dot: 4.419, Alpha: 0.997, Alpha dot: 113.754 Reward: -2.288\n",
      "Theta: 0.276, Theta dot: 3.560, Alpha: 0.998, Alpha dot: 94.466 Reward: -1.889\n",
      "Theta: 0.295, Theta dot: 4.097, Alpha: 0.997, Alpha dot: 75.617 Reward: -1.565\n",
      "Theta: 1.150, Theta dot: 44.467, Alpha: 0.357, Alpha dot: -392.469 Reward: -15.531\n",
      "Theta: 0.948, Theta dot: 30.590, Alpha: -0.985, Alpha dot: -160.740 Reward: -3.555\n",
      "Theta: -0.141, Theta dot: -24.186, Alpha: -0.488, Alpha dot: -19.395 Reward: -0.275\n",
      "Theta: -0.451, Theta dot: -36.638, Alpha: -0.504, Alpha dot: -17.542 Reward: -0.284\n",
      "Theta: -0.325, Theta dot: -27.159, Alpha: -0.703, Alpha dot: -37.223 Reward: -0.632\n",
      "Theta: -0.040, Theta dot: -10.986, Alpha: -0.908, Alpha dot: -63.087 Reward: -1.222\n",
      "Theta: 0.288, Theta dot: 5.692, Alpha: -0.984, Alpha dot: -107.398 Reward: -2.121\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m(alpha\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.0001\u001b[39m\u001b[38;5;241m*\u001b[39malpha_dot\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m )\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTheta: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtheta\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Theta dot: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtheta_dot\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Alpha: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00malpha\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Alpha dot: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00malpha_dot\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m     21\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreward\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 22\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "frequency = 500  # Hz\n",
    "state_theta_dot = np.array([0,0], dtype=np.float64)\n",
    "state_alpha_dot = np.array([0,0], dtype=np.float64)\n",
    "with QubeServo3(hardware = 1, pendulum = 1, frequency=frequency) as board:\n",
    "    while True:\n",
    "        # Have to initialize the board first before reading motorPosition or it won't read\n",
    "        board.read_outputs()\n",
    "        theta = board.motorPosition \n",
    "        alpha = -board.pendulumPosition \n",
    "        theta = np.clip(theta, -np.pi/2, np.pi/2)\n",
    "        theta_dot, state_theta_dot = ddt_filter(theta, state_theta_dot, 50, 1/frequency)\n",
    "        # u - input\n",
    "        # state - previous state returned by this function -- initialize to np.array([0,0], dtype=np.float64)\n",
    "        # Ts - sample time in seconds\n",
    "        # A - filter bandwidth in rad/s\n",
    "        alpha_dot, state_alpha_dot = ddt_filter(alpha, state_alpha_dot, 100, 1/frequency)\n",
    "        alpha = np.mod(alpha, 2*np.pi) - np.pi\n",
    "        alpha = np.cos(alpha)\n",
    "        reward = -(alpha**2 + 0.0001*alpha_dot**2 )\n",
    "        print(f\"Theta: {theta:.3f}, Theta dot: {theta_dot:.3f}, Alpha: {alpha:.3f}, Alpha dot: {alpha_dot:.3f}\", \n",
    "              f\"Reward: {reward:.3f}\")\n",
    "        time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7442b395",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ntk00\\AppData\\Local\\Temp\\ipykernel_17180\\584436252.py:99: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  print(f\"Epoch {step}, Total Reward: {float(reward):.4f}, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Total Reward: -7.2579, Q1: -494.7491, Q2: -493.6043, TargetQ: -489.5000 alpha: -3.1416 alpha_dot: -285.5993\n",
      "Epoch 2, Total Reward: -6.2475, Q1: -502.8865, Q2: -507.0687, TargetQ: -512.6543 alpha: 2.6047 alpha_dot: 231.6599\n",
      "Epoch 3, Total Reward: -2.7013, Q1: -498.9886, Q2: -503.8671, TargetQ: -499.4533 alpha: 3.0250 alpha_dot: 191.3105\n",
      "Epoch 4, Total Reward: -6.2613, Q1: -488.1426, Q2: -493.4980, TargetQ: -475.6987 alpha: -2.1353 alpha_dot: -253.1712\n",
      "Epoch 5, Total Reward: -5.2803, Q1: -512.7960, Q2: -518.9171, TargetQ: -520.7244 alpha: -3.1078 alpha_dot: -249.9806\n",
      "Epoch 6, Total Reward: -7.5439, Q1: -498.1374, Q2: -503.1557, TargetQ: -494.2001 alpha: 2.7243 alpha_dot: 266.8629\n",
      "Epoch 7, Total Reward: -5.2827, Q1: -515.1972, Q2: -520.5097, TargetQ: -521.9016 alpha: -2.7366 alpha_dot: -221.8647\n",
      "Epoch 8, Total Reward: -13.0053, Q1: -502.3815, Q2: -505.7062, TargetQ: -503.7465 alpha: 2.8409 alpha_dot: 363.5497\n",
      "Epoch 9, Total Reward: -6.2533, Q1: -506.9962, Q2: -509.6150, TargetQ: -515.8923 alpha: 3.1078 alpha_dot: 268.7484\n",
      "Epoch 10, Total Reward: -6.7724, Q1: -504.4292, Q2: -505.1839, TargetQ: -509.3253 alpha: -2.7796 alpha_dot: -257.6969\n",
      "Epoch 11, Total Reward: -11.6214, Q1: -494.7440, Q2: -494.3652, TargetQ: -482.7722 alpha: 2.7918 alpha_dot: 339.9685\n",
      "Epoch 12, Total Reward: -8.3828, Q1: -499.2530, Q2: -497.4175, TargetQ: -499.8981 alpha: -3.0741 alpha_dot: -306.2431\n",
      "\n",
      "Stopping (Ctrl+C). Saving…\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "history = []\n",
    "policy_delay = 2  # Delayed policy updates\n",
    "step = 0\n",
    "total_reward = 0.0\n",
    "frequency = 500  # Hz\n",
    "state_theta_dot = np.array([0,0], dtype=np.float64)\n",
    "state_alpha_dot = np.array([0,0], dtype=np.float64)\n",
    "timestamps = 0 \n",
    "try: \n",
    "    with QubeServo3(hardware = 1, pendulum = 1, frequency=10) as board:\n",
    "        while timestamps < 10 and not KILL_THREAD: \n",
    "            avg_q1, avg_q2, avg_target_q = 0.0, 0.0, 0.0\n",
    "            step += 1 \n",
    "            board.read_outputs()\n",
    "            theta = board.motorPosition * -1\n",
    "            alpha = board.pendulumPosition \n",
    "            theta = np.clip(theta, -np.pi/2, np.pi/2)\n",
    "            alpha = np.mod(alpha, 2*np.pi) - np.pi\n",
    "\n",
    "            theta_dot, state_theta_dot = ddt_filter(theta, state_theta_dot, 50, 1/frequency)\n",
    "            alpha_dot, state_alpha_dot = ddt_filter(alpha, state_alpha_dot, 100, 1/frequency)\n",
    "\n",
    "            state = np.array([theta, theta_dot, alpha, alpha_dot], dtype=np.float32)\n",
    "\n",
    "            action = actor_model(tf.convert_to_tensor([state], dtype=tf.float32)).numpy()[0]\n",
    "            action = action + np.random.normal(0, 0.1, size=action_size)  # Add exploration noise\n",
    "            action = np.clip(action, -2.0, 2.0) \n",
    "            board.write_voltage(action)\n",
    "\n",
    "            board.read_outputs()\n",
    "            next_theta = board.motorPosition * -1\n",
    "            next_alpha = board.pendulumPosition\n",
    "            next_alpha = np.mod(next_alpha, 2*np.pi) - np.pi\n",
    "            next_theta = np.clip(next_theta, -np.pi/2, np.pi/2)\n",
    "            next_theta_dot, state_theta_dot = ddt_filter(next_theta, state_theta_dot, 50, 1/frequency)\n",
    "            next_alpha_dot, state_alpha_dot = ddt_filter(next_alpha, state_alpha_dot, 100, 1/frequency)\n",
    "            next_state = np.array([next_theta, next_theta_dot, next_alpha, next_alpha_dot], dtype=np.float32)\n",
    "\n",
    "\n",
    "            # wrapped_alpha = ((alpha - np.pi + np.pi) % (2*np.pi)) - np.pi\n",
    "            reward = -(alpha**2 + 0.0001*alpha_dot**2 + 0.001*action**2)\n",
    "            total_reward += reward\n",
    "\n",
    "            replay_buffer.store(state, action, reward, next_state, False)\n",
    "            state = next_state\n",
    "\n",
    "            if replay_buffer.size() >= batch_size:\n",
    "                states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "                states = tf.convert_to_tensor(states, dtype=tf.float32)\n",
    "                actions = tf.convert_to_tensor(actions, dtype=tf.float32)\n",
    "                rewards = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
    "                next_states = tf.convert_to_tensor(next_states, dtype=tf.float32)\n",
    "                dones = tf.convert_to_tensor(dones, dtype=tf.float32) \n",
    "\n",
    "                # add clipped noise to target action\n",
    "                noise = np.clip(np.random.normal(0, 0.2, size=actions.shape), -0.5, 0.5)\n",
    "                next_actions = target_actor(next_states) + noise\n",
    "                next_actions = tf.clip_by_value(next_actions, -2.0, 2.0)  # Pendulum action bounds\n",
    "\n",
    "                # Compute target Q-values with both critics\n",
    "                target1 = tf.squeeze(target_critic1([next_states, next_actions]), axis=1)\n",
    "                target2 = tf.squeeze(target_critic2([next_states, next_actions]), axis=1)\n",
    "                target_q = rewards + gamma * (1 - dones) * tf.minimum(target1, target2)\n",
    "\n",
    "                with tf.GradientTape() as tape_critic1, tf.GradientTape() as tape_critic2:\n",
    "                    q1 = critic_model1([states, actions], training=True)\n",
    "                    q2 = critic_model2([states, actions], training=True)\n",
    "\n",
    "                    # Compute losses\n",
    "                    loss1 = tf.keras.losses.MSE(target_q, q1)\n",
    "                    loss2 = tf.keras.losses.MSE(target_q, q2)\n",
    "\n",
    "                avg_q1 = tf.reduce_mean(q1).numpy().item()\n",
    "                avg_q2 = tf.reduce_mean(q2).numpy().item()\n",
    "                avg_target_q = tf.reduce_mean(target_q).numpy().item()\n",
    "\n",
    "                # Get gradients for each critic once\n",
    "                critic_grad1 = tape_critic1.gradient(loss1, critic_model1.trainable_variables)\n",
    "                critic_grad2 = tape_critic2.gradient(loss2, critic_model2.trainable_variables)\n",
    "\n",
    "                # Apply gradients\n",
    "                critic_optimizer1.apply_gradients(zip(critic_grad1, critic_model1.trainable_variables))\n",
    "                critic_optimizer2.apply_gradients(zip(critic_grad2, critic_model2.trainable_variables))\n",
    "                if step % policy_delay == 0:  # Delayed policy updates\n",
    "                    with tf.GradientTape() as tape_actor: \n",
    "                        action = actor_model(states)\n",
    "                        actor_loss = -tf.reduce_mean(critic_model1([states, action]))\n",
    "\n",
    "                    actor_grad = tape_actor.gradient(actor_loss, actor_model.trainable_variables)\n",
    "                    actor_optimizer.apply_gradients(zip(actor_grad, actor_model.trainable_variables))\n",
    "\n",
    "                    soft_update(target_actor.variables, actor_model.variables, tau=0.005)\n",
    "                    soft_update(target_critic1.variables, critic_model1.variables, tau=0.005)\n",
    "                    soft_update(target_critic2.variables, critic_model2.variables, tau=0.005)\n",
    "\n",
    "            history.append(total_reward)\n",
    "            if step % 1 == 0:\n",
    "                print(f\"Epoch {step}, Total Reward: {float(reward):.4f}, \"\n",
    "                f\"Q1: {avg_q1:.4f}, Q2: {avg_q2:.4f}, TargetQ: {avg_target_q:.4f}\", \n",
    "                f\"alpha: {alpha:.4f}\", f\"alpha_dot: {alpha_dot:.4f}\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nStopping (Ctrl+C). Saving…\")\n",
    "finally:\n",
    "    # save weights (use .save_weights if you prefer checkpoint style)\n",
    "    actor_model.save_weights(\"saves/quanser/actor_model.weights.h5\")\n",
    "    critic_model1.save_weights(\"saves/quanser/critic_model1.weights.h5\")\n",
    "    critic_model2.save_weights(\"saves/quanser/critic_model2.weights.h5\")\n",
    "    ckpt = tf.train.Checkpoint(actor_optimizer=actor_optimizer,\n",
    "                           critic_optimizer1=critic_optimizer1,\n",
    "                           critic_optimizer2=critic_optimizer2)\n",
    "    ckpt.save(\"saves/quanser/optimizers_ckpt/ckpt\")\n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fd84a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stopping (Ctrl+C). Saving…\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# batch_size = 32\n",
    "# history = []\n",
    "# policy_delay = 2  # Delayed policy updates\n",
    "# step = 0\n",
    "# total_reward = 0.0\n",
    "\n",
    "# try:\n",
    "#     total_reward = 0.0\n",
    "#     while True:\n",
    "#         step += 1\n",
    "    \n",
    "#         # 1) read state\n",
    "#         board.read_encoder(encoder_channels, len(encoder_channels), counts)\n",
    "#         theta_arm  = counts[0] * ARM_RAD_PER_COUNT\n",
    "#         theta_pend = counts[1] * PEND_RAD_PER_COUNT\n",
    "#         theta_arm_dot  = (theta_arm  - theta_arm_prev)  / dt\n",
    "#         theta_pend_dot = (theta_pend - theta_pend_prev) / dt\n",
    "#         state = np.array([theta_arm, theta_pend, theta_arm_dot, theta_pend_dot], dtype=np.float32)\n",
    "\n",
    "#         # 2) select action\n",
    "#         action_vec = actor_model(tf.convert_to_tensor([state], dtype=tf.float32)).numpy()[0]\n",
    "#         action_val = float(np.clip(action_vec[0], -2.0, 2.0))  # scalar in [-2,2]; tune to your safe V range\n",
    "\n",
    "#         # 3) apply action (analog write wants numpy float64 buffer)\n",
    "#         voltages = np.array([action_val], dtype=np.float64)\n",
    "#         board.write_analog(motor_channels, len(motor_channels), voltages)\n",
    "\n",
    "#         # 4) get next_state after action\n",
    "#         time.sleep(dt)  # maintain loop timing around the actuation\n",
    "#         board.read_encoder(encoder_channels, len(encoder_channels), counts)\n",
    "#         next_theta_arm  = counts[0] * ARM_RAD_PER_COUNT\n",
    "#         next_theta_pend = counts[1] * PEND_RAD_PER_COUNT\n",
    "#         next_theta_arm_dot  = (next_theta_arm  - theta_arm)  / dt\n",
    "#         next_theta_pend_dot = (next_theta_pend - theta_pend) / dt\n",
    "#         next_state = np.array([next_theta_arm, next_theta_pend, next_theta_arm_dot, next_theta_pend_dot], dtype=np.float32)\n",
    "\n",
    "#         # 5) reward (example: upright pendulum, gentle motion)\n",
    "#         reward = - ( (np.angle(np.exp(1j*(next_theta_pend - np.pi))))**2\n",
    "#                      + 0.1*next_theta_pend_dot**2 + 0.01*action_val**2 )\n",
    "#         total_reward += reward\n",
    "\n",
    "#         # 6) store\n",
    "#         replay_buffer.store(state, action_val, reward, next_state, False)\n",
    "\n",
    "#         # 7) train if enough samples\n",
    "#         if replay_buffer.size() >= batch_size:\n",
    "#             states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "#             states      = tf.convert_to_tensor(states, dtype=tf.float32)\n",
    "#             actions     = tf.convert_to_tensor(actions.reshape(-1,1), dtype=tf.float32)\n",
    "#             rewards     = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
    "#             next_states = tf.convert_to_tensor(next_states, dtype=tf.float32)\n",
    "#             dones       = tf.convert_to_tensor(dones, dtype=tf.float32)\n",
    "\n",
    "#             # target policy smoothing\n",
    "#             noise = np.clip(np.random.normal(0, 0.2, size=(actions.shape[0], 1)), -0.5, 0.5)\n",
    "#             target_act = tf.clip_by_value(target_actor(next_states) + noise, -2.0, 2.0)\n",
    "\n",
    "#             # twin critics target\n",
    "#             t1 = tf.squeeze(target_critic1([next_states, target_act]), axis=1)\n",
    "#             t2 = tf.squeeze(target_critic2([next_states, target_act]), axis=1)\n",
    "#             target_q = rewards + gamma * (1.0 - dones) * tf.minimum(t1, t2)\n",
    "\n",
    "#             # critic updates\n",
    "#             with tf.GradientTape() as tape1, tf.GradientTape() as tape2:\n",
    "#                 q1 = tf.squeeze(critic_model1([states, actions]), axis=1)\n",
    "#                 q2 = tf.squeeze(critic_model2([states, actions]), axis=1)\n",
    "#                 loss1 = tf.keras.losses.MSE(target_q, q1)\n",
    "#                 loss2 = tf.keras.losses.MSE(target_q, q2)\n",
    "#             critic_optimizer1.apply_gradients(zip(tape1.gradient(loss1, critic_model1.trainable_variables),\n",
    "#                                                   critic_model1.trainable_variables))\n",
    "#             critic_optimizer2.apply_gradients(zip(tape2.gradient(loss2, critic_model2.trainable_variables),\n",
    "#                                                   critic_model2.trainable_variables))\n",
    "\n",
    "#             # delayed actor + target updates\n",
    "#             if step % policy_delay == 0:\n",
    "#                 with tf.GradientTape() as tape_actor:\n",
    "#                     pi = actor_model(states)\n",
    "#                     actor_loss = -tf.reduce_mean(critic_model1([states, pi]))\n",
    "#                 actor_optimizer.apply_gradients(zip(tape_actor.gradient(actor_loss, actor_model.trainable_variables),\n",
    "#                                                     actor_model.trainable_variables))\n",
    "#                 soft_update(target_actor.variables,   actor_model.variables,   tau=0.005)\n",
    "#                 soft_update(target_critic1.variables, critic_model1.variables, tau=0.005)\n",
    "#                 soft_update(target_critic2.variables, critic_model2.variables, tau=0.005)\n",
    "\n",
    "#         if step % 100 == 0:\n",
    "#             print(f\"Step {step}  reward_sum: {total_reward:.2f}\")\n",
    "\n",
    "#         # update prev angles for next derivative\n",
    "#         theta_arm_prev, theta_pend_prev = next_theta_arm, next_theta_pend\n",
    "\n",
    "# except KeyboardInterrupt:\n",
    "#     print(\"\\nStopping (Ctrl+C). Saving…\")\n",
    "# finally:\n",
    "#     # save weights (use .save_weights if you prefer checkpoint style)\n",
    "#     actor_model.save_weights(\"saves/quanser/actor_model.weights.h5\")\n",
    "#     critic_model1.save_weights(\"saves/quanser/critic_model1.weights.h5\")\n",
    "#     critic_model2.save_weights(\"saves/quanser/critic_model2.weights.h5\")\n",
    "#     # set motor to 0V and close safely\n",
    "#     board.write_analog(motor_channels, 1, np.array([0.0], dtype=np.float64))\n",
    "#     board.close()\n",
    "#     print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff966c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ntk00\\AppData\\Local\\Temp\\ipykernel_20244\\4268336479.py:10: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  u = float(action)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "a bytes-like object is required, not 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[70], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m u \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(action)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# send to motor\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[43mboard\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_analog\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmotor_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mu\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(dt)  \u001b[38;5;66;03m# ~0.01s\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ntk00\\anaconda3\\envs\\neural_network\\lib\\site-packages\\quanser\\hardware\\hil.py:2732\u001b[0m, in \u001b[0;36mHIL.write_analog\u001b[1;34m(self, channels, num_channels, buffer)\u001b[0m\n\u001b[0;32m   2675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrite_analog\u001b[39m(\u001b[38;5;28mself\u001b[39m, channels, num_channels, buffer):\n\u001b[0;32m   2676\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Writes to analog outputs immediately. The function does not return until the data has been written.\u001b[39;00m\n\u001b[0;32m   2677\u001b[0m \n\u001b[0;32m   2678\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2727\u001b[0m \n\u001b[0;32m   2728\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   2729\u001b[0m     result \u001b[38;5;241m=\u001b[39m hil_lib\u001b[38;5;241m.\u001b[39mhil_write_analog(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_card \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_card \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m ffi\u001b[38;5;241m.\u001b[39mNULL,\n\u001b[0;32m   2730\u001b[0m                                       ffi\u001b[38;5;241m.\u001b[39mfrom_buffer(_UINT32_ARRAY, channels) \u001b[38;5;28;01mif\u001b[39;00m channels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m ffi\u001b[38;5;241m.\u001b[39mNULL,\n\u001b[0;32m   2731\u001b[0m                                       num_channels,\n\u001b[1;32m-> 2732\u001b[0m                                       \u001b[43mffi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_buffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_DOUBLE_ARRAY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m ffi\u001b[38;5;241m.\u001b[39mNULL)\n\u001b[0;32m   2733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   2734\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m HILError(result)\n",
      "File \u001b[1;32mc:\\Users\\ntk00\\anaconda3\\envs\\neural_network\\lib\\site-packages\\cffi\\api.py:365\u001b[0m, in \u001b[0;36mFFI.from_buffer\u001b[1;34m(self, cdecl, python_buffer, require_writable)\u001b[0m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(cdecl, basestring):\n\u001b[0;32m    364\u001b[0m     cdecl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_typeof(cdecl)\n\u001b[1;32m--> 365\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_buffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcdecl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpython_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    366\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mrequire_writable\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: a bytes-like object is required, not 'list'"
     ]
    }
   ],
   "source": [
    "# while True:\n",
    "#     # read state\n",
    "#     # board.close()\n",
    "#     board.read_encoder(encoder_channels, len(encoder_channels), counts)\n",
    "#     theta_arm = counts[0] * ARM_RAD_PER_COUNT\n",
    "#     theta_pend = counts[1] * PEND_RAD_PER_COUNT\n",
    "\n",
    "#     # compute action from policy\n",
    "#     action = actor_model(tf.convert_to_tensor([state], dtype=tf.float32)).numpy()[0]\n",
    "#     u = float(action)\n",
    "\n",
    "#     # send to motor\n",
    "#     board.write_analog(motor_channels, 1, [u])\n",
    "\n",
    "#     time.sleep(dt)  # ~0.01s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e598aa56",
   "metadata": {},
   "outputs": [
    {
     "ename": "HILError",
     "evalue": "-1410",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHILError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcollections\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m deque\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# --- HIL/QUBE setup ---\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# board.close()\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m board \u001b[38;5;241m=\u001b[39m \u001b[43mHIL\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mqube_servo3_usb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m encoder_channels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39muint32)\n\u001b[0;32m     10\u001b[0m motor_channels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39muint32)\n",
      "File \u001b[1;32mc:\\Users\\ntk00\\anaconda3\\envs\\neural_network\\lib\\site-packages\\quanser\\hardware\\hil.py:663\u001b[0m, in \u001b[0;36mHIL.__init__\u001b[1;34m(self, card_type, card_identifier)\u001b[0m\n\u001b[0;32m    661\u001b[0m \u001b[38;5;66;03m# If non-default arguments are passed, attempt to open the card.\u001b[39;00m\n\u001b[0;32m    662\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m card_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 663\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcard_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcard_identifier\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ntk00\\anaconda3\\envs\\neural_network\\lib\\site-packages\\quanser\\hardware\\hil.py:774\u001b[0m, in \u001b[0;36mHIL.open\u001b[1;34m(self, card_type, card_identifier)\u001b[0m\n\u001b[0;32m    771\u001b[0m     result \u001b[38;5;241m=\u001b[39m hil_lib\u001b[38;5;241m.\u001b[39mhil_open(card_type\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUTF-8\u001b[39m\u001b[38;5;124m'\u001b[39m), card_identifier\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUTF-8\u001b[39m\u001b[38;5;124m'\u001b[39m), card)\n\u001b[0;32m    773\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 774\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HILError(result)\n\u001b[0;32m    776\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_card \u001b[38;5;241m=\u001b[39m card[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mHILError\u001b[0m: -1410"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "\n",
    "# --- HIL/QUBE setup ---\n",
    "board.close()\n",
    "board = HIL(\"qube_servo3_usb\", \"0\")\n",
    "encoder_channels = np.array([0, 1], dtype=np.uint32)\n",
    "motor_channels = np.array([0], dtype=np.uint32)\n",
    "counts = np.zeros(2, dtype=np.int32)\n",
    "\n",
    "ENCODER_RES = 2048\n",
    "ARM_RAD_PER_COUNT = 2*np.pi / ENCODER_RES\n",
    "PEND_RAD_PER_COUNT = 2*np.pi / ENCODER_RES\n",
    "dt = 0.01  # 10 ms loop\n",
    "\n",
    "# --- Replay Buffer ---\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=100000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    def store(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    def sample(self, batch_size):\n",
    "        batch = np.array(random.sample(self.buffer, batch_size))\n",
    "        states, actions, rewards, next_states, dones = map(np.stack, zip(*batch))\n",
    "        return states, actions, rewards, next_states, dones\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "replay_buffer = ReplayBuffer()\n",
    "\n",
    "# --- Soft update ---\n",
    "def soft_update(target_weights, online_weights, tau=0.005):\n",
    "    for (target, online) in zip(target_weights, online_weights):\n",
    "        target.assign(target * (1 - tau) + online * tau)\n",
    "\n",
    "# --- TD3 models already defined: actor_model, critic_model1, critic_model2, \n",
    "# target_actor, target_critic1, target_critic2\n",
    "# optimizers: actor_optimizer, critic_optimizer1, critic_optimizer2\n",
    "\n",
    "state_size = 4\n",
    "action_size = 1\n",
    "gamma = 0.99\n",
    "batch_size = 32\n",
    "policy_delay = 2\n",
    "step = 0\n",
    "\n",
    "theta_arm_prev = 0.0\n",
    "theta_pend_prev = 0.0\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        step += 1\n",
    "\n",
    "        # --- 1. Read state ---\n",
    "        board.read_encoder(encoder_channels, len(encoder_channels), counts)\n",
    "        theta_arm = counts[0] * ARM_RAD_PER_COUNT\n",
    "        theta_pend = counts[1] * PEND_RAD_PER_COUNT\n",
    "\n",
    "        theta_arm_dot = (theta_arm - theta_arm_prev) / dt\n",
    "        theta_pend_dot = (theta_pend - theta_pend_prev) / dt\n",
    "        theta_arm_prev, theta_pend_prev = theta_arm, theta_pend\n",
    "\n",
    "        state = np.array([theta_arm, theta_pend, theta_arm_dot, theta_pend_dot], dtype=np.float32)\n",
    "\n",
    "        # --- 2. Compute action ---\n",
    "        action = actor_model(tf.convert_to_tensor([state], dtype=tf.float32)).numpy()[0]\n",
    "        u_array = np.array([float(action)], dtype=np.float64)\n",
    "\n",
    "        # --- 3. Apply action ---\n",
    "        board.write_analog(motor_channels, 1, u_array)\n",
    "\n",
    "        # --- 4. Read next state ---\n",
    "        board.read_encoder(encoder_channels, len(encoder_channels), counts)\n",
    "        next_theta_arm = counts[0] * ARM_RAD_PER_COUNT\n",
    "        next_theta_pend = counts[1] * PEND_RAD_PER_COUNT\n",
    "        next_theta_arm_dot = (next_theta_arm - theta_arm) / dt\n",
    "        next_theta_pend_dot = (next_theta_pend - theta_pend) / dt\n",
    "        next_state = np.array([next_theta_arm, next_theta_pend, next_theta_arm_dot, next_theta_pend_dot], dtype=np.float32)\n",
    "\n",
    "        # --- 5. Compute reward ---\n",
    "        reward = - (next_theta_pend**2 + 0.1 * next_theta_pend_dot**2)\n",
    "\n",
    "        # --- 6. Store transition ---\n",
    "        replay_buffer.store(state, action, reward, next_state, False)\n",
    "\n",
    "        # --- 7. Train TD3 ---\n",
    "        if replay_buffer.size() >= batch_size:\n",
    "            states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "            states = tf.convert_to_tensor(states, dtype=tf.float32)\n",
    "            actions = tf.convert_to_tensor(actions, dtype=tf.float32)\n",
    "            rewards = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
    "            next_states = tf.convert_to_tensor(next_states, dtype=tf.float32)\n",
    "            dones = tf.convert_to_tensor(dones, dtype=tf.float32)\n",
    "\n",
    "            # Target actions with clipped noise\n",
    "            noise = np.clip(np.random.normal(0, 0.2, size=actions.shape), -0.5, 0.5)\n",
    "            next_actions = tf.clip_by_value(target_actor(next_states) + noise, -2.0, 2.0)\n",
    "\n",
    "            # Target Q-values\n",
    "            target1 = tf.squeeze(target_critic1([next_states, next_actions]), axis=1)\n",
    "            target2 = tf.squeeze(target_critic2([next_states, next_actions]), axis=1)\n",
    "            target_q = rewards + gamma * (1 - dones) * tf.minimum(target1, target2)\n",
    "\n",
    "            # Critic updates\n",
    "            with tf.GradientTape() as tape1, tf.GradientTape() as tape2:\n",
    "                q1 = critic_model1([states, actions], training=True)\n",
    "                q2 = critic_model2([states, actions], training=True)\n",
    "                loss1 = tf.keras.losses.MSE(target_q, q1)\n",
    "                loss2 = tf.keras.losses.MSE(target_q, q2)\n",
    "\n",
    "            critic_grad1 = tape1.gradient(loss1, critic_model1.trainable_variables)\n",
    "            critic_grad2 = tape2.gradient(loss2, critic_model2.trainable_variables)\n",
    "            critic_optimizer1.apply_gradients(zip(critic_grad1, critic_model1.trainable_variables))\n",
    "            critic_optimizer2.apply_gradients(zip(critic_grad2, critic_model2.trainable_variables))\n",
    "\n",
    "            # Delayed actor update\n",
    "            if step % policy_delay == 0:\n",
    "                with tf.GradientTape() as tape_actor:\n",
    "                    act = actor_model(states)\n",
    "                    actor_loss = -tf.reduce_mean(critic_model1([states, act]))\n",
    "                actor_grad = tape_actor.gradient(actor_loss, actor_model.trainable_variables)\n",
    "                actor_optimizer.apply_gradients(zip(actor_grad, actor_model.trainable_variables))\n",
    "\n",
    "                soft_update(target_actor.variables, actor_model.variables)\n",
    "                soft_update(target_critic1.variables, critic_model1.variables)\n",
    "                soft_update(target_critic2.variables, critic_model2.variables)\n",
    "\n",
    "        # --- 8. Sleep to maintain loop ---\n",
    "        time.sleep(dt)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Stopping (Ctrl+C) and saving models...\")\n",
    "\n",
    "finally:\n",
    "    # Save models\n",
    "    actor_model.save(\"td3_actor.h5\")\n",
    "    critic_model1.save(\"td3_critic1.h5\")\n",
    "    critic_model2.save(\"td3_critic2.h5\")\n",
    "    board.close()\n",
    "    print(\"Training finished and models saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neural_network",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
