{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8ceea56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Lambda\n",
    "from keras.layers import Concatenate\n",
    "from collections import deque\n",
    "import random\n",
    "from quanser.hardware import HIL \n",
    "from pal.products.qube import QubeServo3\n",
    "from pal.utilities.math import SignalGenerator, ddt_filter\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8674aa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # board.close()\n",
    "# # Open connection to QUBE\n",
    "# # board = HIL(\"qube_servo3_usb\", \"0\")\n",
    "\n",
    "# encoder_channels = np.array([0, 1], dtype=np.uint32)\n",
    "# motor_channels = np.array([0], dtype=np.uint32)\n",
    "# counts = np.zeros(2, dtype=np.int32)\n",
    "\n",
    "# ENCODER_RES = 2048\n",
    "# ARM_RAD_PER_COUNT = 2*np.pi / ENCODER_RES\n",
    "# PEND_RAD_PER_COUNT = 2*np.pi / ENCODER_RES\n",
    "\n",
    "# dt = 0.01  # 10 ms\n",
    "# theta_arm_prev  = counts[0] * ARM_RAD_PER_COUNT\n",
    "# theta_pend_prev = counts[1] * PEND_RAD_PER_COUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49e7c226",
   "metadata": {},
   "outputs": [],
   "source": [
    "global KILL_THREAD\n",
    "KILL_THREAD = False\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=100000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    def store(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = map(np.array, zip(*batch))\n",
    "        return (np.array(states, dtype=np.float32),\n",
    "            np.array(actions, dtype=np.float32),\n",
    "            np.array(rewards, dtype=np.float32),\n",
    "            np.array(next_states, dtype=np.float32),\n",
    "            np.array(dones, dtype=np.float32))\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "def soft_update(target_weights, online_weights, tau=0.005):\n",
    "    for (target, online) in zip(target_weights, online_weights):\n",
    "        target.assign(target * (1 - tau) + online * tau) \n",
    "\n",
    "def sig_handler(*args): \n",
    "    global KILL_THREAD\n",
    "    KILL_THREAD = True\n",
    "\n",
    "# signal.signal(signal.SIGINT, sig_handler)\n",
    "\n",
    "replay_buffer = ReplayBuffer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8509c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ntk00\\anaconda3\\envs\\neural_network\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:232: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x23e05727130>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_size = 4\n",
    "action_size = 1\n",
    "gamma = 0.99 # discount rate\n",
    "learning_rate = 0.001 # learning rate\n",
    "\n",
    "# Define the actor model\n",
    "states_inputs = Input(shape=(state_size,))\n",
    "dense = Dense(400, activation='relu')(states_inputs)\n",
    "dense = Dense(300, activation='relu')(dense)\n",
    "outputs = Dense(action_size, activation='tanh')(dense)\n",
    "outputs = keras.layers.Lambda(lambda x: x * 2.0)(outputs)  # Scale action to [-2, 2]\n",
    "actor_model = Model(inputs=states_inputs, outputs=outputs)\n",
    "\n",
    "# Critic 1\n",
    "state_input1 = Input(shape=(state_size,))\n",
    "action_input1 = Input(shape=(action_size,))\n",
    "concat1 = Concatenate()([state_input1, action_input1])\n",
    "dense1 = Dense(400, activation='relu')(concat1)\n",
    "dense1 = Dense(300, activation='relu')(dense1)\n",
    "output1 = Dense(1)(dense1)\n",
    "critic_model1 = Model([state_input1, action_input1], output1)\n",
    "\n",
    "# Critic 2\n",
    "state_input2 = Input(shape=(state_size,))\n",
    "action_input2 = Input(shape=(action_size,))\n",
    "concat2 = Concatenate()([state_input2, action_input2])\n",
    "dense2 = Dense(400, activation='relu')(concat2)\n",
    "dense2 = Dense(300, activation='relu')(dense2)\n",
    "output2 = Dense(1)(dense2)\n",
    "critic_model2 = Model([state_input2, action_input2], output2)\n",
    "\n",
    "try:\n",
    "    actor_model.load_weights('saves/quanser/actor_model.weights.h5')\n",
    "    critic_model1.load_weights('saves/quanser/critic_model1.weights.h5')\n",
    "    critic_model2.load_weights('saves/quanser/critic_model2.weights.h5')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "actor_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "critic_optimizer1 = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "critic_optimizer2 = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "target_actor = keras.models.clone_model(actor_model)\n",
    "target_actor.set_weights(actor_model.get_weights())\n",
    "\n",
    "target_critic1 = keras.models.clone_model(critic_model1)\n",
    "target_critic1.set_weights(critic_model1.get_weights())\n",
    "target_critic2 = keras.models.clone_model(critic_model2)\n",
    "target_critic2.set_weights(critic_model2.get_weights())\n",
    "\n",
    "ckpt = tf.train.Checkpoint(actor_optimizer=actor_optimizer,\n",
    "                           critic_optimizer1=critic_optimizer1, \n",
    "                           critic_optimizer2=critic_optimizer2)\n",
    "\n",
    "# Restore the latest checkpoint with optimizer states\n",
    "ckpt.restore(tf.train.latest_checkpoint(\"saves/quanser/optimizers_ckpt\")).expect_partial()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "844cf3fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta: 0.000, Theta dot: 0.000, Alpha: -3.142, Alpha dot: 0.000\n",
      "Theta: 0.000, Theta dot: 0.000, Alpha: -3.142, Alpha dot: 0.000\n",
      "Theta: 0.000, Theta dot: 0.000, Alpha: -3.142, Alpha dot: 0.000\n",
      "Theta: 0.000, Theta dot: 0.000, Alpha: -3.142, Alpha dot: 0.000\n",
      "Theta: 0.000, Theta dot: 0.000, Alpha: -3.142, Alpha dot: 0.000\n",
      "Theta: 0.000, Theta dot: 0.000, Alpha: -3.142, Alpha dot: 0.000\n",
      "Theta: 0.000, Theta dot: 0.000, Alpha: -3.142, Alpha dot: 0.000\n",
      "Theta: 0.000, Theta dot: 0.000, Alpha: -3.142, Alpha dot: 0.000\n",
      "Theta: 0.000, Theta dot: 0.000, Alpha: -3.142, Alpha dot: 0.000\n",
      "Theta: 0.000, Theta dot: 0.000, Alpha: -3.142, Alpha dot: 0.000\n",
      "Theta: 0.000, Theta dot: 0.000, Alpha: -3.142, Alpha dot: 0.000\n",
      "Theta: 0.000, Theta dot: 0.000, Alpha: -3.142, Alpha dot: 0.000\n",
      "Theta: 0.021, Theta dot: 1.023, Alpha: -3.114, Alpha dot: 2.510\n",
      "Theta: -0.298, Theta dot: -14.268, Alpha: 1.466, Alpha dot: -152.739\n",
      "Theta: -0.074, Theta dot: -2.245, Alpha: -0.052, Alpha dot: -263.026\n",
      "Theta: -0.110, Theta dot: -3.784, Alpha: -0.040, Alpha dot: -214.088\n",
      "Theta: -0.120, Theta dot: -3.862, Alpha: -0.040, Alpha dot: -175.163\n",
      "Theta: -0.120, Theta dot: -3.494, Alpha: -0.037, Alpha dot: -143.036\n",
      "Theta: -0.114, Theta dot: -2.869, Alpha: -0.021, Alpha dot: -115.635\n",
      "Theta: -0.117, Theta dot: -2.742, Alpha: -0.028, Alpha dot: -95.168\n",
      "Theta: -0.129, Theta dot: -3.065, Alpha: -0.015, Alpha dot: -76.749\n",
      "Theta: -0.141, Theta dot: -3.358, Alpha: -0.037, Alpha dot: -64.747\n",
      "Theta: -0.181, Theta dot: -4.937, Alpha: -0.049, Alpha dot: -54.091\n",
      "Theta: -0.169, Theta dot: -3.883, Alpha: -0.046, Alpha dot: -43.977\n",
      "Theta: -0.178, Theta dot: -3.951, Alpha: -0.031, Alpha dot: -34.587\n",
      "Theta: -0.132, Theta dot: -1.383, Alpha: -0.040, Alpha dot: -29.135\n",
      "Theta: -0.135, Theta dot: -1.398, Alpha: -0.034, Alpha dot: -23.280\n",
      "Theta: -0.126, Theta dot: -0.826, Alpha: -0.034, Alpha dot: -19.047\n",
      "Theta: -0.089, Theta dot: 1.005, Alpha: -0.021, Alpha dot: -14.468\n",
      "Theta: -0.110, Theta dot: -0.113, Alpha: -0.031, Alpha dot: -12.675\n",
      "Theta: -0.129, Theta dot: -0.979, Alpha: -0.006, Alpha dot: -8.139\n",
      "Theta: -0.126, Theta dot: -0.739, Alpha: 0.046, Alpha dot: -1.918\n",
      "Theta: -0.135, Theta dot: -1.107, Alpha: 0.055, Alpha dot: -0.732\n",
      "Theta: -0.126, Theta dot: -0.564, Alpha: 0.061, Alpha dot: -0.041\n",
      "Theta: -0.150, Theta dot: -1.679, Alpha: 0.114, Alpha dot: 4.708\n",
      "Theta: -0.117, Theta dot: 0.088, Alpha: 2.138, Alpha dot: 187.929\n",
      "Theta: 0.184, Theta dot: 14.397, Alpha: -3.142, Alpha dot: 244.963\n",
      "Theta: 0.003, Theta dot: 4.406, Alpha: 3.108, Alpha dot: 197.356\n",
      "Theta: -0.052, Theta dot: 1.357, Alpha: -3.120, Alpha dot: 166.493\n",
      "Theta: -0.061, Theta dot: 0.790, Alpha: 3.139, Alpha dot: 133.991\n",
      "Theta: -0.061, Theta dot: 0.714, Alpha: -3.139, Alpha dot: 110.186\n",
      "Theta: -0.061, Theta dot: 0.646, Alpha: -3.139, Alpha dot: 90.153\n",
      "Theta: -0.061, Theta dot: 0.585, Alpha: -3.139, Alpha dot: 73.761\n",
      "Theta: -0.061, Theta dot: 0.529, Alpha: -3.139, Alpha dot: 60.350\n",
      "Theta: -0.061, Theta dot: 0.479, Alpha: -3.139, Alpha dot: 49.377\n",
      "Theta: -0.129, Theta dot: -2.781, Alpha: -3.139, Alpha dot: 40.400\n",
      "Theta: -0.114, Theta dot: -1.786, Alpha: -3.142, Alpha dot: 32.775\n",
      "Theta: -0.058, Theta dot: 1.014, Alpha: -3.010, Alpha dot: 38.809\n",
      "Theta: 0.012, Theta dot: 4.278, Alpha: -2.605, Alpha dot: 68.568\n",
      "Theta: 0.052, Theta dot: 5.769, Alpha: -1.816, Alpha dot: 127.780\n",
      "Theta: 0.009, Theta dot: 3.175, Alpha: -1.015, Alpha dot: 177.342\n",
      "Theta: 0.031, Theta dot: 3.895, Alpha: -0.310, Alpha dot: 209.246\n",
      "Theta: -0.052, Theta dot: -0.420, Alpha: 0.282, Alpha dot: 225.030\n",
      "Theta: -0.083, Theta dot: -1.841, Alpha: 0.844, Alpha dot: 235.155\n",
      "Theta: -0.049, Theta dot: -0.059, Alpha: 1.304, Alpha dot: 234.236\n",
      "Theta: -0.040, Theta dot: 0.385, Alpha: 1.414, Alpha dot: 201.688\n",
      "Theta: -0.012, Theta dot: 1.663, Alpha: 0.758, Alpha dot: 105.332\n",
      "Theta: -0.012, Theta dot: 1.505, Alpha: 0.006, Alpha dot: 17.849\n",
      "Theta: -0.015, Theta dot: 1.215, Alpha: 0.015, Alpha dot: 15.440\n",
      "Theta: -0.012, Theta dot: 1.246, Alpha: 0.031, Alpha dot: 14.027\n",
      "Theta: -0.009, Theta dot: 1.273, Alpha: 0.031, Alpha dot: 11.477\n",
      "Theta: -0.006, Theta dot: 1.298, Alpha: 0.037, Alpha dot: 9.948\n",
      "Theta: 0.006, Theta dot: 1.759, Alpha: 0.040, Alpha dot: 8.418\n",
      "Theta: 0.015, Theta dot: 2.030, Alpha: 0.052, Alpha dot: 8.003\n",
      "Theta: -0.025, Theta dot: -0.063, Alpha: -0.064, Alpha dot: -4.050\n",
      "Theta: -0.114, Theta dot: -4.294, Alpha: -1.543, Alpha dot: -137.746\n",
      "Theta: -0.077, Theta dot: -2.132, Alpha: -2.187, Alpha dot: -171.272\n",
      "Theta: -0.064, Theta dot: -1.344, Alpha: -2.187, Alpha dot: -140.131\n",
      "Theta: -0.092, Theta dot: -2.531, Alpha: -2.267, Alpha dot: -121.905\n",
      "Theta: 0.307, Theta dot: 16.702, Alpha: -0.221, Alpha dot: 86.290\n",
      "Theta: 0.101, Theta dot: 5.323, Alpha: 0.994, Alpha dot: 181.047\n",
      "Theta: 0.163, Theta dot: 7.738, Alpha: 1.985, Alpha dot: 238.216\n",
      "Theta: 0.104, Theta dot: 4.225, Alpha: 2.908, Alpha dot: 278.855\n",
      "Theta: 0.012, Theta dot: -0.560, Alpha: 3.135, Alpha dot: 248.793\n",
      "Theta: 0.012, Theta dot: -0.507, Alpha: -3.135, Alpha dot: 204.673\n",
      "Theta: 0.009, Theta dot: -0.604, Alpha: 3.135, Alpha dot: 166.345\n",
      "Theta: 0.006, Theta dot: -0.693, Alpha: 3.139, Alpha dot: 136.379\n",
      "Theta: 0.058, Theta dot: 1.857, Alpha: -2.948, Alpha dot: 129.433\n",
      "Theta: 0.123, Theta dot: 4.748, Alpha: -1.273, Alpha dot: 258.182\n",
      "Theta: 0.163, Theta dot: 6.195, Alpha: 0.141, Alpha dot: 339.815\n",
      "Theta: 0.218, Theta dot: 8.235, Alpha: 0.025, Alpha dot: 267.432\n",
      "Theta: 0.209, Theta dot: 7.012, Alpha: 0.031, Alpha dot: 219.366\n",
      "Theta: 0.215, Theta dot: 6.636, Alpha: 0.040, Alpha dot: 180.318\n",
      "Theta: 0.212, Theta dot: 5.858, Alpha: 0.049, Alpha dot: 148.370\n",
      "Theta: 0.209, Theta dot: 5.154, Alpha: 0.055, Alpha dot: 121.951\n",
      "Theta: 0.202, Theta dot: 4.371, Alpha: 0.058, Alpha dot: 100.057\n",
      "Theta: 0.163, Theta dot: 2.056, Alpha: -0.006, Alpha dot: 76.008\n",
      "Theta: 0.187, Theta dot: 3.029, Alpha: 0.040, Alpha dot: 66.372\n",
      "Theta: 0.209, Theta dot: 3.763, Alpha: 0.034, Alpha dot: 53.746\n",
      "Theta: 0.221, Theta dot: 3.989, Alpha: -0.037, Alpha dot: 37.560\n",
      "Theta: 0.224, Theta dot: 3.755, Alpha: -0.018, Alpha dot: 32.404\n",
      "Theta: 0.181, Theta dot: 1.352, Alpha: -0.058, Alpha dot: 22.887\n",
      "Theta: 0.230, Theta dot: 3.561, Alpha: -0.043, Alpha dot: 20.120\n",
      "Theta: 0.209, Theta dot: 2.199, Alpha: -0.055, Alpha dot: 15.346\n",
      "Theta: 0.224, Theta dot: 2.720, Alpha: -0.187, Alpha dot: 0.563\n",
      "Theta: 0.187, Theta dot: 0.708, Alpha: -0.322, Alpha dot: -11.811\n",
      "Theta: 0.175, Theta dot: 0.056, Alpha: -0.374, Alpha dot: -14.405\n",
      "Theta: 0.282, Theta dot: 5.164, Alpha: -0.693, Alpha dot: -40.792\n",
      "Theta: 0.006, Theta dot: -8.476, Alpha: -2.792, Alpha dot: -224.147\n",
      "Theta: -0.307, Theta dot: -22.570, Alpha: 3.077, Alpha dot: -221.045\n",
      "Theta: -0.454, Theta dot: -27.433, Alpha: 0.930, Alpha dot: -376.089\n",
      "Theta: -0.540, Theta dot: -28.911, Alpha: -3.050, Alpha dot: -669.450\n",
      "Theta: -0.500, Theta dot: -24.259, Alpha: 3.093, Alpha dot: -560.561\n",
      "Theta: -0.209, Theta dot: -8.069, Alpha: -3.126, Alpha dot: -452.784\n",
      "Theta: -0.230, Theta dot: -8.324, Alpha: -3.142, Alpha dot: -371.854\n",
      "Theta: -0.224, Theta dot: -7.239, Alpha: 3.135, Alpha dot: -304.802\n",
      "Theta: -0.224, Theta dot: -6.549, Alpha: 3.139, Alpha dot: -249.105\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m alpha \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmod(alpha, \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39mpi) \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mpi\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTheta: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtheta\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Theta dot: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtheta_dot\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Alpha: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00malpha\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Alpha dot: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00malpha_dot\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 19\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "frequency = 500  # Hz\n",
    "state_theta_dot = np.array([0,0], dtype=np.float64)\n",
    "state_alpha_dot = np.array([0,0], dtype=np.float64)\n",
    "with QubeServo3(hardware = 1, pendulum = 1, frequency=frequency) as board:\n",
    "    while True:\n",
    "        # Have to initialize the board first before reading motorPosition or it won't read\n",
    "        board.read_outputs()\n",
    "        theta = board.motorPosition \n",
    "        alpha = board.pendulumPosition \n",
    "        theta = np.clip(theta, -np.pi/2, np.pi/2)\n",
    "        theta_dot, state_theta_dot = ddt_filter(theta, state_theta_dot, 50, 1/frequency)\n",
    "        # u - input\n",
    "        # state - previous state returned by this function -- initialize to np.array([0,0], dtype=np.float64)\n",
    "        # Ts - sample time in seconds\n",
    "        # A - filter bandwidth in rad/s\n",
    "        alpha_dot, state_alpha_dot = ddt_filter(alpha, state_alpha_dot, 100, 1/frequency)\n",
    "        alpha = np.mod(alpha, 2*np.pi) - np.pi\n",
    "        print(f\"Theta: {theta:.3f}, Theta dot: {theta_dot:.3f}, Alpha: {alpha:.3f}, Alpha dot: {alpha_dot:.3f}\")\n",
    "        time.sleep(0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7442b395",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ntk00\\AppData\\Local\\Temp\\ipykernel_29496\\50178365.py:97: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  print(f\"Epoch {step}, Total Reward: {float(reward):.4f}, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Total Reward: -91.4406, Q1: -7974.4395, Q2: -7996.4419, TargetQ: -7391.7920 alpha: -3.1416\n",
      "Epoch 2, Total Reward: -61.1357, Q1: -7500.3945, Q2: -7529.3428, TargetQ: -6920.3638 alpha: 2.6200\n",
      "Epoch 3, Total Reward: -35.8207, Q1: -7987.5566, Q2: -7996.3394, TargetQ: -8576.7686 alpha: -2.0739\n",
      "Epoch 4, Total Reward: -103.7050, Q1: -7513.1436, Q2: -7511.7739, TargetQ: -7428.1025 alpha: 2.6384\n",
      "Epoch 5, Total Reward: -41.3010, Q1: -7410.3647, Q2: -7407.1909, TargetQ: -7763.3174 alpha: 2.2856\n",
      "Epoch 6, Total Reward: -21.6234, Q1: -7736.5059, Q2: -7736.4961, TargetQ: -8025.3223 alpha: -1.4941\n",
      "Epoch 7, Total Reward: -90.8898, Q1: -8044.7549, Q2: -8076.8975, TargetQ: -8133.7549 alpha: 2.6599\n",
      "Epoch 8, Total Reward: -2.3554, Q1: -7896.8232, Q2: -7946.6309, TargetQ: -8473.4219 alpha: 0.7271\n",
      "Epoch 9, Total Reward: -117.5281, Q1: -7508.0405, Q2: -7511.3955, TargetQ: -6958.6670 alpha: -3.0956\n",
      "Epoch 10, Total Reward: -10.8032, Q1: -7775.0059, Q2: -7775.1816, TargetQ: -8203.5469 alpha: -1.4266\n",
      "Epoch 11, Total Reward: -75.4665, Q1: -8047.6069, Q2: -8051.3799, TargetQ: -8420.5078 alpha: 2.0064\n",
      "Epoch 12, Total Reward: -41.7205, Q1: -7758.6104, Q2: -7737.8057, TargetQ: -7341.9785 alpha: 2.0555\n",
      "Epoch 13, Total Reward: -12.9531, Q1: -7517.6221, Q2: -7494.8906, TargetQ: -6677.9209 alpha: -1.3744\n",
      "Epoch 14, Total Reward: -11.2754, Q1: -8027.0117, Q2: -8003.1133, TargetQ: -8162.3232 alpha: -1.5524\n",
      "Epoch 15, Total Reward: -13.3640, Q1: -7974.2974, Q2: -7949.7305, TargetQ: -8568.3779 alpha: 1.1474\n",
      "Epoch 16, Total Reward: -7.8169, Q1: -7945.2388, Q2: -7935.0098, TargetQ: -7889.8892 alpha: -0.7670\n",
      "Epoch 17, Total Reward: -66.2660, Q1: -7952.8438, Q2: -7925.3623, TargetQ: -7990.3584 alpha: -3.0434\n",
      "Epoch 18, Total Reward: -69.3418, Q1: -8420.6562, Q2: -8424.6543, TargetQ: -8760.2695 alpha: 2.4758\n",
      "Epoch 19, Total Reward: -24.7856, Q1: -7791.9219, Q2: -7793.2671, TargetQ: -6966.3086 alpha: -1.9144\n",
      "Epoch 20, Total Reward: -89.4593, Q1: -7995.5171, Q2: -7981.8027, TargetQ: -7813.0576 alpha: 2.2795\n",
      "Epoch 21, Total Reward: -38.9689, Q1: -7515.4790, Q2: -7495.6582, TargetQ: -7369.6055 alpha: 2.0586\n",
      "Epoch 22, Total Reward: -14.9153, Q1: -7661.8013, Q2: -7660.8018, TargetQ: -8185.0186 alpha: -1.3867\n",
      "Epoch 23, Total Reward: -61.0306, Q1: -8225.1895, Q2: -8209.0625, TargetQ: -8302.8594 alpha: -3.0741\n",
      "Epoch 24, Total Reward: -18.2455, Q1: -7611.1929, Q2: -7615.0654, TargetQ: -7127.5752 alpha: 0.8560\n",
      "Epoch 25, Total Reward: -54.2219, Q1: -7818.2119, Q2: -7831.1685, TargetQ: -8406.1133 alpha: -2.3899\n",
      "Epoch 26, Total Reward: -0.3555, Q1: -7876.0723, Q2: -7886.2451, TargetQ: -8205.9004 alpha: -0.3283\n",
      "Epoch 27, Total Reward: -11.8432, Q1: -7998.8242, Q2: -8001.9258, TargetQ: -8911.3184 alpha: -1.5370\n",
      "Epoch 28, Total Reward: -23.4348, Q1: -7415.0024, Q2: -7400.0312, TargetQ: -8011.5581 alpha: 1.5708\n",
      "Epoch 29, Total Reward: -45.0047, Q1: -7848.0405, Q2: -7853.5283, TargetQ: -7615.8394 alpha: 2.6354\n",
      "Epoch 30, Total Reward: -24.6888, Q1: -7359.0742, Q2: -7373.5684, TargetQ: -7091.8262 alpha: -1.2885\n",
      "Epoch 31, Total Reward: -53.3759, Q1: -7395.3032, Q2: -7397.7197, TargetQ: -6863.5020 alpha: 2.1322\n",
      "Epoch 32, Total Reward: -0.1572, Q1: -8027.1870, Q2: -8063.8188, TargetQ: -8024.4556 alpha: 0.1350\n",
      "Epoch 33, Total Reward: -3.2190, Q1: -7573.9141, Q2: -7602.2544, TargetQ: -7147.8643 alpha: 0.7578\n",
      "Epoch 34, Total Reward: -39.7886, Q1: -7103.8813, Q2: -7098.8174, TargetQ: -7009.6279 alpha: -1.4205\n",
      "Epoch 35, Total Reward: -19.9819, Q1: -8765.5195, Q2: -8798.3789, TargetQ: -9497.4443 alpha: -1.3254\n",
      "Epoch 36, Total Reward: -41.0497, Q1: -7950.9131, Q2: -7950.2520, TargetQ: -8010.7578 alpha: 1.5278\n",
      "Epoch 37, Total Reward: -28.4468, Q1: -7469.3906, Q2: -7463.0840, TargetQ: -7924.2598 alpha: 1.7395\n",
      "Epoch 38, Total Reward: -7.6344, Q1: -7995.3994, Q2: -8026.5923, TargetQ: -7839.3916 alpha: -1.0953\n",
      "Epoch 39, Total Reward: -0.5271, Q1: -7487.2285, Q2: -7468.9790, TargetQ: -7010.4824 alpha: -0.5829\n",
      "Epoch 40, Total Reward: -59.9614, Q1: -8937.4775, Q2: -8952.0000, TargetQ: -9907.0928 alpha: -3.0741\n",
      "Epoch 41, Total Reward: -6.8482, Q1: -7597.8838, Q2: -7609.2207, TargetQ: -7552.4326 alpha: 0.2638\n",
      "Epoch 42, Total Reward: -40.9303, Q1: -7331.0303, Q2: -7357.5527, TargetQ: -6801.4336 alpha: -2.5863\n",
      "Epoch 43, Total Reward: -137.0527, Q1: -7783.0889, Q2: -7799.2842, TargetQ: -7372.5894 alpha: 2.7213\n",
      "Epoch 44, Total Reward: -63.0531, Q1: -8122.3892, Q2: -8130.6357, TargetQ: -8236.0107 alpha: -2.5403\n",
      "Epoch 45, Total Reward: -106.9609, Q1: -7744.8164, Q2: -7761.4810, TargetQ: -8578.8877 alpha: 2.7213\n",
      "Epoch 46, Total Reward: -36.7437, Q1: -8584.3672, Q2: -8612.0664, TargetQ: -8609.9580 alpha: 2.1997\n",
      "Epoch 47, Total Reward: -41.3535, Q1: -7789.8027, Q2: -7793.3696, TargetQ: -7792.2275 alpha: -2.1813\n",
      "Epoch 48, Total Reward: -68.1307, Q1: -8005.0601, Q2: -7989.4473, TargetQ: -9176.3926 alpha: 1.9574\n",
      "Epoch 49, Total Reward: -2.9947, Q1: -7844.1646, Q2: -7862.9585, TargetQ: -7413.3506 alpha: 0.4234\n",
      "Epoch 50, Total Reward: -91.1917, Q1: -7286.8979, Q2: -7302.9536, TargetQ: -6861.7651 alpha: -2.9452\n",
      "Epoch 51, Total Reward: -128.9522, Q1: -8520.3418, Q2: -8529.1992, TargetQ: -9270.1367 alpha: 3.0649\n",
      "Epoch 52, Total Reward: -20.5588, Q1: -8351.9434, Q2: -8343.5039, TargetQ: -7658.9990 alpha: 1.7334\n",
      "Epoch 53, Total Reward: -111.7339, Q1: -8621.4521, Q2: -8598.4082, TargetQ: -8398.0996 alpha: -2.5740\n",
      "Epoch 54, Total Reward: -25.6640, Q1: -7921.9253, Q2: -7944.8726, TargetQ: -8078.7256 alpha: 1.4143\n",
      "Epoch 55, Total Reward: -8.4614, Q1: -8279.6650, Q2: -8281.1211, TargetQ: -7650.8325 alpha: 1.0830\n",
      "Epoch 56, Total Reward: -86.9634, Q1: -9340.8613, Q2: -9307.9297, TargetQ: -9395.6992 alpha: -2.3623\n",
      "Epoch 57, Total Reward: -48.9539, Q1: -7831.8584, Q2: -7839.8340, TargetQ: -7205.8979 alpha: 2.0279\n",
      "Epoch 58, Total Reward: -0.8196, Q1: -7824.3110, Q2: -7846.4824, TargetQ: -8203.3730 alpha: 0.4847\n",
      "Epoch 59, Total Reward: -100.5376, Q1: -8479.7842, Q2: -8487.9932, TargetQ: -9014.7432 alpha: -2.8655\n",
      "Epoch 60, Total Reward: -43.9112, Q1: -7641.8452, Q2: -7641.6260, TargetQ: -6740.6523 alpha: -2.6139\n",
      "Epoch 61, Total Reward: -8.2243, Q1: -8294.6016, Q2: -8296.2520, TargetQ: -8421.5938 alpha: 0.5983\n",
      "Epoch 62, Total Reward: -12.5803, Q1: -8056.2808, Q2: -8054.2939, TargetQ: -7560.9951 alpha: 1.0677\n",
      "Epoch 63, Total Reward: -72.5882, Q1: -7188.0195, Q2: -7213.8647, TargetQ: -6553.9028 alpha: -2.2151\n",
      "Epoch 64, Total Reward: -84.3464, Q1: -7967.0464, Q2: -7973.1436, TargetQ: -8417.1465 alpha: 2.7059\n",
      "Epoch 65, Total Reward: -29.9777, Q1: -7299.7178, Q2: -7274.2051, TargetQ: -7689.3047 alpha: 2.2795\n",
      "Epoch 66, Total Reward: -141.3512, Q1: -7260.1055, Q2: -7228.9424, TargetQ: -6651.8818 alpha: -2.7857\n",
      "Epoch 67, Total Reward: -2.2476, Q1: -7600.9219, Q2: -7585.0405, TargetQ: -7648.0093 alpha: 1.2579\n",
      "Epoch 68, Total Reward: -1.2643, Q1: -7696.4077, Q2: -7659.1553, TargetQ: -7817.3936 alpha: 1.0584\n",
      "Epoch 69, Total Reward: -98.4007, Q1: -8097.2290, Q2: -8058.2310, TargetQ: -8099.2358 alpha: -2.0739\n",
      "Epoch 70, Total Reward: -71.7184, Q1: -8207.7529, Q2: -8177.0908, TargetQ: -8886.0449 alpha: 2.9544\n",
      "Epoch 71, Total Reward: -30.9035, Q1: -6982.3594, Q2: -6952.7988, TargetQ: -6625.5991 alpha: 2.7765\n",
      "Epoch 72, Total Reward: -198.9021, Q1: -8364.7822, Q2: -8353.6973, TargetQ: -9019.1807 alpha: -3.1170\n",
      "Epoch 73, Total Reward: -1.3088, Q1: -7621.5635, Q2: -7601.0820, TargetQ: -7419.4619 alpha: 1.1382\n",
      "Epoch 74, Total Reward: -2.2287, Q1: -8026.1328, Q2: -8017.5234, TargetQ: -8536.5645 alpha: 1.3100\n",
      "Epoch 75, Total Reward: -122.0089, Q1: -7476.0381, Q2: -7487.1680, TargetQ: -6894.8135 alpha: -2.3102\n",
      "Epoch 76, Total Reward: -71.3164, Q1: -7734.6348, Q2: -7742.4854, TargetQ: -7799.2373 alpha: 2.9299\n",
      "Epoch 77, Total Reward: -3.9673, Q1: -9308.9570, Q2: -9313.2783, TargetQ: -10051.3408 alpha: 1.3560\n",
      "Epoch 78, Total Reward: -20.1558, Q1: -7481.4082, Q2: -7482.9214, TargetQ: -7050.2363 alpha: 2.4176\n",
      "Epoch 79, Total Reward: -2.2264, Q1: -7939.8438, Q2: -7939.7402, TargetQ: -7964.9272 alpha: 1.4910\n",
      "Epoch 80, Total Reward: -15.6766, Q1: -7964.4346, Q2: -7973.8828, TargetQ: -7732.7710 alpha: 2.5986\n",
      "Epoch 81, Total Reward: -9.6605, Q1: -8291.8809, Q2: -8311.2031, TargetQ: -8161.3066 alpha: 2.5556\n",
      "Epoch 82, Total Reward: -4.3413, Q1: -8019.4731, Q2: -8031.2422, TargetQ: -8444.8350 alpha: 2.0831\n",
      "Epoch 83, Total Reward: -7.3685, Q1: -7488.7031, Q2: -7513.3765, TargetQ: -7175.9272 alpha: 2.4912\n",
      "Epoch 84, Total Reward: -249.8797, Q1: -8522.0195, Q2: -8537.2129, TargetQ: -9204.9648 alpha: -3.1048\n",
      "Epoch 85, Total Reward: -24.5765, Q1: -8264.1279, Q2: -8287.7705, TargetQ: -8214.8613 alpha: 2.9882\n",
      "Epoch 86, Total Reward: -15.9415, Q1: -8158.9678, Q2: -8171.1646, TargetQ: -8522.5352 alpha: 2.9882\n",
      "Epoch 87, Total Reward: -12.0723, Q1: -7889.1577, Q2: -7908.7617, TargetQ: -7446.7441 alpha: 2.9882\n",
      "Epoch 88, Total Reward: -10.2992, Q1: -7830.7002, Q2: -7831.2080, TargetQ: -8272.2305 alpha: 2.9851\n",
      "Epoch 89, Total Reward: -9.5329, Q1: -8111.3018, Q2: -8089.3037, TargetQ: -8215.5137 alpha: 2.9851\n",
      "Epoch 90, Total Reward: -9.1605, Q1: -8308.7773, Q2: -8322.3047, TargetQ: -7616.3276 alpha: 2.9821\n",
      "Epoch 91, Total Reward: -8.9642, Q1: -7458.1924, Q2: -7462.8076, TargetQ: -6622.9873 alpha: 2.9759\n",
      "Epoch 92, Total Reward: -9.0629, Q1: -7775.5371, Q2: -7769.7012, TargetQ: -7971.2217 alpha: 2.9974\n",
      "Epoch 93, Total Reward: -8.9767, Q1: -7977.4082, Q2: -7984.8911, TargetQ: -7687.6582 alpha: 2.9913\n",
      "Epoch 94, Total Reward: -7.7797, Q1: -7725.4180, Q2: -7727.6548, TargetQ: -8289.9668 alpha: 2.6661\n",
      "Epoch 95, Total Reward: -7.4892, Q1: -7824.8574, Q2: -7793.7744, TargetQ: -8103.8184 alpha: 2.6446\n",
      "Epoch 96, Total Reward: -7.1830, Q1: -8810.7520, Q2: -8810.0068, TargetQ: -9293.9707 alpha: 2.2059\n",
      "Epoch 97, Total Reward: -6.1179, Q1: -7294.9268, Q2: -7290.1143, TargetQ: -6726.6367 alpha: 2.1445\n",
      "Epoch 98, Total Reward: -5.3774, Q1: -7829.7773, Q2: -7818.6338, TargetQ: -7519.3237 alpha: 2.2795\n",
      "Epoch 99, Total Reward: -5.9574, Q1: -7964.6309, Q2: -7958.6143, TargetQ: -8930.8359 alpha: 2.4390\n",
      "Epoch 100, Total Reward: -5.5186, Q1: -7887.1772, Q2: -7901.7803, TargetQ: -7541.0596 alpha: 2.3439\n",
      "Epoch 101, Total Reward: -7.3956, Q1: -8007.3057, Q2: -8007.4746, TargetQ: -7937.8750 alpha: 2.6415\n",
      "Epoch 102, Total Reward: -5.4355, Q1: -7483.7324, Q2: -7491.7788, TargetQ: -6803.7607 alpha: 2.2764\n",
      "Epoch 103, Total Reward: -6.3042, Q1: -8407.3730, Q2: -8387.6914, TargetQ: -8318.8965 alpha: 2.5004\n",
      "Epoch 104, Total Reward: -6.0913, Q1: -7418.2974, Q2: -7420.4932, TargetQ: -7136.0029 alpha: 2.4666\n",
      "Epoch 105, Total Reward: -5.7486, Q1: -7714.4189, Q2: -7692.1016, TargetQ: -7214.7549 alpha: 2.3930\n",
      "Epoch 106, Total Reward: -6.2185, Q1: -7261.9658, Q2: -7286.2998, TargetQ: -7873.1055 alpha: 2.4881\n",
      "Epoch 107, Total Reward: -5.3166, Q1: -7969.3853, Q2: -7961.3994, TargetQ: -8173.9219 alpha: 2.2365\n",
      "Epoch 108, Total Reward: -5.5958, Q1: -8679.9785, Q2: -8684.6758, TargetQ: -8455.3223 alpha: 2.3654\n",
      "Epoch 109, Total Reward: -5.6100, Q1: -7736.2051, Q2: -7712.6855, TargetQ: -7318.1572 alpha: 2.3685\n",
      "Epoch 110, Total Reward: -6.5449, Q1: -8024.4639, Q2: -8025.5527, TargetQ: -8773.4023 alpha: 2.5249\n",
      "Epoch 111, Total Reward: -6.2501, Q1: -8502.2031, Q2: -8517.4482, TargetQ: -10217.4883 alpha: 2.4943\n",
      "Epoch 112, Total Reward: -5.3527, Q1: -7887.5981, Q2: -7855.5742, TargetQ: -7880.5830 alpha: 2.2703\n",
      "Epoch 113, Total Reward: -5.6000, Q1: -8277.9551, Q2: -8275.0234, TargetQ: -7832.5605 alpha: 2.3654\n",
      "Epoch 114, Total Reward: -5.3908, Q1: -8395.4043, Q2: -8396.1660, TargetQ: -9540.2061 alpha: 2.3163\n",
      "Epoch 115, Total Reward: -6.6682, Q1: -8469.5400, Q2: -8520.7793, TargetQ: -8377.3535 alpha: 2.5403\n",
      "Epoch 116, Total Reward: -6.3232, Q1: -8869.7168, Q2: -8876.0176, TargetQ: -9426.2461 alpha: 2.5035\n",
      "Epoch 117, Total Reward: -5.3549, Q1: -8063.2388, Q2: -8054.1362, TargetQ: -8875.4404 alpha: 2.2611\n",
      "Epoch 118, Total Reward: -5.4069, Q1: -8171.8550, Q2: -8180.7261, TargetQ: -8496.1406 alpha: 2.3194\n",
      "Epoch 119, Total Reward: -5.9147, Q1: -8247.2754, Q2: -8248.4951, TargetQ: -8107.1245 alpha: 2.4268\n",
      "Epoch 120, Total Reward: -6.1898, Q1: -7719.7393, Q2: -7728.6885, TargetQ: -6932.0654 alpha: 2.4758\n",
      "Epoch 121, Total Reward: -5.7864, Q1: -8062.8257, Q2: -8058.0703, TargetQ: -7750.5762 alpha: 2.4053\n",
      "Epoch 122, Total Reward: -5.7711, Q1: -8298.7100, Q2: -8275.8574, TargetQ: -8258.7656 alpha: 2.4022\n",
      "Epoch 123, Total Reward: -5.6737, Q1: -8802.2598, Q2: -8739.0488, TargetQ: -8676.4199 alpha: 2.3807\n",
      "Epoch 124, Total Reward: -5.7123, Q1: -8469.8555, Q2: -8506.0801, TargetQ: -8150.3506 alpha: 2.3899\n",
      "Epoch 125, Total Reward: -5.6979, Q1: -8181.0693, Q2: -8175.3687, TargetQ: -7722.8242 alpha: 2.3869\n",
      "Epoch 126, Total Reward: -5.7864, Q1: -8712.7480, Q2: -8722.2998, TargetQ: -8417.3359 alpha: 2.4053\n",
      "Epoch 127, Total Reward: -5.7708, Q1: -7667.6064, Q2: -7656.2119, TargetQ: -6755.2305 alpha: 2.4022\n",
      "Epoch 128, Total Reward: -5.7267, Q1: -8548.8174, Q2: -8538.0010, TargetQ: -8456.1230 alpha: 2.3930\n",
      "Epoch 129, Total Reward: -5.7266, Q1: -7927.1436, Q2: -7921.6963, TargetQ: -7781.3262 alpha: 2.3930\n",
      "Epoch 130, Total Reward: -5.7560, Q1: -7866.2539, Q2: -7864.8521, TargetQ: -6962.0317 alpha: 2.3991\n",
      "Epoch 131, Total Reward: -5.7559, Q1: -8424.6318, Q2: -8413.8730, TargetQ: -8834.8379 alpha: 2.3991\n",
      "Epoch 132, Total Reward: -5.7267, Q1: -7616.1743, Q2: -7606.4756, TargetQ: -7748.0376 alpha: 2.3930\n",
      "Epoch 133, Total Reward: -5.7412, Q1: -7617.9893, Q2: -7578.0146, TargetQ: -8088.2002 alpha: 2.3961\n",
      "Epoch 134, Total Reward: -5.7559, Q1: -7565.5171, Q2: -7545.1108, TargetQ: -7575.0527 alpha: 2.3991\n",
      "Epoch 135, Total Reward: -5.7412, Q1: -7868.7739, Q2: -7847.2930, TargetQ: -8098.7603 alpha: 2.3961\n",
      "Epoch 136, Total Reward: -5.7412, Q1: -8259.5010, Q2: -8216.3232, TargetQ: -9290.3916 alpha: 2.3961\n",
      "Epoch 137, Total Reward: -5.7412, Q1: -8075.3857, Q2: -8048.5186, TargetQ: -8772.2090 alpha: 2.3961\n",
      "Epoch 138, Total Reward: -5.7560, Q1: -7438.7178, Q2: -7432.3306, TargetQ: -7382.1475 alpha: 2.3991\n",
      "Epoch 139, Total Reward: -5.7412, Q1: -8230.3145, Q2: -8225.0820, TargetQ: -8971.6299 alpha: 2.3961\n",
      "Epoch 140, Total Reward: -5.7412, Q1: -8436.2949, Q2: -8444.0449, TargetQ: -9663.6641 alpha: 2.3961\n",
      "Epoch 141, Total Reward: -5.7412, Q1: -8134.6572, Q2: -8116.7129, TargetQ: -7609.8252 alpha: 2.3961\n",
      "Epoch 142, Total Reward: -5.7559, Q1: -8303.4434, Q2: -8342.1436, TargetQ: -7690.8555 alpha: 2.3991\n",
      "Epoch 143, Total Reward: -5.7412, Q1: -7755.5938, Q2: -7808.3613, TargetQ: -6980.3438 alpha: 2.3961\n",
      "Epoch 144, Total Reward: -5.7412, Q1: -8891.9590, Q2: -8931.2578, TargetQ: -8221.4941 alpha: 2.3961\n",
      "Epoch 145, Total Reward: -5.7412, Q1: -8156.3467, Q2: -8196.0918, TargetQ: -8677.6426 alpha: 2.3961\n",
      "Epoch 146, Total Reward: -5.7412, Q1: -8991.6445, Q2: -9019.9121, TargetQ: -8911.4004 alpha: 2.3961\n",
      "Epoch 147, Total Reward: -5.7413, Q1: -8259.2422, Q2: -8281.7090, TargetQ: -7683.6729 alpha: 2.3961\n",
      "Epoch 148, Total Reward: -5.9284, Q1: -8111.3018, Q2: -8120.6606, TargetQ: -7589.2588 alpha: 2.4329\n",
      "Epoch 149, Total Reward: -5.5329, Q1: -8377.0391, Q2: -8417.0176, TargetQ: -7900.9141 alpha: 2.3470\n",
      "Epoch 150, Total Reward: -5.8506, Q1: -7408.4912, Q2: -7443.2744, TargetQ: -7112.9219 alpha: 2.4176\n",
      "Epoch 151, Total Reward: -5.6557, Q1: -7317.3354, Q2: -7348.7388, TargetQ: -6714.1406 alpha: 2.3777\n",
      "Epoch 152, Total Reward: -5.8646, Q1: -8056.6714, Q2: -8068.0088, TargetQ: -8764.6309 alpha: 2.4206\n",
      "Epoch 153, Total Reward: -5.7126, Q1: -8230.5352, Q2: -8252.5420, TargetQ: -9254.9805 alpha: 2.3899\n",
      "Epoch 154, Total Reward: -5.7267, Q1: -7887.6035, Q2: -7884.8394, TargetQ: -7690.7549 alpha: 2.3930\n",
      "Epoch 155, Total Reward: -5.7412, Q1: -7747.6436, Q2: -7752.1309, TargetQ: -7199.4829 alpha: 2.3961\n",
      "Epoch 156, Total Reward: -5.7709, Q1: -7580.6924, Q2: -7589.2393, TargetQ: -9212.0254 alpha: 2.4022\n",
      "Epoch 157, Total Reward: -5.7412, Q1: -8040.2305, Q2: -8022.1660, TargetQ: -8095.0669 alpha: 2.3961\n",
      "Epoch 158, Total Reward: -5.7266, Q1: -8294.9541, Q2: -8287.6113, TargetQ: -8701.2559 alpha: 2.3930\n",
      "Epoch 159, Total Reward: -5.7412, Q1: -8370.1650, Q2: -8339.9277, TargetQ: -8819.8516 alpha: 2.3961\n",
      "Epoch 160, Total Reward: -5.7560, Q1: -7754.3701, Q2: -7768.5859, TargetQ: -7547.9189 alpha: 2.3991\n",
      "Epoch 161, Total Reward: -5.7412, Q1: -8351.8418, Q2: -8341.3496, TargetQ: -8546.0117 alpha: 2.3961\n",
      "Epoch 162, Total Reward: -5.7412, Q1: -8153.4321, Q2: -8152.4624, TargetQ: -8148.4629 alpha: 2.3961\n",
      "Epoch 163, Total Reward: -5.7412, Q1: -8260.4199, Q2: -8239.5918, TargetQ: -9224.2656 alpha: 2.3961\n",
      "Epoch 164, Total Reward: -5.7412, Q1: -7940.2783, Q2: -7933.1025, TargetQ: -7879.2749 alpha: 2.3961\n",
      "Epoch 165, Total Reward: -5.7412, Q1: -8148.0498, Q2: -8163.2510, TargetQ: -8253.9141 alpha: 2.3961\n",
      "Epoch 166, Total Reward: -5.7560, Q1: -8264.7744, Q2: -8293.7529, TargetQ: -8047.0234 alpha: 2.3991\n",
      "Epoch 167, Total Reward: -5.7266, Q1: -8635.8105, Q2: -8685.9463, TargetQ: -8386.1650 alpha: 2.3930\n",
      "Epoch 168, Total Reward: -5.7560, Q1: -8064.1553, Q2: -8075.0381, TargetQ: -7526.8643 alpha: 2.3991\n",
      "Epoch 169, Total Reward: -5.7560, Q1: -7736.5449, Q2: -7743.0063, TargetQ: -8163.8633 alpha: 2.3991\n",
      "Epoch 170, Total Reward: -5.7412, Q1: -8330.4531, Q2: -8382.4082, TargetQ: -8580.6719 alpha: 2.3961\n",
      "Epoch 171, Total Reward: -5.7412, Q1: -7850.6689, Q2: -7866.4478, TargetQ: -7722.3076 alpha: 2.3961\n",
      "Epoch 172, Total Reward: -5.7559, Q1: -8211.3965, Q2: -8206.4980, TargetQ: -8489.0020 alpha: 2.3991\n",
      "Epoch 173, Total Reward: -5.7412, Q1: -7625.5859, Q2: -7630.2295, TargetQ: -6813.3423 alpha: 2.3961\n",
      "Epoch 174, Total Reward: -5.7412, Q1: -8384.3633, Q2: -8400.0488, TargetQ: -8557.0820 alpha: 2.3961\n",
      "Epoch 175, Total Reward: -5.7412, Q1: -8327.4707, Q2: -8348.2686, TargetQ: -7874.7373 alpha: 2.3961\n",
      "Epoch 176, Total Reward: -5.7412, Q1: -8294.4824, Q2: -8298.9297, TargetQ: -7509.2388 alpha: 2.3961\n",
      "Epoch 177, Total Reward: -5.7412, Q1: -8033.7959, Q2: -8040.4141, TargetQ: -8045.8301 alpha: 2.3961\n",
      "Epoch 178, Total Reward: -5.7412, Q1: -7746.2441, Q2: -7719.2085, TargetQ: -7267.6909 alpha: 2.3961\n",
      "Epoch 179, Total Reward: -5.7412, Q1: -7772.4434, Q2: -7761.0869, TargetQ: -7696.9219 alpha: 2.3961\n",
      "Epoch 180, Total Reward: -5.7412, Q1: -7725.1235, Q2: -7729.4170, TargetQ: -7366.2627 alpha: 2.3961\n",
      "Epoch 181, Total Reward: -5.7412, Q1: -8060.3652, Q2: -8034.2168, TargetQ: -8506.5801 alpha: 2.3961\n",
      "Epoch 182, Total Reward: -5.7412, Q1: -7719.0859, Q2: -7699.0571, TargetQ: -7427.5039 alpha: 2.3961\n",
      "Epoch 183, Total Reward: -5.7412, Q1: -7455.6562, Q2: -7450.8135, TargetQ: -6979.1514 alpha: 2.3961\n",
      "Epoch 184, Total Reward: -5.7412, Q1: -7700.7007, Q2: -7693.7588, TargetQ: -8117.6548 alpha: 2.3961\n",
      "Epoch 185, Total Reward: -5.7412, Q1: -7626.2036, Q2: -7598.9248, TargetQ: -7835.3901 alpha: 2.3961\n",
      "Epoch 186, Total Reward: -5.7413, Q1: -7424.2920, Q2: -7406.5352, TargetQ: -7636.1133 alpha: 2.3961\n",
      "Epoch 187, Total Reward: -5.8476, Q1: -7615.0889, Q2: -7590.2061, TargetQ: -8258.3730 alpha: 2.4176\n",
      "Epoch 188, Total Reward: -5.5793, Q1: -7339.6533, Q2: -7349.6582, TargetQ: -7112.9395 alpha: 2.3593\n",
      "Epoch 189, Total Reward: -5.6827, Q1: -7800.3779, Q2: -7796.3955, TargetQ: -8327.4922 alpha: 2.3838\n",
      "Epoch 190, Total Reward: -5.9669, Q1: -7793.5186, Q2: -7788.4023, TargetQ: -8244.4600 alpha: 2.4390\n",
      "Epoch 191, Total Reward: -5.9732, Q1: -7530.2168, Q2: -7524.8667, TargetQ: -7523.7593 alpha: 2.4421\n",
      "Epoch 192, Total Reward: -5.4251, Q1: -7672.9185, Q2: -7687.0703, TargetQ: -7964.4990 alpha: 2.3132\n",
      "Epoch 193, Total Reward: -5.9196, Q1: -7282.6099, Q2: -7293.8789, TargetQ: -7388.1494 alpha: 2.4298\n",
      "Epoch 194, Total Reward: -5.3596, Q1: -8055.7583, Q2: -8041.1416, TargetQ: -8180.3750 alpha: 2.3010\n",
      "Epoch 195, Total Reward: -6.4579, Q1: -7752.6182, Q2: -7747.7578, TargetQ: -7889.3677 alpha: 2.5127\n",
      "Epoch 196, Total Reward: -5.6233, Q1: -8113.2100, Q2: -8115.1963, TargetQ: -8781.3672 alpha: 2.3685\n",
      "Epoch 197, Total Reward: -5.8310, Q1: -7540.9189, Q2: -7569.8408, TargetQ: -7181.5405 alpha: 2.4145\n",
      "Epoch 198, Total Reward: -5.5915, Q1: -7712.1777, Q2: -7715.7979, TargetQ: -8300.3164 alpha: 2.3623\n",
      "Epoch 199, Total Reward: -5.6829, Q1: -8205.2256, Q2: -8187.1660, TargetQ: -8211.2314 alpha: 2.3838\n",
      "Epoch 200, Total Reward: -6.0349, Q1: -8123.2256, Q2: -8096.5366, TargetQ: -8303.4414 alpha: 2.4513\n",
      "Epoch 201, Total Reward: -5.5817, Q1: -7851.8223, Q2: -7855.5830, TargetQ: -7976.7021 alpha: 2.3593\n",
      "Epoch 202, Total Reward: -5.8644, Q1: -8015.2227, Q2: -7993.1694, TargetQ: -7703.7617 alpha: 2.4206\n",
      "Epoch 203, Total Reward: -5.5896, Q1: -8471.7520, Q2: -8455.9746, TargetQ: -8752.6172 alpha: 2.3623\n",
      "Epoch 204, Total Reward: -5.7266, Q1: -7714.0576, Q2: -7683.7173, TargetQ: -7076.9707 alpha: 2.3930\n",
      "Epoch 205, Total Reward: -6.0169, Q1: -7983.0850, Q2: -7965.8696, TargetQ: -8313.2109 alpha: 2.4482\n",
      "Epoch 206, Total Reward: -5.5949, Q1: -7822.6318, Q2: -7812.9082, TargetQ: -7858.9692 alpha: 2.3623\n",
      "Epoch 207, Total Reward: -5.8017, Q1: -8102.2246, Q2: -8101.0186, TargetQ: -8204.8457 alpha: 2.4083\n",
      "Epoch 208, Total Reward: -5.5375, Q1: -7918.5869, Q2: -7898.9126, TargetQ: -7364.0029 alpha: 2.3501\n",
      "Epoch 209, Total Reward: -6.0564, Q1: -7547.5107, Q2: -7557.4731, TargetQ: -6950.0469 alpha: 2.4544\n",
      "Epoch 210, Total Reward: -5.6584, Q1: -7997.8301, Q2: -7979.4639, TargetQ: -7642.1118 alpha: 2.3777\n",
      "Epoch 211, Total Reward: -5.7856, Q1: -8050.0996, Q2: -8068.7402, TargetQ: -7890.9170 alpha: 2.4053\n",
      "Epoch 212, Total Reward: -5.6700, Q1: -8662.9561, Q2: -8658.2656, TargetQ: -8797.8506 alpha: 2.3807\n",
      "Epoch 213, Total Reward: -5.7560, Q1: -8311.8105, Q2: -8304.5488, TargetQ: -8100.7656 alpha: 2.3991\n",
      "Epoch 214, Total Reward: -5.7861, Q1: -7997.6182, Q2: -7979.8052, TargetQ: -8407.2754 alpha: 2.4053\n",
      "Epoch 215, Total Reward: -5.7267, Q1: -7992.4150, Q2: -7990.1396, TargetQ: -8041.5879 alpha: 2.3930\n",
      "\n",
      "Stopping (Ctrl+C). Saving…\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "history = []\n",
    "policy_delay = 2  # Delayed policy updates\n",
    "step = 0\n",
    "total_reward = 0.0\n",
    "frequency = 500  # Hz\n",
    "state_theta_dot = np.array([0,0], dtype=np.float64)\n",
    "state_alpha_dot = np.array([0,0], dtype=np.float64)\n",
    "try: \n",
    "    with QubeServo3(hardware = 1, pendulum = 1, frequency=10) as board:\n",
    "        while True: \n",
    "            avg_q1, avg_q2, avg_target_q = 0.0, 0.0, 0.0\n",
    "            step += 1 \n",
    "            board.read_outputs()\n",
    "            theta = board.motorPosition * -1\n",
    "            alpha = board.pendulumPosition \n",
    "            theta = np.clip(theta, -np.pi/2, np.pi/2)\n",
    "            alpha = np.mod(alpha, 2*np.pi) - np.pi\n",
    "\n",
    "            theta_dot, state_theta_dot = ddt_filter(theta, state_theta_dot, 50, 1/frequency)\n",
    "            alpha_dot, state_alpha_dot = ddt_filter(alpha, state_alpha_dot, 100, 1/frequency)\n",
    "\n",
    "            state = np.array([theta, theta_dot, alpha, alpha_dot], dtype=np.float32)\n",
    "\n",
    "            action = actor_model(tf.convert_to_tensor([state], dtype=tf.float32)).numpy()[0]\n",
    "            action = action + np.random.normal(0, 0.1, size=action_size)  # Add exploration noise\n",
    "            action = np.clip(action, -2.0, 2.0) \n",
    "            board.write_voltage(action)\n",
    "\n",
    "            board.read_outputs()\n",
    "            next_theta = board.motorPosition * -1\n",
    "            next_alpha = board.pendulumPosition\n",
    "            next_alpha = np.mod(next_alpha, 2*np.pi) - np.pi\n",
    "            next_theta = np.clip(next_theta, -np.pi/2, np.pi/2)\n",
    "            next_theta_dot, state_theta_dot = ddt_filter(next_theta, state_theta_dot, 50, 1/frequency)\n",
    "            next_alpha_dot, state_alpha_dot = ddt_filter(next_alpha, state_alpha_dot, 100, 1/frequency)\n",
    "            next_state = np.array([next_theta, next_theta_dot, next_alpha, next_alpha_dot], dtype=np.float32)\n",
    "\n",
    "\n",
    "            # wrapped_alpha = ((alpha - np.pi + np.pi) % (2*np.pi)) - np.pi\n",
    "            reward = -(alpha**2 + 0.0001*alpha_dot**2 + 0.001*action**2)\n",
    "            total_reward += reward\n",
    "\n",
    "            replay_buffer.store(state, action, reward, next_state, False)\n",
    "            state = next_state\n",
    "\n",
    "            if replay_buffer.size() >= batch_size:\n",
    "                states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "                states      = tf.convert_to_tensor(states, dtype=tf.float32)\n",
    "                actions     = tf.convert_to_tensor(actions.reshape(-1,1), dtype=tf.float32)\n",
    "                rewards     = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
    "                next_states = tf.convert_to_tensor(next_states, dtype=tf.float32)\n",
    "                dones       = tf.convert_to_tensor(dones, dtype=tf.float32) \n",
    "\n",
    "                # add clipped noise to target action\n",
    "                noise = np.clip(np.random.normal(0, 0.2, size=actions.shape), -0.5, 0.5)\n",
    "                next_actions = target_actor(next_states) + noise\n",
    "                next_actions = tf.clip_by_value(next_actions, -2.0, 2.0)  # Pendulum action bounds\n",
    "\n",
    "                # Compute target Q-values with both critics\n",
    "                target1 = tf.squeeze(target_critic1([next_states, next_actions]), axis=1)\n",
    "                target2 = tf.squeeze(target_critic2([next_states, next_actions]), axis=1)\n",
    "                target_q = rewards + gamma * (1 - dones) * tf.minimum(target1, target2)\n",
    "\n",
    "                with tf.GradientTape() as tape_critic1, tf.GradientTape() as tape_critic2:\n",
    "                    q1 = critic_model1([states, actions], training=True)\n",
    "                    q2 = critic_model2([states, actions], training=True)\n",
    "\n",
    "                    # Compute losses\n",
    "                    loss1 = tf.keras.losses.MSE(target_q, q1)\n",
    "                    loss2 = tf.keras.losses.MSE(target_q, q2)\n",
    "\n",
    "                avg_q1 = tf.reduce_mean(q1).numpy().item()\n",
    "                avg_q2 = tf.reduce_mean(q2).numpy().item()\n",
    "                avg_target_q = tf.reduce_mean(target_q).numpy().item()\n",
    "\n",
    "                # Get gradients for each critic once\n",
    "                critic_grad1 = tape_critic1.gradient(loss1, critic_model1.trainable_variables)\n",
    "                critic_grad2 = tape_critic2.gradient(loss2, critic_model2.trainable_variables)\n",
    "\n",
    "                # Apply gradients\n",
    "                critic_optimizer1.apply_gradients(zip(critic_grad1, critic_model1.trainable_variables))\n",
    "                critic_optimizer2.apply_gradients(zip(critic_grad2, critic_model2.trainable_variables))\n",
    "                if step % policy_delay == 0:  # Delayed policy updates\n",
    "                    with tf.GradientTape() as tape_actor: \n",
    "                        action = actor_model(states)\n",
    "                        actor_loss = -tf.reduce_mean(critic_model1([states, action]))\n",
    "\n",
    "                    actor_grad = tape_actor.gradient(actor_loss, actor_model.trainable_variables)\n",
    "                    actor_optimizer.apply_gradients(zip(actor_grad, actor_model.trainable_variables))\n",
    "\n",
    "                    soft_update(target_actor.variables, actor_model.variables, tau=0.005)\n",
    "                    soft_update(target_critic1.variables, critic_model1.variables, tau=0.005)\n",
    "                    soft_update(target_critic2.variables, critic_model2.variables, tau=0.005)\n",
    "            history.append(total_reward)\n",
    "            if step % 1 == 0:\n",
    "                print(f\"Epoch {step}, Total Reward: {float(reward):.4f}, \"\n",
    "                f\"Q1: {avg_q1:.4f}, Q2: {avg_q2:.4f}, TargetQ: {avg_target_q:.4f}\", \n",
    "                f\"alpha: {alpha:.4f}\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nStopping (Ctrl+C). Saving…\")\n",
    "finally:\n",
    "    # save weights (use .save_weights if you prefer checkpoint style)\n",
    "    actor_model.save_weights(\"saves/quanser/actor_model.weights.h5\")\n",
    "    critic_model1.save_weights(\"saves/quanser/critic_model1.weights.h5\")\n",
    "    critic_model2.save_weights(\"saves/quanser/critic_model2.weights.h5\")\n",
    "    ckpt = tf.train.Checkpoint(actor_optimizer=actor_optimizer,\n",
    "                           critic_optimizer1=critic_optimizer1,\n",
    "                           critic_optimizer2=critic_optimizer2)\n",
    "    ckpt.save(\"saves/quanser/optimizers_ckpt/ckpt\")\n",
    "    print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fd84a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stopping (Ctrl+C). Saving…\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# batch_size = 32\n",
    "# history = []\n",
    "# policy_delay = 2  # Delayed policy updates\n",
    "# step = 0\n",
    "# total_reward = 0.0\n",
    "\n",
    "# try:\n",
    "#     total_reward = 0.0\n",
    "#     while True:\n",
    "#         step += 1\n",
    "    \n",
    "#         # 1) read state\n",
    "#         board.read_encoder(encoder_channels, len(encoder_channels), counts)\n",
    "#         theta_arm  = counts[0] * ARM_RAD_PER_COUNT\n",
    "#         theta_pend = counts[1] * PEND_RAD_PER_COUNT\n",
    "#         theta_arm_dot  = (theta_arm  - theta_arm_prev)  / dt\n",
    "#         theta_pend_dot = (theta_pend - theta_pend_prev) / dt\n",
    "#         state = np.array([theta_arm, theta_pend, theta_arm_dot, theta_pend_dot], dtype=np.float32)\n",
    "\n",
    "#         # 2) select action\n",
    "#         action_vec = actor_model(tf.convert_to_tensor([state], dtype=tf.float32)).numpy()[0]\n",
    "#         action_val = float(np.clip(action_vec[0], -2.0, 2.0))  # scalar in [-2,2]; tune to your safe V range\n",
    "\n",
    "#         # 3) apply action (analog write wants numpy float64 buffer)\n",
    "#         voltages = np.array([action_val], dtype=np.float64)\n",
    "#         board.write_analog(motor_channels, len(motor_channels), voltages)\n",
    "\n",
    "#         # 4) get next_state after action\n",
    "#         time.sleep(dt)  # maintain loop timing around the actuation\n",
    "#         board.read_encoder(encoder_channels, len(encoder_channels), counts)\n",
    "#         next_theta_arm  = counts[0] * ARM_RAD_PER_COUNT\n",
    "#         next_theta_pend = counts[1] * PEND_RAD_PER_COUNT\n",
    "#         next_theta_arm_dot  = (next_theta_arm  - theta_arm)  / dt\n",
    "#         next_theta_pend_dot = (next_theta_pend - theta_pend) / dt\n",
    "#         next_state = np.array([next_theta_arm, next_theta_pend, next_theta_arm_dot, next_theta_pend_dot], dtype=np.float32)\n",
    "\n",
    "#         # 5) reward (example: upright pendulum, gentle motion)\n",
    "#         reward = - ( (np.angle(np.exp(1j*(next_theta_pend - np.pi))))**2\n",
    "#                      + 0.1*next_theta_pend_dot**2 + 0.01*action_val**2 )\n",
    "#         total_reward += reward\n",
    "\n",
    "#         # 6) store\n",
    "#         replay_buffer.store(state, action_val, reward, next_state, False)\n",
    "\n",
    "#         # 7) train if enough samples\n",
    "#         if replay_buffer.size() >= batch_size:\n",
    "#             states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "#             states      = tf.convert_to_tensor(states, dtype=tf.float32)\n",
    "#             actions     = tf.convert_to_tensor(actions.reshape(-1,1), dtype=tf.float32)\n",
    "#             rewards     = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
    "#             next_states = tf.convert_to_tensor(next_states, dtype=tf.float32)\n",
    "#             dones       = tf.convert_to_tensor(dones, dtype=tf.float32)\n",
    "\n",
    "#             # target policy smoothing\n",
    "#             noise = np.clip(np.random.normal(0, 0.2, size=(actions.shape[0], 1)), -0.5, 0.5)\n",
    "#             target_act = tf.clip_by_value(target_actor(next_states) + noise, -2.0, 2.0)\n",
    "\n",
    "#             # twin critics target\n",
    "#             t1 = tf.squeeze(target_critic1([next_states, target_act]), axis=1)\n",
    "#             t2 = tf.squeeze(target_critic2([next_states, target_act]), axis=1)\n",
    "#             target_q = rewards + gamma * (1.0 - dones) * tf.minimum(t1, t2)\n",
    "\n",
    "#             # critic updates\n",
    "#             with tf.GradientTape() as tape1, tf.GradientTape() as tape2:\n",
    "#                 q1 = tf.squeeze(critic_model1([states, actions]), axis=1)\n",
    "#                 q2 = tf.squeeze(critic_model2([states, actions]), axis=1)\n",
    "#                 loss1 = tf.keras.losses.MSE(target_q, q1)\n",
    "#                 loss2 = tf.keras.losses.MSE(target_q, q2)\n",
    "#             critic_optimizer1.apply_gradients(zip(tape1.gradient(loss1, critic_model1.trainable_variables),\n",
    "#                                                   critic_model1.trainable_variables))\n",
    "#             critic_optimizer2.apply_gradients(zip(tape2.gradient(loss2, critic_model2.trainable_variables),\n",
    "#                                                   critic_model2.trainable_variables))\n",
    "\n",
    "#             # delayed actor + target updates\n",
    "#             if step % policy_delay == 0:\n",
    "#                 with tf.GradientTape() as tape_actor:\n",
    "#                     pi = actor_model(states)\n",
    "#                     actor_loss = -tf.reduce_mean(critic_model1([states, pi]))\n",
    "#                 actor_optimizer.apply_gradients(zip(tape_actor.gradient(actor_loss, actor_model.trainable_variables),\n",
    "#                                                     actor_model.trainable_variables))\n",
    "#                 soft_update(target_actor.variables,   actor_model.variables,   tau=0.005)\n",
    "#                 soft_update(target_critic1.variables, critic_model1.variables, tau=0.005)\n",
    "#                 soft_update(target_critic2.variables, critic_model2.variables, tau=0.005)\n",
    "\n",
    "#         if step % 100 == 0:\n",
    "#             print(f\"Step {step}  reward_sum: {total_reward:.2f}\")\n",
    "\n",
    "#         # update prev angles for next derivative\n",
    "#         theta_arm_prev, theta_pend_prev = next_theta_arm, next_theta_pend\n",
    "\n",
    "# except KeyboardInterrupt:\n",
    "#     print(\"\\nStopping (Ctrl+C). Saving…\")\n",
    "# finally:\n",
    "#     # save weights (use .save_weights if you prefer checkpoint style)\n",
    "#     actor_model.save_weights(\"saves/quanser/actor_model.weights.h5\")\n",
    "#     critic_model1.save_weights(\"saves/quanser/critic_model1.weights.h5\")\n",
    "#     critic_model2.save_weights(\"saves/quanser/critic_model2.weights.h5\")\n",
    "#     # set motor to 0V and close safely\n",
    "#     board.write_analog(motor_channels, 1, np.array([0.0], dtype=np.float64))\n",
    "#     board.close()\n",
    "#     print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff966c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ntk00\\AppData\\Local\\Temp\\ipykernel_20244\\4268336479.py:10: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  u = float(action)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "a bytes-like object is required, not 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[70], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m u \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(action)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# send to motor\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[43mboard\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_analog\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmotor_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mu\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(dt)  \u001b[38;5;66;03m# ~0.01s\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ntk00\\anaconda3\\envs\\neural_network\\lib\\site-packages\\quanser\\hardware\\hil.py:2732\u001b[0m, in \u001b[0;36mHIL.write_analog\u001b[1;34m(self, channels, num_channels, buffer)\u001b[0m\n\u001b[0;32m   2675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrite_analog\u001b[39m(\u001b[38;5;28mself\u001b[39m, channels, num_channels, buffer):\n\u001b[0;32m   2676\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Writes to analog outputs immediately. The function does not return until the data has been written.\u001b[39;00m\n\u001b[0;32m   2677\u001b[0m \n\u001b[0;32m   2678\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2727\u001b[0m \n\u001b[0;32m   2728\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   2729\u001b[0m     result \u001b[38;5;241m=\u001b[39m hil_lib\u001b[38;5;241m.\u001b[39mhil_write_analog(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_card \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_card \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m ffi\u001b[38;5;241m.\u001b[39mNULL,\n\u001b[0;32m   2730\u001b[0m                                       ffi\u001b[38;5;241m.\u001b[39mfrom_buffer(_UINT32_ARRAY, channels) \u001b[38;5;28;01mif\u001b[39;00m channels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m ffi\u001b[38;5;241m.\u001b[39mNULL,\n\u001b[0;32m   2731\u001b[0m                                       num_channels,\n\u001b[1;32m-> 2732\u001b[0m                                       \u001b[43mffi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_buffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_DOUBLE_ARRAY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m ffi\u001b[38;5;241m.\u001b[39mNULL)\n\u001b[0;32m   2733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   2734\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m HILError(result)\n",
      "File \u001b[1;32mc:\\Users\\ntk00\\anaconda3\\envs\\neural_network\\lib\\site-packages\\cffi\\api.py:365\u001b[0m, in \u001b[0;36mFFI.from_buffer\u001b[1;34m(self, cdecl, python_buffer, require_writable)\u001b[0m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(cdecl, basestring):\n\u001b[0;32m    364\u001b[0m     cdecl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_typeof(cdecl)\n\u001b[1;32m--> 365\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_buffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcdecl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpython_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    366\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mrequire_writable\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: a bytes-like object is required, not 'list'"
     ]
    }
   ],
   "source": [
    "# while True:\n",
    "#     # read state\n",
    "#     # board.close()\n",
    "#     board.read_encoder(encoder_channels, len(encoder_channels), counts)\n",
    "#     theta_arm = counts[0] * ARM_RAD_PER_COUNT\n",
    "#     theta_pend = counts[1] * PEND_RAD_PER_COUNT\n",
    "\n",
    "#     # compute action from policy\n",
    "#     action = actor_model(tf.convert_to_tensor([state], dtype=tf.float32)).numpy()[0]\n",
    "#     u = float(action)\n",
    "\n",
    "#     # send to motor\n",
    "#     board.write_analog(motor_channels, 1, [u])\n",
    "\n",
    "#     time.sleep(dt)  # ~0.01s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e598aa56",
   "metadata": {},
   "outputs": [
    {
     "ename": "HILError",
     "evalue": "-1410",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHILError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcollections\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m deque\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# --- HIL/QUBE setup ---\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# board.close()\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m board \u001b[38;5;241m=\u001b[39m \u001b[43mHIL\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mqube_servo3_usb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m encoder_channels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39muint32)\n\u001b[0;32m     10\u001b[0m motor_channels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39muint32)\n",
      "File \u001b[1;32mc:\\Users\\ntk00\\anaconda3\\envs\\neural_network\\lib\\site-packages\\quanser\\hardware\\hil.py:663\u001b[0m, in \u001b[0;36mHIL.__init__\u001b[1;34m(self, card_type, card_identifier)\u001b[0m\n\u001b[0;32m    661\u001b[0m \u001b[38;5;66;03m# If non-default arguments are passed, attempt to open the card.\u001b[39;00m\n\u001b[0;32m    662\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m card_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 663\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcard_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcard_identifier\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ntk00\\anaconda3\\envs\\neural_network\\lib\\site-packages\\quanser\\hardware\\hil.py:774\u001b[0m, in \u001b[0;36mHIL.open\u001b[1;34m(self, card_type, card_identifier)\u001b[0m\n\u001b[0;32m    771\u001b[0m     result \u001b[38;5;241m=\u001b[39m hil_lib\u001b[38;5;241m.\u001b[39mhil_open(card_type\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUTF-8\u001b[39m\u001b[38;5;124m'\u001b[39m), card_identifier\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUTF-8\u001b[39m\u001b[38;5;124m'\u001b[39m), card)\n\u001b[0;32m    773\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 774\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HILError(result)\n\u001b[0;32m    776\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_card \u001b[38;5;241m=\u001b[39m card[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mHILError\u001b[0m: -1410"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "\n",
    "# --- HIL/QUBE setup ---\n",
    "board.close()\n",
    "board = HIL(\"qube_servo3_usb\", \"0\")\n",
    "encoder_channels = np.array([0, 1], dtype=np.uint32)\n",
    "motor_channels = np.array([0], dtype=np.uint32)\n",
    "counts = np.zeros(2, dtype=np.int32)\n",
    "\n",
    "ENCODER_RES = 2048\n",
    "ARM_RAD_PER_COUNT = 2*np.pi / ENCODER_RES\n",
    "PEND_RAD_PER_COUNT = 2*np.pi / ENCODER_RES\n",
    "dt = 0.01  # 10 ms loop\n",
    "\n",
    "# --- Replay Buffer ---\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=100000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    def store(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    def sample(self, batch_size):\n",
    "        batch = np.array(random.sample(self.buffer, batch_size))\n",
    "        states, actions, rewards, next_states, dones = map(np.stack, zip(*batch))\n",
    "        return states, actions, rewards, next_states, dones\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "replay_buffer = ReplayBuffer()\n",
    "\n",
    "# --- Soft update ---\n",
    "def soft_update(target_weights, online_weights, tau=0.005):\n",
    "    for (target, online) in zip(target_weights, online_weights):\n",
    "        target.assign(target * (1 - tau) + online * tau)\n",
    "\n",
    "# --- TD3 models already defined: actor_model, critic_model1, critic_model2, \n",
    "# target_actor, target_critic1, target_critic2\n",
    "# optimizers: actor_optimizer, critic_optimizer1, critic_optimizer2\n",
    "\n",
    "state_size = 4\n",
    "action_size = 1\n",
    "gamma = 0.99\n",
    "batch_size = 32\n",
    "policy_delay = 2\n",
    "step = 0\n",
    "\n",
    "theta_arm_prev = 0.0\n",
    "theta_pend_prev = 0.0\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        step += 1\n",
    "\n",
    "        # --- 1. Read state ---\n",
    "        board.read_encoder(encoder_channels, len(encoder_channels), counts)\n",
    "        theta_arm = counts[0] * ARM_RAD_PER_COUNT\n",
    "        theta_pend = counts[1] * PEND_RAD_PER_COUNT\n",
    "\n",
    "        theta_arm_dot = (theta_arm - theta_arm_prev) / dt\n",
    "        theta_pend_dot = (theta_pend - theta_pend_prev) / dt\n",
    "        theta_arm_prev, theta_pend_prev = theta_arm, theta_pend\n",
    "\n",
    "        state = np.array([theta_arm, theta_pend, theta_arm_dot, theta_pend_dot], dtype=np.float32)\n",
    "\n",
    "        # --- 2. Compute action ---\n",
    "        action = actor_model(tf.convert_to_tensor([state], dtype=tf.float32)).numpy()[0]\n",
    "        u_array = np.array([float(action)], dtype=np.float64)\n",
    "\n",
    "        # --- 3. Apply action ---\n",
    "        board.write_analog(motor_channels, 1, u_array)\n",
    "\n",
    "        # --- 4. Read next state ---\n",
    "        board.read_encoder(encoder_channels, len(encoder_channels), counts)\n",
    "        next_theta_arm = counts[0] * ARM_RAD_PER_COUNT\n",
    "        next_theta_pend = counts[1] * PEND_RAD_PER_COUNT\n",
    "        next_theta_arm_dot = (next_theta_arm - theta_arm) / dt\n",
    "        next_theta_pend_dot = (next_theta_pend - theta_pend) / dt\n",
    "        next_state = np.array([next_theta_arm, next_theta_pend, next_theta_arm_dot, next_theta_pend_dot], dtype=np.float32)\n",
    "\n",
    "        # --- 5. Compute reward ---\n",
    "        reward = - (next_theta_pend**2 + 0.1 * next_theta_pend_dot**2)\n",
    "\n",
    "        # --- 6. Store transition ---\n",
    "        replay_buffer.store(state, action, reward, next_state, False)\n",
    "\n",
    "        # --- 7. Train TD3 ---\n",
    "        if replay_buffer.size() >= batch_size:\n",
    "            states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "            states = tf.convert_to_tensor(states, dtype=tf.float32)\n",
    "            actions = tf.convert_to_tensor(actions, dtype=tf.float32)\n",
    "            rewards = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
    "            next_states = tf.convert_to_tensor(next_states, dtype=tf.float32)\n",
    "            dones = tf.convert_to_tensor(dones, dtype=tf.float32)\n",
    "\n",
    "            # Target actions with clipped noise\n",
    "            noise = np.clip(np.random.normal(0, 0.2, size=actions.shape), -0.5, 0.5)\n",
    "            next_actions = tf.clip_by_value(target_actor(next_states) + noise, -2.0, 2.0)\n",
    "\n",
    "            # Target Q-values\n",
    "            target1 = tf.squeeze(target_critic1([next_states, next_actions]), axis=1)\n",
    "            target2 = tf.squeeze(target_critic2([next_states, next_actions]), axis=1)\n",
    "            target_q = rewards + gamma * (1 - dones) * tf.minimum(target1, target2)\n",
    "\n",
    "            # Critic updates\n",
    "            with tf.GradientTape() as tape1, tf.GradientTape() as tape2:\n",
    "                q1 = critic_model1([states, actions], training=True)\n",
    "                q2 = critic_model2([states, actions], training=True)\n",
    "                loss1 = tf.keras.losses.MSE(target_q, q1)\n",
    "                loss2 = tf.keras.losses.MSE(target_q, q2)\n",
    "\n",
    "            critic_grad1 = tape1.gradient(loss1, critic_model1.trainable_variables)\n",
    "            critic_grad2 = tape2.gradient(loss2, critic_model2.trainable_variables)\n",
    "            critic_optimizer1.apply_gradients(zip(critic_grad1, critic_model1.trainable_variables))\n",
    "            critic_optimizer2.apply_gradients(zip(critic_grad2, critic_model2.trainable_variables))\n",
    "\n",
    "            # Delayed actor update\n",
    "            if step % policy_delay == 0:\n",
    "                with tf.GradientTape() as tape_actor:\n",
    "                    act = actor_model(states)\n",
    "                    actor_loss = -tf.reduce_mean(critic_model1([states, act]))\n",
    "                actor_grad = tape_actor.gradient(actor_loss, actor_model.trainable_variables)\n",
    "                actor_optimizer.apply_gradients(zip(actor_grad, actor_model.trainable_variables))\n",
    "\n",
    "                soft_update(target_actor.variables, actor_model.variables)\n",
    "                soft_update(target_critic1.variables, critic_model1.variables)\n",
    "                soft_update(target_critic2.variables, critic_model2.variables)\n",
    "\n",
    "        # --- 8. Sleep to maintain loop ---\n",
    "        time.sleep(dt)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Stopping (Ctrl+C) and saving models...\")\n",
    "\n",
    "finally:\n",
    "    # Save models\n",
    "    actor_model.save(\"td3_actor.h5\")\n",
    "    critic_model1.save(\"td3_critic1.h5\")\n",
    "    critic_model2.save(\"td3_critic2.h5\")\n",
    "    board.close()\n",
    "    print(\"Training finished and models saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neural_network",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
