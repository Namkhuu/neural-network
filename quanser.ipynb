{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8ceea56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Lambda\n",
    "from keras.layers import Concatenate\n",
    "from collections import deque\n",
    "import random\n",
    "from quanser.hardware import HIL \n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8674aa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "board.close()\n",
    "# Open connection to QUBE\n",
    "board = HIL(\"qube_servo3_usb\", \"0\")\n",
    "\n",
    "encoder_channels = np.array([0, 1], dtype=np.uint32)\n",
    "motor_channels = np.array([0], dtype=np.uint32)\n",
    "counts = np.zeros(2, dtype=np.int32)\n",
    "\n",
    "ENCODER_RES = 2048\n",
    "ARM_RAD_PER_COUNT = 2*np.pi / ENCODER_RES\n",
    "PEND_RAD_PER_COUNT = 2*np.pi / ENCODER_RES\n",
    "\n",
    "dt = 0.01  # 10 ms\n",
    "theta_arm_prev  = counts[0] * ARM_RAD_PER_COUNT\n",
    "theta_pend_prev = counts[1] * PEND_RAD_PER_COUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "49e7c226",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=100000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    def store(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = map(np.array, zip(*batch))\n",
    "        return states, actions, rewards, next_states, dones\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "def soft_update(target_weights, online_weights, tau=0.005):\n",
    "    for (target, online) in zip(target_weights, online_weights):\n",
    "        target.assign(target * (1 - tau) + online * tau) \n",
    "\n",
    "replay_buffer = ReplayBuffer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8509c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ntk00\\anaconda3\\envs\\neural_network\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:232: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.InitializationOnlyStatus at 0x20fd0d17280>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_size = 4\n",
    "action_size = 1\n",
    "gamma = 0.99 # discount rate\n",
    "learning_rate = 0.001 # learning rate\n",
    "\n",
    "# Define the actor model\n",
    "states_inputs = Input(shape=(state_size,))\n",
    "dense = Dense(400, activation='relu')(states_inputs)\n",
    "dense = Dense(300, activation='relu')(dense)\n",
    "outputs = Dense(action_size, activation='tanh')(dense)\n",
    "outputs = keras.layers.Lambda(lambda x: x * 2.0)(outputs)  # Scale action to [-2, 2]\n",
    "actor_model = Model(inputs=states_inputs, outputs=outputs)\n",
    "\n",
    "# Critic 1\n",
    "state_input1 = Input(shape=(state_size,))\n",
    "action_input1 = Input(shape=(action_size,))\n",
    "concat1 = Concatenate()([state_input1, action_input1])\n",
    "dense1 = Dense(400, activation='relu')(concat1)\n",
    "dense1 = Dense(300, activation='relu')(dense1)\n",
    "output1 = Dense(1)(dense1)\n",
    "critic_model1 = Model([state_input1, action_input1], output1)\n",
    "\n",
    "# Critic 2\n",
    "state_input2 = Input(shape=(state_size,))\n",
    "action_input2 = Input(shape=(action_size,))\n",
    "concat2 = Concatenate()([state_input2, action_input2])\n",
    "dense2 = Dense(400, activation='relu')(concat2)\n",
    "dense2 = Dense(300, activation='relu')(dense2)\n",
    "output2 = Dense(1)(dense2)\n",
    "critic_model2 = Model([state_input2, action_input2], output2)\n",
    "\n",
    "try:\n",
    "    actor_model.load_weights('saves/quanser/actor_model.weights.h5')\n",
    "    critic_model1.load_weights('saves/quanser/critic_model1.weights.h5')\n",
    "    critic_model2.load_weights('saves/quanser/critic_model2.weights.h5')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "actor_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "critic_optimizer1 = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "critic_optimizer2 = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "target_actor = keras.models.clone_model(actor_model)\n",
    "target_actor.set_weights(actor_model.get_weights())\n",
    "\n",
    "target_critic1 = keras.models.clone_model(critic_model1)\n",
    "target_critic1.set_weights(critic_model1.get_weights())\n",
    "target_critic2 = keras.models.clone_model(critic_model2)\n",
    "target_critic2.set_weights(critic_model2.get_weights())\n",
    "\n",
    "ckpt = tf.train.Checkpoint(actor_optimizer=actor_optimizer,\n",
    "                           critic_optimizer1=critic_optimizer1, \n",
    "                           critic_optimizer2=critic_optimizer2)\n",
    "\n",
    "# Restore the latest checkpoint with optimizer states\n",
    "ckpt.restore(tf.train.latest_checkpoint(\"saves/td3_quanser/optimizers_ckpt\")).expect_partial()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "96fd84a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stopping (Ctrl+C). Saving…\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "history = []\n",
    "policy_delay = 2  # Delayed policy updates\n",
    "step = 0\n",
    "total_reward = 0.0\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        step += 1\n",
    "\n",
    "        # 1) read state\n",
    "        board.read_encoder(encoder_channels, len(encoder_channels), counts)\n",
    "        theta_arm  = counts[0] * ARM_RAD_PER_COUNT\n",
    "        theta_pend = counts[1] * PEND_RAD_PER_COUNT\n",
    "        theta_arm_dot  = (theta_arm  - theta_arm_prev)  / dt\n",
    "        theta_pend_dot = (theta_pend - theta_pend_prev) / dt\n",
    "        state = np.array([theta_arm, theta_pend, theta_arm_dot, theta_pend_dot], dtype=np.float32)\n",
    "\n",
    "        # 2) select action\n",
    "        action_vec = actor_model(tf.convert_to_tensor([state], dtype=tf.float32)).numpy()[0]\n",
    "        action_val = float(np.clip(action_vec[0], -2.0, 2.0))  # scalar in [-2,2]; tune to your safe V range\n",
    "\n",
    "        # 3) apply action (analog write wants numpy float64 buffer)\n",
    "        voltages = np.array([action_val], dtype=np.float64)\n",
    "        board.write_analog(motor_channels, len(motor_channels), voltages)\n",
    "\n",
    "        # 4) get next_state after action\n",
    "        time.sleep(dt)  # maintain loop timing around the actuation\n",
    "        board.read_encoder(encoder_channels, len(encoder_channels), counts)\n",
    "        next_theta_arm  = counts[0] * ARM_RAD_PER_COUNT\n",
    "        next_theta_pend = counts[1] * PEND_RAD_PER_COUNT\n",
    "        next_theta_arm_dot  = (next_theta_arm  - theta_arm)  / dt\n",
    "        next_theta_pend_dot = (next_theta_pend - theta_pend) / dt\n",
    "        next_state = np.array([next_theta_arm, next_theta_pend, next_theta_arm_dot, next_theta_pend_dot], dtype=np.float32)\n",
    "\n",
    "        # 5) reward (example: upright pendulum, gentle motion)\n",
    "        reward = - ( (np.angle(np.exp(1j*(next_theta_pend - np.pi))))**2\n",
    "                     + 0.1*next_theta_pend_dot**2 + 0.01*action_val**2 )\n",
    "        total_reward += reward\n",
    "\n",
    "        # 6) store\n",
    "        replay_buffer.store(state, action_val, reward, next_state, False)\n",
    "\n",
    "        # 7) train if enough samples\n",
    "        if replay_buffer.size() >= batch_size:\n",
    "            states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "            states      = tf.convert_to_tensor(states, dtype=tf.float32)\n",
    "            actions     = tf.convert_to_tensor(actions.reshape(-1,1), dtype=tf.float32)\n",
    "            rewards     = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
    "            next_states = tf.convert_to_tensor(next_states, dtype=tf.float32)\n",
    "            dones       = tf.convert_to_tensor(dones, dtype=tf.float32)\n",
    "\n",
    "            # target policy smoothing\n",
    "            noise = np.clip(np.random.normal(0, 0.2, size=(actions.shape[0], 1)), -0.5, 0.5)\n",
    "            target_act = tf.clip_by_value(target_actor(next_states) + noise, -2.0, 2.0)\n",
    "\n",
    "            # twin critics target\n",
    "            t1 = tf.squeeze(target_critic1([next_states, target_act]), axis=1)\n",
    "            t2 = tf.squeeze(target_critic2([next_states, target_act]), axis=1)\n",
    "            target_q = rewards + gamma * (1.0 - dones) * tf.minimum(t1, t2)\n",
    "\n",
    "            # critic updates\n",
    "            with tf.GradientTape() as tape1, tf.GradientTape() as tape2:\n",
    "                q1 = tf.squeeze(critic_model1([states, actions]), axis=1)\n",
    "                q2 = tf.squeeze(critic_model2([states, actions]), axis=1)\n",
    "                loss1 = tf.keras.losses.MSE(target_q, q1)\n",
    "                loss2 = tf.keras.losses.MSE(target_q, q2)\n",
    "            critic_optimizer1.apply_gradients(zip(tape1.gradient(loss1, critic_model1.trainable_variables),\n",
    "                                                  critic_model1.trainable_variables))\n",
    "            critic_optimizer2.apply_gradients(zip(tape2.gradient(loss2, critic_model2.trainable_variables),\n",
    "                                                  critic_model2.trainable_variables))\n",
    "\n",
    "            # delayed actor + target updates\n",
    "            if step % policy_delay == 0:\n",
    "                with tf.GradientTape() as tape_actor:\n",
    "                    pi = actor_model(states)\n",
    "                    actor_loss = -tf.reduce_mean(critic_model1([states, pi]))\n",
    "                actor_optimizer.apply_gradients(zip(tape_actor.gradient(actor_loss, actor_model.trainable_variables),\n",
    "                                                    actor_model.trainable_variables))\n",
    "                soft_update(target_actor.variables,   actor_model.variables,   tau=0.005)\n",
    "                soft_update(target_critic1.variables, critic_model1.variables, tau=0.005)\n",
    "                soft_update(target_critic2.variables, critic_model2.variables, tau=0.005)\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            print(f\"Step {step}  reward_sum: {total_reward:.2f}\")\n",
    "\n",
    "        # update prev angles for next derivative\n",
    "        theta_arm_prev, theta_pend_prev = next_theta_arm, next_theta_pend\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nStopping (Ctrl+C). Saving…\")\n",
    "finally:\n",
    "    # save weights (use .save_weights if you prefer checkpoint style)\n",
    "    actor_model.save_weights(\"saves/quanser/actor_model.weights.h5\")\n",
    "    critic_model1.save_weights(\"saves/quanser/critic_model1.weights.h5\")\n",
    "    critic_model2.save_weights(\"saves/quanser/critic_model2.weights.h5\")\n",
    "    # set motor to 0V and close safely\n",
    "    board.write_analog(motor_channels, 1, np.array([0.0], dtype=np.float64))\n",
    "    board.close()\n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ff966c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ntk00\\AppData\\Local\\Temp\\ipykernel_20244\\4268336479.py:10: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  u = float(action)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "a bytes-like object is required, not 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[70], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m u \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(action)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# send to motor\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[43mboard\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_analog\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmotor_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mu\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(dt)  \u001b[38;5;66;03m# ~0.01s\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ntk00\\anaconda3\\envs\\neural_network\\lib\\site-packages\\quanser\\hardware\\hil.py:2732\u001b[0m, in \u001b[0;36mHIL.write_analog\u001b[1;34m(self, channels, num_channels, buffer)\u001b[0m\n\u001b[0;32m   2675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrite_analog\u001b[39m(\u001b[38;5;28mself\u001b[39m, channels, num_channels, buffer):\n\u001b[0;32m   2676\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Writes to analog outputs immediately. The function does not return until the data has been written.\u001b[39;00m\n\u001b[0;32m   2677\u001b[0m \n\u001b[0;32m   2678\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2727\u001b[0m \n\u001b[0;32m   2728\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   2729\u001b[0m     result \u001b[38;5;241m=\u001b[39m hil_lib\u001b[38;5;241m.\u001b[39mhil_write_analog(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_card \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_card \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m ffi\u001b[38;5;241m.\u001b[39mNULL,\n\u001b[0;32m   2730\u001b[0m                                       ffi\u001b[38;5;241m.\u001b[39mfrom_buffer(_UINT32_ARRAY, channels) \u001b[38;5;28;01mif\u001b[39;00m channels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m ffi\u001b[38;5;241m.\u001b[39mNULL,\n\u001b[0;32m   2731\u001b[0m                                       num_channels,\n\u001b[1;32m-> 2732\u001b[0m                                       \u001b[43mffi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_buffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_DOUBLE_ARRAY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m ffi\u001b[38;5;241m.\u001b[39mNULL)\n\u001b[0;32m   2733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   2734\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m HILError(result)\n",
      "File \u001b[1;32mc:\\Users\\ntk00\\anaconda3\\envs\\neural_network\\lib\\site-packages\\cffi\\api.py:365\u001b[0m, in \u001b[0;36mFFI.from_buffer\u001b[1;34m(self, cdecl, python_buffer, require_writable)\u001b[0m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(cdecl, basestring):\n\u001b[0;32m    364\u001b[0m     cdecl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_typeof(cdecl)\n\u001b[1;32m--> 365\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_buffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcdecl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpython_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    366\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mrequire_writable\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: a bytes-like object is required, not 'list'"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    # read state\n",
    "    # board.close()\n",
    "    board.read_encoder(encoder_channels, len(encoder_channels), counts)\n",
    "    theta_arm = counts[0] * ARM_RAD_PER_COUNT\n",
    "    theta_pend = counts[1] * PEND_RAD_PER_COUNT\n",
    "\n",
    "    # compute action from policy\n",
    "    action = actor_model(tf.convert_to_tensor([state], dtype=tf.float32)).numpy()[0]\n",
    "    u = float(action)\n",
    "\n",
    "    # send to motor\n",
    "    board.write_analog(motor_channels, 1, [u])\n",
    "\n",
    "    time.sleep(dt)  # ~0.01s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e598aa56",
   "metadata": {},
   "outputs": [
    {
     "ename": "HILError",
     "evalue": "-1410",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHILError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcollections\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m deque\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# --- HIL/QUBE setup ---\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# board.close()\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m board \u001b[38;5;241m=\u001b[39m \u001b[43mHIL\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mqube_servo3_usb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m encoder_channels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39muint32)\n\u001b[0;32m     10\u001b[0m motor_channels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39muint32)\n",
      "File \u001b[1;32mc:\\Users\\ntk00\\anaconda3\\envs\\neural_network\\lib\\site-packages\\quanser\\hardware\\hil.py:663\u001b[0m, in \u001b[0;36mHIL.__init__\u001b[1;34m(self, card_type, card_identifier)\u001b[0m\n\u001b[0;32m    661\u001b[0m \u001b[38;5;66;03m# If non-default arguments are passed, attempt to open the card.\u001b[39;00m\n\u001b[0;32m    662\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m card_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 663\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcard_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcard_identifier\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ntk00\\anaconda3\\envs\\neural_network\\lib\\site-packages\\quanser\\hardware\\hil.py:774\u001b[0m, in \u001b[0;36mHIL.open\u001b[1;34m(self, card_type, card_identifier)\u001b[0m\n\u001b[0;32m    771\u001b[0m     result \u001b[38;5;241m=\u001b[39m hil_lib\u001b[38;5;241m.\u001b[39mhil_open(card_type\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUTF-8\u001b[39m\u001b[38;5;124m'\u001b[39m), card_identifier\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUTF-8\u001b[39m\u001b[38;5;124m'\u001b[39m), card)\n\u001b[0;32m    773\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 774\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HILError(result)\n\u001b[0;32m    776\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_card \u001b[38;5;241m=\u001b[39m card[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mHILError\u001b[0m: -1410"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "\n",
    "# --- HIL/QUBE setup ---\n",
    "board.close()\n",
    "board = HIL(\"qube_servo3_usb\", \"0\")\n",
    "encoder_channels = np.array([0, 1], dtype=np.uint32)\n",
    "motor_channels = np.array([0], dtype=np.uint32)\n",
    "counts = np.zeros(2, dtype=np.int32)\n",
    "\n",
    "ENCODER_RES = 2048\n",
    "ARM_RAD_PER_COUNT = 2*np.pi / ENCODER_RES\n",
    "PEND_RAD_PER_COUNT = 2*np.pi / ENCODER_RES\n",
    "dt = 0.01  # 10 ms loop\n",
    "\n",
    "# --- Replay Buffer ---\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=100000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    def store(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    def sample(self, batch_size):\n",
    "        batch = np.array(random.sample(self.buffer, batch_size))\n",
    "        states, actions, rewards, next_states, dones = map(np.stack, zip(*batch))\n",
    "        return states, actions, rewards, next_states, dones\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "replay_buffer = ReplayBuffer()\n",
    "\n",
    "# --- Soft update ---\n",
    "def soft_update(target_weights, online_weights, tau=0.005):\n",
    "    for (target, online) in zip(target_weights, online_weights):\n",
    "        target.assign(target * (1 - tau) + online * tau)\n",
    "\n",
    "# --- TD3 models already defined: actor_model, critic_model1, critic_model2, \n",
    "# target_actor, target_critic1, target_critic2\n",
    "# optimizers: actor_optimizer, critic_optimizer1, critic_optimizer2\n",
    "\n",
    "state_size = 4\n",
    "action_size = 1\n",
    "gamma = 0.99\n",
    "batch_size = 32\n",
    "policy_delay = 2\n",
    "step = 0\n",
    "\n",
    "theta_arm_prev = 0.0\n",
    "theta_pend_prev = 0.0\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        step += 1\n",
    "\n",
    "        # --- 1. Read state ---\n",
    "        board.read_encoder(encoder_channels, len(encoder_channels), counts)\n",
    "        theta_arm = counts[0] * ARM_RAD_PER_COUNT\n",
    "        theta_pend = counts[1] * PEND_RAD_PER_COUNT\n",
    "\n",
    "        theta_arm_dot = (theta_arm - theta_arm_prev) / dt\n",
    "        theta_pend_dot = (theta_pend - theta_pend_prev) / dt\n",
    "        theta_arm_prev, theta_pend_prev = theta_arm, theta_pend\n",
    "\n",
    "        state = np.array([theta_arm, theta_pend, theta_arm_dot, theta_pend_dot], dtype=np.float32)\n",
    "\n",
    "        # --- 2. Compute action ---\n",
    "        action = actor_model(tf.convert_to_tensor([state], dtype=tf.float32)).numpy()[0]\n",
    "        u_array = np.array([float(action)], dtype=np.float64)\n",
    "\n",
    "        # --- 3. Apply action ---\n",
    "        board.write_analog(motor_channels, 1, u_array)\n",
    "\n",
    "        # --- 4. Read next state ---\n",
    "        board.read_encoder(encoder_channels, len(encoder_channels), counts)\n",
    "        next_theta_arm = counts[0] * ARM_RAD_PER_COUNT\n",
    "        next_theta_pend = counts[1] * PEND_RAD_PER_COUNT\n",
    "        next_theta_arm_dot = (next_theta_arm - theta_arm) / dt\n",
    "        next_theta_pend_dot = (next_theta_pend - theta_pend) / dt\n",
    "        next_state = np.array([next_theta_arm, next_theta_pend, next_theta_arm_dot, next_theta_pend_dot], dtype=np.float32)\n",
    "\n",
    "        # --- 5. Compute reward ---\n",
    "        reward = - (next_theta_pend**2 + 0.1 * next_theta_pend_dot**2)\n",
    "\n",
    "        # --- 6. Store transition ---\n",
    "        replay_buffer.store(state, action, reward, next_state, False)\n",
    "\n",
    "        # --- 7. Train TD3 ---\n",
    "        if replay_buffer.size() >= batch_size:\n",
    "            states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "            states = tf.convert_to_tensor(states, dtype=tf.float32)\n",
    "            actions = tf.convert_to_tensor(actions, dtype=tf.float32)\n",
    "            rewards = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
    "            next_states = tf.convert_to_tensor(next_states, dtype=tf.float32)\n",
    "            dones = tf.convert_to_tensor(dones, dtype=tf.float32)\n",
    "\n",
    "            # Target actions with clipped noise\n",
    "            noise = np.clip(np.random.normal(0, 0.2, size=actions.shape), -0.5, 0.5)\n",
    "            next_actions = tf.clip_by_value(target_actor(next_states) + noise, -2.0, 2.0)\n",
    "\n",
    "            # Target Q-values\n",
    "            target1 = tf.squeeze(target_critic1([next_states, next_actions]), axis=1)\n",
    "            target2 = tf.squeeze(target_critic2([next_states, next_actions]), axis=1)\n",
    "            target_q = rewards + gamma * (1 - dones) * tf.minimum(target1, target2)\n",
    "\n",
    "            # Critic updates\n",
    "            with tf.GradientTape() as tape1, tf.GradientTape() as tape2:\n",
    "                q1 = critic_model1([states, actions], training=True)\n",
    "                q2 = critic_model2([states, actions], training=True)\n",
    "                loss1 = tf.keras.losses.MSE(target_q, q1)\n",
    "                loss2 = tf.keras.losses.MSE(target_q, q2)\n",
    "\n",
    "            critic_grad1 = tape1.gradient(loss1, critic_model1.trainable_variables)\n",
    "            critic_grad2 = tape2.gradient(loss2, critic_model2.trainable_variables)\n",
    "            critic_optimizer1.apply_gradients(zip(critic_grad1, critic_model1.trainable_variables))\n",
    "            critic_optimizer2.apply_gradients(zip(critic_grad2, critic_model2.trainable_variables))\n",
    "\n",
    "            # Delayed actor update\n",
    "            if step % policy_delay == 0:\n",
    "                with tf.GradientTape() as tape_actor:\n",
    "                    act = actor_model(states)\n",
    "                    actor_loss = -tf.reduce_mean(critic_model1([states, act]))\n",
    "                actor_grad = tape_actor.gradient(actor_loss, actor_model.trainable_variables)\n",
    "                actor_optimizer.apply_gradients(zip(actor_grad, actor_model.trainable_variables))\n",
    "\n",
    "                soft_update(target_actor.variables, actor_model.variables)\n",
    "                soft_update(target_critic1.variables, critic_model1.variables)\n",
    "                soft_update(target_critic2.variables, critic_model2.variables)\n",
    "\n",
    "        # --- 8. Sleep to maintain loop ---\n",
    "        time.sleep(dt)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Stopping (Ctrl+C) and saving models...\")\n",
    "\n",
    "finally:\n",
    "    # Save models\n",
    "    actor_model.save(\"td3_actor.h5\")\n",
    "    critic_model1.save(\"td3_critic1.h5\")\n",
    "    critic_model2.save(\"td3_critic2.h5\")\n",
    "    board.close()\n",
    "    print(\"Training finished and models saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neural_network",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
