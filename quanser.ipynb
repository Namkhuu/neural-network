{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b8ceea56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Lambda\n",
    "from keras.layers import Concatenate\n",
    "from collections import deque\n",
    "import random\n",
    "from quanser.hardware import HIL \n",
    "from pal.products.qube import QubeServo3\n",
    "from pal.utilities.math import SignalGenerator, ddt_filter\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8674aa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # board.close()\n",
    "# # Open connection to QUBE\n",
    "# # board = HIL(\"qube_servo3_usb\", \"0\")\n",
    "\n",
    "# encoder_channels = np.array([0, 1], dtype=np.uint32)\n",
    "# motor_channels = np.array([0], dtype=np.uint32)\n",
    "# counts = np.zeros(2, dtype=np.int32)\n",
    "\n",
    "# ENCODER_RES = 2048\n",
    "# ARM_RAD_PER_COUNT = 2*np.pi / ENCODER_RES\n",
    "# PEND_RAD_PER_COUNT = 2*np.pi / ENCODER_RES\n",
    "\n",
    "# dt = 0.01  # 10 ms\n",
    "# theta_arm_prev  = counts[0] * ARM_RAD_PER_COUNT\n",
    "# theta_pend_prev = counts[1] * PEND_RAD_PER_COUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "49e7c226",
   "metadata": {},
   "outputs": [],
   "source": [
    "global KILL_THREAD\n",
    "KILL_THREAD = False\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=100000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    def store(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = map(np.array, zip(*batch))\n",
    "        return (np.array(states, dtype=np.float32),\n",
    "            np.array(actions, dtype=np.float32),\n",
    "            np.array(rewards, dtype=np.float32),\n",
    "            np.array(next_states, dtype=np.float32),\n",
    "            np.array(dones, dtype=np.float32))\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "def soft_update(target_weights, online_weights, tau=0.005):\n",
    "    for (target, online) in zip(target_weights, online_weights):\n",
    "        target.assign(target * (1 - tau) + online * tau) \n",
    "\n",
    "def sig_handler(*args): \n",
    "    global KILL_THREAD\n",
    "    KILL_THREAD = True\n",
    "\n",
    "# signal.signal(signal.SIGINT, sig_handler)\n",
    "\n",
    "replay_buffer = ReplayBuffer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f8509c67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.InitializationOnlyStatus at 0x1a1e0664c40>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_size = 4\n",
    "action_size = 1\n",
    "gamma = 0.99 # discount rate\n",
    "learning_rate = 0.001 # learning rate\n",
    "\n",
    "# Define the actor model\n",
    "states_inputs = Input(shape=(state_size,))\n",
    "dense = Dense(400, activation='relu')(states_inputs)\n",
    "dense = Dense(300, activation='relu')(dense)\n",
    "outputs = Dense(action_size, activation='tanh')(dense)\n",
    "outputs = keras.layers.Lambda(lambda x: x * 2.0)(outputs)  # Scale action to [-2, 2]\n",
    "actor_model = Model(inputs=states_inputs, outputs=outputs)\n",
    "\n",
    "# Critic 1\n",
    "state_input1 = Input(shape=(state_size,))\n",
    "action_input1 = Input(shape=(action_size,))\n",
    "concat1 = Concatenate()([state_input1, action_input1])\n",
    "dense1 = Dense(400, activation='relu')(concat1)\n",
    "dense1 = Dense(300, activation='relu')(dense1)\n",
    "output1 = Dense(1)(dense1)\n",
    "critic_model1 = Model([state_input1, action_input1], output1)\n",
    "\n",
    "# Critic 2\n",
    "state_input2 = Input(shape=(state_size,))\n",
    "action_input2 = Input(shape=(action_size,))\n",
    "concat2 = Concatenate()([state_input2, action_input2])\n",
    "dense2 = Dense(400, activation='relu')(concat2)\n",
    "dense2 = Dense(300, activation='relu')(dense2)\n",
    "output2 = Dense(1)(dense2)\n",
    "critic_model2 = Model([state_input2, action_input2], output2)\n",
    "\n",
    "try:\n",
    "    actor_model.load_weights('saves/quanser/actor_model.weights.h5')\n",
    "    critic_model1.load_weights('saves/quanser/critic_model1.weights.h5')\n",
    "    critic_model2.load_weights('saves/quanser/critic_model2.weights.h5')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "actor_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "critic_optimizer1 = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "critic_optimizer2 = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "target_actor = keras.models.clone_model(actor_model)\n",
    "target_actor.set_weights(actor_model.get_weights())\n",
    "\n",
    "target_critic1 = keras.models.clone_model(critic_model1)\n",
    "target_critic1.set_weights(critic_model1.get_weights())\n",
    "target_critic2 = keras.models.clone_model(critic_model2)\n",
    "target_critic2.set_weights(critic_model2.get_weights())\n",
    "\n",
    "ckpt = tf.train.Checkpoint(actor_optimizer=actor_optimizer,\n",
    "                           critic_optimizer1=critic_optimizer1, \n",
    "                           critic_optimizer2=critic_optimizer2)\n",
    "\n",
    "# Restore the latest checkpoint with optimizer states\n",
    "ckpt.restore(tf.train.latest_checkpoint(\"saves/td3_quanser/optimizers_ckpt\")).expect_partial()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "844cf3fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta: 0.000, Theta dot: 0.000, Alpha: 0.000, Alpha dot: 0.000\n",
      "Theta: 0.000, Theta dot: 0.000, Alpha: 0.000, Alpha dot: 0.000\n",
      "Theta: 0.000, Theta dot: 0.000, Alpha: 0.000, Alpha dot: 0.000\n",
      "Theta: 0.000, Theta dot: 0.000, Alpha: 0.000, Alpha dot: 0.000\n",
      "Theta: 0.000, Theta dot: 0.000, Alpha: 0.000, Alpha dot: 0.000\n",
      "Theta: 0.000, Theta dot: 0.000, Alpha: 0.000, Alpha dot: 0.000\n",
      "Theta: 0.914, Theta dot: 43.536, Alpha: 0.120, Alpha dot: 10.877\n",
      "Theta: 1.331, Theta dot: 59.258, Alpha: 0.291, Alpha dot: 24.518\n",
      "Theta: 1.405, Theta dot: 57.121, Alpha: 0.221, Alpha dot: 13.646\n",
      "Theta: 1.485, Theta dot: 55.479, Alpha: 0.141, Alpha dot: 3.913\n",
      "Theta: 1.491, Theta dot: 50.488, Alpha: 0.015, Alpha dot: -8.234\n",
      "Theta: 1.571, Theta dot: 49.478, Alpha: 0.095, Alpha dot: 0.515\n",
      "Theta: 1.571, Theta dot: 44.766, Alpha: 0.334, Alpha dot: 22.176\n",
      "Theta: 1.571, Theta dot: 40.502, Alpha: 0.285, Alpha dot: 13.682\n",
      "Theta: 1.571, Theta dot: 36.645, Alpha: 0.123, Alpha dot: -3.588\n",
      "Theta: 1.571, Theta dot: 33.155, Alpha: -0.138, Alpha dot: -26.643\n",
      "Theta: 1.390, Theta dot: 21.378, Alpha: -0.083, Alpha dot: -16.778\n",
      "Theta: 1.442, Theta dot: 21.825, Alpha: 0.009, Alpha dot: -5.360\n",
      "Theta: 0.675, Theta dot: -16.777, Alpha: -0.144, Alpha dot: -18.331\n",
      "Theta: -1.138, Theta dot: -101.520, Alpha: 0.184, Alpha dot: 14.845\n",
      "Theta: -1.313, Theta dot: -100.179, Alpha: 0.166, Alpha dot: 10.472\n",
      "Theta: -0.672, Theta dot: -60.104, Alpha: -0.037, Alpha dot: -9.840\n",
      "Theta: -0.291, Theta dot: -36.265, Alpha: -0.077, Alpha dot: -11.676\n",
      "Theta: -0.239, Theta dot: -30.327, Alpha: 0.015, Alpha dot: -1.186\n",
      "Theta: -0.279, Theta dot: -29.338, Alpha: -0.003, Alpha dot: -2.644\n",
      "Theta: -0.273, Theta dot: -26.252, Alpha: -0.003, Alpha dot: -2.163\n",
      "Theta: -0.276, Theta dot: -23.898, Alpha: -0.006, Alpha dot: -2.049\n",
      "Theta: -0.276, Theta dot: -21.622, Alpha: -0.003, Alpha dot: -1.397\n",
      "Theta: -0.276, Theta dot: -19.563, Alpha: -0.003, Alpha dot: -1.143\n",
      "Theta: -0.276, Theta dot: -17.699, Alpha: -0.003, Alpha dot: -0.935\n",
      "Theta: -0.276, Theta dot: -16.014, Alpha: -0.003, Alpha dot: -0.765\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[62], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m alpha_dot, state_alpha_dot \u001b[38;5;241m=\u001b[39m ddt_filter(alpha, state_alpha_dot, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39mfrequency)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTheta: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtheta\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Theta dot: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtheta_dot\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Alpha: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00malpha\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Alpha dot: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00malpha_dot\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 18\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "frequency = 500  # Hz\n",
    "state_theta_dot = np.array([0,0], dtype=np.float64)\n",
    "state_alpha_dot = np.array([0,0], dtype=np.float64)\n",
    "with QubeServo3(hardware = 1, pendulum = 1, frequency=frequency) as board:\n",
    "    while True:\n",
    "        # Have to initialize the board first before reading motorPosition or it won't read\n",
    "        board.read_outputs()\n",
    "        theta = board.motorPosition \n",
    "        alpha = board.pendulumPosition \n",
    "        theta = np.clip(theta, -np.pi/2, np.pi/2)\n",
    "        theta_dot, state_theta_dot = ddt_filter(theta, state_theta_dot, 50, 1/frequency)\n",
    "        # u - input\n",
    "        # state - previous state returned by this function -- initialize to np.array([0,0], dtype=np.float64)\n",
    "        # Ts - sample time in seconds\n",
    "        # A - filter bandwidth in rad/s\n",
    "        alpha_dot, state_alpha_dot = ddt_filter(alpha, state_alpha_dot, 100, 1/frequency)\n",
    "        print(f\"Theta: {theta:.3f}, Theta dot: {theta_dot:.3f}, Alpha: {alpha:.3f}, Alpha dot: {alpha_dot:.3f}\")\n",
    "        time.sleep(0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7442b395",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ntk00\\AppData\\Local\\Temp\\ipykernel_19068\\872869145.py:94: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  print(f\"Epoch {step + 1}/{step}, Total Reward: {float(reward):.4f}, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/1, Total Reward: -10.0835, Q1: -913.3600, Q2: -909.9741, TargetQ: -846.1288\n",
      "Epoch 3/2, Total Reward: -106.5498, Q1: -952.4332, Q2: -948.6178, TargetQ: -960.0959\n",
      "Epoch 4/3, Total Reward: -841.1502, Q1: -859.0465, Q2: -859.4873, TargetQ: -915.0922\n",
      "Epoch 5/4, Total Reward: -1184.3095, Q1: -889.6292, Q2: -888.7556, TargetQ: -949.9143\n",
      "Epoch 6/5, Total Reward: -8.8291, Q1: -910.0326, Q2: -911.4404, TargetQ: -853.1587\n",
      "Epoch 7/6, Total Reward: -1755.6012, Q1: -841.2726, Q2: -848.4001, TargetQ: -812.2518\n",
      "Epoch 8/7, Total Reward: -2680.9104, Q1: -887.2628, Q2: -893.1108, TargetQ: -952.3666\n",
      "Epoch 9/8, Total Reward: -662.6842, Q1: -928.9559, Q2: -935.4340, TargetQ: -1000.8057\n",
      "Epoch 10/9, Total Reward: -5411.6230, Q1: -1046.9744, Q2: -1057.1564, TargetQ: -1055.8726\n",
      "Epoch 11/10, Total Reward: -2834.5914, Q1: -770.9133, Q2: -779.5778, TargetQ: -739.4778\n",
      "Epoch 12/11, Total Reward: -663.2492, Q1: -823.7369, Q2: -832.7930, TargetQ: -891.7587\n",
      "Epoch 13/12, Total Reward: -12.7601, Q1: -923.7491, Q2: -936.3682, TargetQ: -874.0413\n",
      "Epoch 14/13, Total Reward: -1810.1588, Q1: -896.7039, Q2: -910.1339, TargetQ: -904.5412\n",
      "Epoch 15/14, Total Reward: -755.6078, Q1: -742.9829, Q2: -746.7693, TargetQ: -822.9484\n",
      "Epoch 16/15, Total Reward: -527.0794, Q1: -1022.8588, Q2: -1033.0830, TargetQ: -1181.1768\n",
      "Epoch 17/16, Total Reward: -323.6011, Q1: -774.4708, Q2: -779.2028, TargetQ: -881.1077\n",
      "Epoch 18/17, Total Reward: -76.7175, Q1: -877.9467, Q2: -884.5688, TargetQ: -838.8674\n",
      "Epoch 19/18, Total Reward: -124.0347, Q1: -814.0236, Q2: -818.4829, TargetQ: -850.2040\n",
      "Epoch 20/19, Total Reward: -935.2662, Q1: -841.9606, Q2: -846.1720, TargetQ: -915.5319\n",
      "Epoch 21/20, Total Reward: -30.7058, Q1: -747.7669, Q2: -755.7545, TargetQ: -721.6274\n",
      "Epoch 22/21, Total Reward: -11.0129, Q1: -1006.2097, Q2: -1008.4988, TargetQ: -1125.2886\n",
      "Epoch 23/22, Total Reward: -694.2718, Q1: -897.9983, Q2: -901.8087, TargetQ: -975.1521\n",
      "Epoch 24/23, Total Reward: -8.8006, Q1: -961.5805, Q2: -958.4792, TargetQ: -1030.6475\n",
      "Epoch 25/24, Total Reward: -224.7779, Q1: -902.2700, Q2: -901.8847, TargetQ: -1013.0813\n",
      "Epoch 26/25, Total Reward: -511.8723, Q1: -949.4462, Q2: -953.3452, TargetQ: -1003.9633\n",
      "Epoch 27/26, Total Reward: -59.3375, Q1: -960.2268, Q2: -954.2288, TargetQ: -1017.7035\n",
      "Epoch 28/27, Total Reward: -260.4813, Q1: -957.8440, Q2: -952.6603, TargetQ: -1053.7568\n",
      "Epoch 29/28, Total Reward: -768.7542, Q1: -967.1978, Q2: -960.0966, TargetQ: -876.0662\n",
      "Epoch 30/29, Total Reward: -493.1901, Q1: -1020.8152, Q2: -1016.0923, TargetQ: -1017.7844\n",
      "Epoch 31/30, Total Reward: -10.8186, Q1: -902.3173, Q2: -900.0447, TargetQ: -934.9047\n",
      "Epoch 32/31, Total Reward: -193.9131, Q1: -996.3688, Q2: -987.2140, TargetQ: -927.8085\n",
      "Epoch 33/32, Total Reward: -1009.9822, Q1: -981.6451, Q2: -974.1267, TargetQ: -1053.1855\n",
      "Epoch 34/33, Total Reward: -562.4807, Q1: -877.7100, Q2: -876.1084, TargetQ: -794.6901\n",
      "Epoch 35/34, Total Reward: -1018.4683, Q1: -859.3695, Q2: -859.5151, TargetQ: -718.0530\n",
      "Epoch 36/35, Total Reward: -182.4927, Q1: -942.7231, Q2: -941.8638, TargetQ: -1169.2764\n",
      "Epoch 37/36, Total Reward: -107.5061, Q1: -1068.9934, Q2: -1064.2385, TargetQ: -1207.0986\n",
      "Epoch 38/37, Total Reward: -981.2833, Q1: -1038.9653, Q2: -1038.5542, TargetQ: -1163.1921\n",
      "Epoch 39/38, Total Reward: -402.2585, Q1: -988.3358, Q2: -986.4182, TargetQ: -1087.9771\n",
      "Epoch 40/39, Total Reward: -11.0958, Q1: -1011.6528, Q2: -1018.2917, TargetQ: -1055.9399\n",
      "Epoch 41/40, Total Reward: -341.2936, Q1: -858.9670, Q2: -861.4325, TargetQ: -841.3520\n",
      "Epoch 42/41, Total Reward: -1670.9490, Q1: -827.2269, Q2: -829.5710, TargetQ: -952.2834\n",
      "Epoch 43/42, Total Reward: -33.2668, Q1: -959.6890, Q2: -962.6406, TargetQ: -794.6449\n",
      "Epoch 44/43, Total Reward: -350.7140, Q1: -1012.0830, Q2: -1015.7606, TargetQ: -986.2704\n",
      "Epoch 45/44, Total Reward: -690.6472, Q1: -987.4788, Q2: -990.4705, TargetQ: -848.2360\n",
      "Epoch 46/45, Total Reward: -970.3832, Q1: -930.2184, Q2: -931.5792, TargetQ: -754.6494\n",
      "Epoch 47/46, Total Reward: -2927.0448, Q1: -955.8794, Q2: -957.7911, TargetQ: -1080.8270\n",
      "Epoch 48/47, Total Reward: -28.2812, Q1: -958.7352, Q2: -959.7743, TargetQ: -984.2084\n",
      "Epoch 49/48, Total Reward: -1384.3091, Q1: -876.8027, Q2: -873.8513, TargetQ: -853.5989\n",
      "Epoch 50/49, Total Reward: -1678.5740, Q1: -961.3978, Q2: -947.9053, TargetQ: -883.4337\n",
      "Epoch 51/50, Total Reward: -92.7996, Q1: -835.7728, Q2: -840.8063, TargetQ: -907.9148\n",
      "Epoch 52/51, Total Reward: -4208.7371, Q1: -819.0919, Q2: -821.6835, TargetQ: -876.9000\n",
      "Epoch 53/52, Total Reward: -1036.5994, Q1: -757.1187, Q2: -763.6246, TargetQ: -776.2684\n",
      "Epoch 54/53, Total Reward: -1574.1680, Q1: -888.9430, Q2: -892.3553, TargetQ: -907.0801\n",
      "Epoch 55/54, Total Reward: -1980.9725, Q1: -908.8575, Q2: -915.0177, TargetQ: -1074.9526\n",
      "Epoch 56/55, Total Reward: -46.1114, Q1: -839.7288, Q2: -848.9985, TargetQ: -920.4550\n",
      "Epoch 57/56, Total Reward: -337.2264, Q1: -830.4524, Q2: -843.5668, TargetQ: -911.1324\n",
      "Epoch 58/57, Total Reward: -1812.9351, Q1: -871.4296, Q2: -877.7371, TargetQ: -959.6515\n",
      "Epoch 59/58, Total Reward: -690.4539, Q1: -817.0148, Q2: -824.4183, TargetQ: -941.8083\n",
      "Epoch 60/59, Total Reward: -1472.3798, Q1: -901.7400, Q2: -908.1104, TargetQ: -1194.8920\n",
      "Epoch 61/60, Total Reward: -1215.0985, Q1: -884.9337, Q2: -891.4476, TargetQ: -1178.0004\n",
      "Epoch 62/61, Total Reward: -157.3440, Q1: -834.2638, Q2: -841.8515, TargetQ: -849.7673\n",
      "Epoch 63/62, Total Reward: -102.7782, Q1: -970.0325, Q2: -973.1005, TargetQ: -1032.7710\n",
      "Epoch 64/63, Total Reward: -1918.8788, Q1: -930.3882, Q2: -933.1388, TargetQ: -884.7307\n",
      "Epoch 65/64, Total Reward: -878.0865, Q1: -1065.7384, Q2: -1064.8470, TargetQ: -1136.3549\n",
      "Epoch 66/65, Total Reward: -1148.6317, Q1: -899.2889, Q2: -899.2866, TargetQ: -848.9637\n",
      "Epoch 67/66, Total Reward: -1276.4031, Q1: -963.3304, Q2: -968.1066, TargetQ: -878.9176\n",
      "Epoch 68/67, Total Reward: -454.0007, Q1: -913.5732, Q2: -915.3480, TargetQ: -926.6701\n",
      "Epoch 69/68, Total Reward: -219.7202, Q1: -1021.3306, Q2: -1021.9879, TargetQ: -977.6611\n",
      "Epoch 70/69, Total Reward: -2783.3352, Q1: -985.9557, Q2: -985.2063, TargetQ: -996.2845\n",
      "Epoch 71/70, Total Reward: -575.8709, Q1: -1137.2598, Q2: -1131.5345, TargetQ: -1486.5581\n",
      "Epoch 72/71, Total Reward: -1684.9900, Q1: -1009.4904, Q2: -1009.3871, TargetQ: -1136.5420\n",
      "Epoch 73/72, Total Reward: -791.1195, Q1: -899.8066, Q2: -901.4912, TargetQ: -1027.9742\n",
      "Epoch 74/73, Total Reward: -134.5214, Q1: -939.7732, Q2: -937.7039, TargetQ: -912.6729\n",
      "Epoch 75/74, Total Reward: -10.1701, Q1: -1028.0470, Q2: -1024.6267, TargetQ: -1001.5535\n",
      "Epoch 76/75, Total Reward: -1481.9682, Q1: -965.6309, Q2: -968.2222, TargetQ: -858.8849\n",
      "Epoch 77/76, Total Reward: -991.3423, Q1: -937.2369, Q2: -945.5003, TargetQ: -760.8737\n",
      "Epoch 78/77, Total Reward: -842.2780, Q1: -1037.0872, Q2: -1039.3906, TargetQ: -1066.8877\n",
      "Epoch 79/78, Total Reward: -749.3538, Q1: -1092.6344, Q2: -1096.4211, TargetQ: -1069.8779\n",
      "Epoch 80/79, Total Reward: -11.8427, Q1: -1086.2678, Q2: -1094.4883, TargetQ: -950.2150\n",
      "Epoch 81/80, Total Reward: -204.8948, Q1: -1087.3789, Q2: -1101.5116, TargetQ: -949.3671\n",
      "Epoch 82/81, Total Reward: -295.0712, Q1: -1039.6940, Q2: -1049.5940, TargetQ: -1113.9626\n",
      "Epoch 83/82, Total Reward: -637.7101, Q1: -1186.4774, Q2: -1204.7744, TargetQ: -1103.8358\n",
      "Epoch 84/83, Total Reward: -656.7024, Q1: -888.0099, Q2: -901.0015, TargetQ: -808.2238\n",
      "Epoch 85/84, Total Reward: -59.3424, Q1: -902.7421, Q2: -915.6621, TargetQ: -812.8226\n",
      "Epoch 86/85, Total Reward: -1176.4391, Q1: -924.4693, Q2: -939.3747, TargetQ: -998.5872\n",
      "Epoch 87/86, Total Reward: -127.0853, Q1: -1040.5546, Q2: -1054.3932, TargetQ: -1213.2048\n",
      "Epoch 88/87, Total Reward: -31.0185, Q1: -934.7751, Q2: -939.3303, TargetQ: -1010.5005\n",
      "Epoch 89/88, Total Reward: -167.7235, Q1: -958.8152, Q2: -972.8870, TargetQ: -1023.3732\n",
      "Epoch 90/89, Total Reward: -1789.9134, Q1: -809.7907, Q2: -811.0746, TargetQ: -795.5834\n",
      "Epoch 91/90, Total Reward: -69.4016, Q1: -837.7261, Q2: -850.7568, TargetQ: -862.6689\n",
      "Epoch 92/91, Total Reward: -449.7928, Q1: -924.5934, Q2: -927.9688, TargetQ: -885.6503\n",
      "Epoch 93/92, Total Reward: -810.7226, Q1: -780.9677, Q2: -789.1045, TargetQ: -871.7747\n",
      "Epoch 94/93, Total Reward: -937.3220, Q1: -845.1938, Q2: -854.2980, TargetQ: -1005.2210\n",
      "Epoch 95/94, Total Reward: -1738.5211, Q1: -897.1146, Q2: -901.7134, TargetQ: -952.2770\n",
      "Epoch 96/95, Total Reward: -67.3331, Q1: -900.1066, Q2: -899.8343, TargetQ: -966.6827\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19068\\872869145.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m     76\u001b[0m             \u001b[0mcritic_grad2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape_critic2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcritic_model2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m             \u001b[1;31m# Apply gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m             \u001b[0mcritic_optimizer1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcritic_grad1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcritic_model1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m             \u001b[0mcritic_optimizer2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcritic_grad2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcritic_model2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mpolicy_delay\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Delayed policy updates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtape_actor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m                     \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mactor_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ntk00\\anaconda3\\envs\\neural_network\\lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, grads_and_vars)\u001b[0m\n\u001b[0;32m    461\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    462\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainable_variables\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 463\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    464\u001b[0m         \u001b[1;31m# Return iterations for compat with tf.keras.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    465\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ntk00\\anaconda3\\envs\\neural_network\\lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, grads, trainable_variables)\u001b[0m\n\u001b[0;32m    523\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mscale\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    524\u001b[0m                     \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mg\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mg\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mg\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mscale\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrads\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    525\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    526\u001b[0m                 \u001b[1;31m# Apply gradient updates.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 527\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend_apply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    528\u001b[0m                 \u001b[1;31m# Apply variable constraints after applying gradients.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    529\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mvariable\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrainable_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mvariable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstraint\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ntk00\\anaconda3\\envs\\neural_network\\lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, grads, trainable_variables)\u001b[0m\n\u001b[0;32m    589\u001b[0m             \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_clip_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply_weight_decay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    591\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m             \u001b[1;31m# Run update step.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 593\u001b[1;33m             self._backend_update_step(\n\u001b[0m\u001b[0;32m    594\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainable_variables\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m             )\n\u001b[0;32m    596\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ntk00\\anaconda3\\envs\\neural_network\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\optimizer.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, grads, trainable_variables, learning_rate)\u001b[0m\n\u001b[0;32m    116\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrainable_variables\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m         ]\n\u001b[0;32m    118\u001b[0m         \u001b[0mgrads_and_vars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m         \u001b[0mgrads_and_vars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_all_reduce_sum_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m         tf.__internal__.distribute.interim.maybe_merge_call(\n\u001b[0m\u001b[0;32m    121\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_distributed_tf_update_step\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_distribution_strategy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m             \u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ntk00\\anaconda3\\envs\\neural_network\\lib\\site-packages\\tensorflow\\python\\distribute\\merge_call_interim.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(fn, strategy, *args, **kwargs)\u001b[0m\n\u001b[0;32m     47\u001b[0m   \u001b[0mReturns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[0mThe\u001b[0m \u001b[1;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mcall\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m   \"\"\"\n\u001b[0;32m     50\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mstrategy_supports_no_merge_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     return distribute_lib.get_replica_context().merge_call(\n\u001b[0;32m     54\u001b[0m         fn, args=args, kwargs=kwargs)\n",
      "\u001b[1;32mc:\\Users\\ntk00\\anaconda3\\envs\\neural_network\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\optimizer.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, distribution, grads_and_vars, learning_rate)\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mapply_grad_to_update_var\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m             distribution.extended.update(\n\u001b[0m\u001b[0;32m    135\u001b[0m                 \u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m                 \u001b[0mapply_grad_to_update_var\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m                 \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ntk00\\anaconda3\\envs\\neural_network\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[0;32m   3001\u001b[0m         _get_default_replica_context()):\n\u001b[0;32m   3002\u001b[0m       fn = autograph.tf_convert(\n\u001b[0;32m   3003\u001b[0m           fn, autograph_ctx.control_status_ctx(), convert_by_default=False)\n\u001b[0;32m   3004\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3005\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3006\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3007\u001b[0m       return self._replica_ctx_update(\n\u001b[0;32m   3008\u001b[0m           var, fn, args=args, kwargs=kwargs, group=group)\n",
      "\u001b[1;32mc:\\Users\\ntk00\\anaconda3\\envs\\neural_network\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[0;32m   4072\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4073\u001b[0m     \u001b[1;31m# The implementations of _update() and _update_non_slot() are identical\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4074\u001b[0m     \u001b[1;31m# except _update() passes `var` as the first argument to `fn()`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4075\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_non_slot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\ntk00\\anaconda3\\envs\\neural_network\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, colocate_with, fn, args, kwargs, should_group)\u001b[0m\n\u001b[0;32m   4077\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_update_non_slot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolocate_with\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshould_group\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4078\u001b[0m     \u001b[1;31m# TODO(josh11b): Figure out what we should be passing to UpdateContext()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4079\u001b[0m     \u001b[1;31m# once that value is used for something.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4080\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mUpdateContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4081\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4082\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mshould_group\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4083\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4084\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ntk00\\anaconda3\\envs\\neural_network\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    594\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mControlStatusCtx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mag_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUNSPECIFIED\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 596\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\ntk00\\anaconda3\\envs\\neural_network\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\optimizer.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(var, grad, learning_rate)\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mapply_grad_to_update_var\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\ntk00\\anaconda3\\envs\\neural_network\\lib\\site-packages\\keras\\src\\optimizers\\adam.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, gradient, variable, learning_rate)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_momentums\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_variable_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvariable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m         \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_velocities\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_variable_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvariable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m         \u001b[0malpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta_2_power\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta_1_power\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m         self.assign_add(\n\u001b[0;32m    120\u001b[0m             \u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubtract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbeta_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ntk00\\anaconda3\\envs\\neural_network\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\ntk00\\anaconda3\\envs\\neural_network\\lib\\site-packages\\tensorflow\\python\\framework\\override_binary_operator.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;31m# TODO(b/178860388): Figure out why binary_op_wrapper and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[1;31m#   r_binary_op_wrapper use different force_same_dtype values.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmaybe_promote_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m         \u001b[1;31m# Even if dispatching the op failed, the RHS may be a tensor aware\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;31m# object that can implement the operator with knowledge of itself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[1;31m# and the tensor.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ntk00\\anaconda3\\envs\\neural_network\\lib\\site-packages\\tensorflow\\python\\ops\\tensor_math_operator_overrides.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_truediv_factory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m   \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtruediv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\ntk00\\anaconda3\\envs\\neural_network\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\ntk00\\anaconda3\\envs\\neural_network\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1258\u001b[0m       \u001b[1;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1260\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1261\u001b[1;33m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1262\u001b[0m         \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1263\u001b[0m         \u001b[1;31m# TypeError, when given unexpected types.  So we need to catch both.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1264\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_dispatch_handler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ntk00\\anaconda3\\envs\\neural_network\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   1503\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1504\u001b[0m   \u001b[0mRaises\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1505\u001b[0m     \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIf\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mx\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0my\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mhave\u001b[0m \u001b[0mdifferent\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1506\u001b[0m   \"\"\"\n\u001b[1;32m-> 1507\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0m_truediv_python3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\ntk00\\anaconda3\\envs\\neural_network\\lib\\site-packages\\tensorflow\\python\\ops\\weak_tensor_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_auto_dtype_conversion_enabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m     \u001b[0mbound_arguments\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[0mbound_arguments\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_defaults\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[0mbound_kwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbound_arguments\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marguments\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ntk00\\anaconda3\\envs\\neural_network\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   1441\u001b[0m           f\"of {{{', '.join([repr(x) for x in _TRUEDIV_TABLE.keys()])}}}.\")\n\u001b[0;32m   1442\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1443\u001b[0m       \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1444\u001b[0m       \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1445\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreal_div\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\ntk00\\anaconda3\\envs\\neural_network\\lib\\site-packages\\tensorflow\\python\\ops\\weak_tensor_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_auto_dtype_conversion_enabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m     \u001b[0mbound_arguments\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[0mbound_arguments\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_defaults\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[0mbound_kwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbound_arguments\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marguments\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ntk00\\anaconda3\\envs\\neural_network\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   8163\u001b[0m         _ctx, \"RealDiv\", name, x, y)\n\u001b[0;32m   8164\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8165\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8166\u001b[0m       \u001b[0m_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 8167\u001b[1;33m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   8168\u001b[0m       \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8169\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8170\u001b[0m       _result = _dispatcher_for_real_div(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "history = []\n",
    "policy_delay = 2  # Delayed policy updates\n",
    "step = 0\n",
    "total_reward = 0.0\n",
    "frequency = 500  # Hz\n",
    "state_theta_dot = np.array([0,0], dtype=np.float64)\n",
    "state_alpha_dot = np.array([0,0], dtype=np.float64)\n",
    "\n",
    "with QubeServo3(hardware = 1, pendulum = 1, frequency=10) as board:\n",
    "    while True: \n",
    "        avg_q1, avg_q2, avg_target_q = 0.0, 0.0, 0.0\n",
    "        step += 1 \n",
    "        board.read_outputs()\n",
    "        theta = board.motorPosition * -1\n",
    "        alpha = board.pendulumPosition \n",
    "        theta = np.clip(theta, -np.pi/2, np.pi/2)\n",
    "\n",
    "        theta_dot, state_theta_dot = ddt_filter(theta, state_theta_dot, 50, 1/frequency)\n",
    "        alpha_dot, state_alpha_dot = ddt_filter(alpha, state_alpha_dot, 100, 1/frequency)\n",
    "\n",
    "        state = np.array([theta, theta_dot, alpha, alpha_dot], dtype=np.float32)\n",
    "\n",
    "        action = actor_model(tf.convert_to_tensor([state], dtype=tf.float32)).numpy()[0]\n",
    "        action = action + np.random.normal(0, 0.1, size=action_size)  # Add exploration noise\n",
    "        action = np.clip(action, -2.0, 2.0) \n",
    "        board.write_voltage(action)\n",
    "\n",
    "        board.read_outputs()\n",
    "        next_theta = board.motorPosition * -1\n",
    "        next_alpha = board.pendulumPosition\n",
    "        next_theta = np.clip(next_theta, -np.pi/2, np.pi/2)\n",
    "        next_theta_dot, state_theta_dot = ddt_filter(next_theta, state_theta_dot, 50, 1/frequency)\n",
    "        next_alpha_dot, state_alpha_dot = ddt_filter(next_alpha, state_alpha_dot, 100, 1/frequency)\n",
    "        next_state = np.array([next_theta, next_theta_dot, next_alpha, next_alpha_dot], dtype=np.float32)\n",
    "\n",
    "        wrapped_alpha = ((alpha - np.pi + np.pi) % (2*np.pi)) - np.pi\n",
    "        reward = -(wrapped_alpha**2 + 0.1*alpha_dot**2 + 0.1*action**2)\n",
    "        total_reward += reward\n",
    "\n",
    "        replay_buffer.store(state, action, reward, next_state, False)\n",
    "        state = next_state\n",
    "        \n",
    "        if replay_buffer.size() >= batch_size:\n",
    "            states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "            states      = tf.convert_to_tensor(states, dtype=tf.float32)\n",
    "            actions     = tf.convert_to_tensor(actions.reshape(-1,1), dtype=tf.float32)\n",
    "            rewards     = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
    "            next_states = tf.convert_to_tensor(next_states, dtype=tf.float32)\n",
    "            dones       = tf.convert_to_tensor(dones, dtype=tf.float32)\n",
    "\n",
    "            # add clipped noise to target action\n",
    "            noise = np.clip(np.random.normal(0, 0.2, size=actions.shape), -0.5, 0.5)\n",
    "            next_actions = target_actor(next_states) + noise\n",
    "            next_actions = tf.clip_by_value(next_actions, -2.0, 2.0)  # Pendulum action bounds\n",
    "\n",
    "            # Compute target Q-values with both critics\n",
    "            target1 = tf.squeeze(target_critic1([next_states, next_actions]), axis=1)\n",
    "            target2 = tf.squeeze(target_critic2([next_states, next_actions]), axis=1)\n",
    "            target_q = rewards + gamma * (1 - dones) * tf.minimum(target1, target2)\n",
    "\n",
    "            with tf.GradientTape() as tape_critic1, tf.GradientTape() as tape_critic2:\n",
    "                q1 = critic_model1([states, actions], training=True)\n",
    "                q2 = critic_model2([states, actions], training=True)\n",
    "\n",
    "                # Compute losses\n",
    "                loss1 = tf.keras.losses.MSE(target_q, q1)\n",
    "                loss2 = tf.keras.losses.MSE(target_q, q2)\n",
    "\n",
    "            avg_q1 = tf.reduce_mean(q1).numpy().item()\n",
    "            avg_q2 = tf.reduce_mean(q2).numpy().item()\n",
    "            avg_target_q = tf.reduce_mean(target_q).numpy().item()\n",
    "\n",
    "            # Get gradients for each critic once\n",
    "            critic_grad1 = tape_critic1.gradient(loss1, critic_model1.trainable_variables)\n",
    "            critic_grad2 = tape_critic2.gradient(loss2, critic_model2.trainable_variables)\n",
    "\n",
    "            # Apply gradients\n",
    "            critic_optimizer1.apply_gradients(zip(critic_grad1, critic_model1.trainable_variables))\n",
    "            critic_optimizer2.apply_gradients(zip(critic_grad2, critic_model2.trainable_variables))\n",
    "            if step % policy_delay == 0:  # Delayed policy updates\n",
    "                with tf.GradientTape() as tape_actor: \n",
    "                    action = actor_model(states)\n",
    "                    actor_loss = -tf.reduce_mean(critic_model1([states, action]))\n",
    "\n",
    "                actor_grad = tape_actor.gradient(actor_loss, actor_model.trainable_variables)\n",
    "                actor_optimizer.apply_gradients(zip(actor_grad, actor_model.trainable_variables))\n",
    "\n",
    "                soft_update(target_actor.variables, actor_model.variables, tau=0.005)\n",
    "                soft_update(target_critic1.variables, critic_model1.variables, tau=0.005)\n",
    "                soft_update(target_critic2.variables, critic_model2.variables, tau=0.005)\n",
    "        history.append(total_reward)\n",
    "        if step % 1 == 0:\n",
    "            print(f\"Epoch {step + 1}/{step}, Total Reward: {float(reward):.4f}, \"\n",
    "            f\"Q1: {avg_q1:.4f}, Q2: {avg_q2:.4f}, TargetQ: {avg_target_q:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fd84a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stopping (Ctrl+C). Saving\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# batch_size = 32\n",
    "# history = []\n",
    "# policy_delay = 2  # Delayed policy updates\n",
    "# step = 0\n",
    "# total_reward = 0.0\n",
    "\n",
    "# try:\n",
    "#     total_reward = 0.0\n",
    "#     while True:\n",
    "#         step += 1\n",
    "    \n",
    "#         # 1) read state\n",
    "#         board.read_encoder(encoder_channels, len(encoder_channels), counts)\n",
    "#         theta_arm  = counts[0] * ARM_RAD_PER_COUNT\n",
    "#         theta_pend = counts[1] * PEND_RAD_PER_COUNT\n",
    "#         theta_arm_dot  = (theta_arm  - theta_arm_prev)  / dt\n",
    "#         theta_pend_dot = (theta_pend - theta_pend_prev) / dt\n",
    "#         state = np.array([theta_arm, theta_pend, theta_arm_dot, theta_pend_dot], dtype=np.float32)\n",
    "\n",
    "#         # 2) select action\n",
    "#         action_vec = actor_model(tf.convert_to_tensor([state], dtype=tf.float32)).numpy()[0]\n",
    "#         action_val = float(np.clip(action_vec[0], -2.0, 2.0))  # scalar in [-2,2]; tune to your safe V range\n",
    "\n",
    "#         # 3) apply action (analog write wants numpy float64 buffer)\n",
    "#         voltages = np.array([action_val], dtype=np.float64)\n",
    "#         board.write_analog(motor_channels, len(motor_channels), voltages)\n",
    "\n",
    "#         # 4) get next_state after action\n",
    "#         time.sleep(dt)  # maintain loop timing around the actuation\n",
    "#         board.read_encoder(encoder_channels, len(encoder_channels), counts)\n",
    "#         next_theta_arm  = counts[0] * ARM_RAD_PER_COUNT\n",
    "#         next_theta_pend = counts[1] * PEND_RAD_PER_COUNT\n",
    "#         next_theta_arm_dot  = (next_theta_arm  - theta_arm)  / dt\n",
    "#         next_theta_pend_dot = (next_theta_pend - theta_pend) / dt\n",
    "#         next_state = np.array([next_theta_arm, next_theta_pend, next_theta_arm_dot, next_theta_pend_dot], dtype=np.float32)\n",
    "\n",
    "#         # 5) reward (example: upright pendulum, gentle motion)\n",
    "#         reward = - ( (np.angle(np.exp(1j*(next_theta_pend - np.pi))))**2\n",
    "#                      + 0.1*next_theta_pend_dot**2 + 0.01*action_val**2 )\n",
    "#         total_reward += reward\n",
    "\n",
    "#         # 6) store\n",
    "#         replay_buffer.store(state, action_val, reward, next_state, False)\n",
    "\n",
    "#         # 7) train if enough samples\n",
    "#         if replay_buffer.size() >= batch_size:\n",
    "#             states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "#             states      = tf.convert_to_tensor(states, dtype=tf.float32)\n",
    "#             actions     = tf.convert_to_tensor(actions.reshape(-1,1), dtype=tf.float32)\n",
    "#             rewards     = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
    "#             next_states = tf.convert_to_tensor(next_states, dtype=tf.float32)\n",
    "#             dones       = tf.convert_to_tensor(dones, dtype=tf.float32)\n",
    "\n",
    "#             # target policy smoothing\n",
    "#             noise = np.clip(np.random.normal(0, 0.2, size=(actions.shape[0], 1)), -0.5, 0.5)\n",
    "#             target_act = tf.clip_by_value(target_actor(next_states) + noise, -2.0, 2.0)\n",
    "\n",
    "#             # twin critics target\n",
    "#             t1 = tf.squeeze(target_critic1([next_states, target_act]), axis=1)\n",
    "#             t2 = tf.squeeze(target_critic2([next_states, target_act]), axis=1)\n",
    "#             target_q = rewards + gamma * (1.0 - dones) * tf.minimum(t1, t2)\n",
    "\n",
    "#             # critic updates\n",
    "#             with tf.GradientTape() as tape1, tf.GradientTape() as tape2:\n",
    "#                 q1 = tf.squeeze(critic_model1([states, actions]), axis=1)\n",
    "#                 q2 = tf.squeeze(critic_model2([states, actions]), axis=1)\n",
    "#                 loss1 = tf.keras.losses.MSE(target_q, q1)\n",
    "#                 loss2 = tf.keras.losses.MSE(target_q, q2)\n",
    "#             critic_optimizer1.apply_gradients(zip(tape1.gradient(loss1, critic_model1.trainable_variables),\n",
    "#                                                   critic_model1.trainable_variables))\n",
    "#             critic_optimizer2.apply_gradients(zip(tape2.gradient(loss2, critic_model2.trainable_variables),\n",
    "#                                                   critic_model2.trainable_variables))\n",
    "\n",
    "#             # delayed actor + target updates\n",
    "#             if step % policy_delay == 0:\n",
    "#                 with tf.GradientTape() as tape_actor:\n",
    "#                     pi = actor_model(states)\n",
    "#                     actor_loss = -tf.reduce_mean(critic_model1([states, pi]))\n",
    "#                 actor_optimizer.apply_gradients(zip(tape_actor.gradient(actor_loss, actor_model.trainable_variables),\n",
    "#                                                     actor_model.trainable_variables))\n",
    "#                 soft_update(target_actor.variables,   actor_model.variables,   tau=0.005)\n",
    "#                 soft_update(target_critic1.variables, critic_model1.variables, tau=0.005)\n",
    "#                 soft_update(target_critic2.variables, critic_model2.variables, tau=0.005)\n",
    "\n",
    "#         if step % 100 == 0:\n",
    "#             print(f\"Step {step}  reward_sum: {total_reward:.2f}\")\n",
    "\n",
    "#         # update prev angles for next derivative\n",
    "#         theta_arm_prev, theta_pend_prev = next_theta_arm, next_theta_pend\n",
    "\n",
    "# except KeyboardInterrupt:\n",
    "#     print(\"\\nStopping (Ctrl+C). Saving\")\n",
    "# finally:\n",
    "#     # save weights (use .save_weights if you prefer checkpoint style)\n",
    "#     actor_model.save_weights(\"saves/quanser/actor_model.weights.h5\")\n",
    "#     critic_model1.save_weights(\"saves/quanser/critic_model1.weights.h5\")\n",
    "#     critic_model2.save_weights(\"saves/quanser/critic_model2.weights.h5\")\n",
    "#     # set motor to 0V and close safely\n",
    "#     board.write_analog(motor_channels, 1, np.array([0.0], dtype=np.float64))\n",
    "#     board.close()\n",
    "#     print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff966c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ntk00\\AppData\\Local\\Temp\\ipykernel_20244\\4268336479.py:10: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  u = float(action)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "a bytes-like object is required, not 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[70], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m u \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(action)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# send to motor\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[43mboard\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_analog\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmotor_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mu\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(dt)  \u001b[38;5;66;03m# ~0.01s\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ntk00\\anaconda3\\envs\\neural_network\\lib\\site-packages\\quanser\\hardware\\hil.py:2732\u001b[0m, in \u001b[0;36mHIL.write_analog\u001b[1;34m(self, channels, num_channels, buffer)\u001b[0m\n\u001b[0;32m   2675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrite_analog\u001b[39m(\u001b[38;5;28mself\u001b[39m, channels, num_channels, buffer):\n\u001b[0;32m   2676\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Writes to analog outputs immediately. The function does not return until the data has been written.\u001b[39;00m\n\u001b[0;32m   2677\u001b[0m \n\u001b[0;32m   2678\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2727\u001b[0m \n\u001b[0;32m   2728\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   2729\u001b[0m     result \u001b[38;5;241m=\u001b[39m hil_lib\u001b[38;5;241m.\u001b[39mhil_write_analog(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_card \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_card \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m ffi\u001b[38;5;241m.\u001b[39mNULL,\n\u001b[0;32m   2730\u001b[0m                                       ffi\u001b[38;5;241m.\u001b[39mfrom_buffer(_UINT32_ARRAY, channels) \u001b[38;5;28;01mif\u001b[39;00m channels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m ffi\u001b[38;5;241m.\u001b[39mNULL,\n\u001b[0;32m   2731\u001b[0m                                       num_channels,\n\u001b[1;32m-> 2732\u001b[0m                                       \u001b[43mffi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_buffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_DOUBLE_ARRAY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m ffi\u001b[38;5;241m.\u001b[39mNULL)\n\u001b[0;32m   2733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   2734\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m HILError(result)\n",
      "File \u001b[1;32mc:\\Users\\ntk00\\anaconda3\\envs\\neural_network\\lib\\site-packages\\cffi\\api.py:365\u001b[0m, in \u001b[0;36mFFI.from_buffer\u001b[1;34m(self, cdecl, python_buffer, require_writable)\u001b[0m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(cdecl, basestring):\n\u001b[0;32m    364\u001b[0m     cdecl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_typeof(cdecl)\n\u001b[1;32m--> 365\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_buffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcdecl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpython_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    366\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mrequire_writable\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: a bytes-like object is required, not 'list'"
     ]
    }
   ],
   "source": [
    "# while True:\n",
    "#     # read state\n",
    "#     # board.close()\n",
    "#     board.read_encoder(encoder_channels, len(encoder_channels), counts)\n",
    "#     theta_arm = counts[0] * ARM_RAD_PER_COUNT\n",
    "#     theta_pend = counts[1] * PEND_RAD_PER_COUNT\n",
    "\n",
    "#     # compute action from policy\n",
    "#     action = actor_model(tf.convert_to_tensor([state], dtype=tf.float32)).numpy()[0]\n",
    "#     u = float(action)\n",
    "\n",
    "#     # send to motor\n",
    "#     board.write_analog(motor_channels, 1, [u])\n",
    "\n",
    "#     time.sleep(dt)  # ~0.01s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e598aa56",
   "metadata": {},
   "outputs": [
    {
     "ename": "HILError",
     "evalue": "-1410",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHILError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcollections\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m deque\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# --- HIL/QUBE setup ---\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# board.close()\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m board \u001b[38;5;241m=\u001b[39m \u001b[43mHIL\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mqube_servo3_usb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m encoder_channels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39muint32)\n\u001b[0;32m     10\u001b[0m motor_channels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39muint32)\n",
      "File \u001b[1;32mc:\\Users\\ntk00\\anaconda3\\envs\\neural_network\\lib\\site-packages\\quanser\\hardware\\hil.py:663\u001b[0m, in \u001b[0;36mHIL.__init__\u001b[1;34m(self, card_type, card_identifier)\u001b[0m\n\u001b[0;32m    661\u001b[0m \u001b[38;5;66;03m# If non-default arguments are passed, attempt to open the card.\u001b[39;00m\n\u001b[0;32m    662\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m card_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 663\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcard_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcard_identifier\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ntk00\\anaconda3\\envs\\neural_network\\lib\\site-packages\\quanser\\hardware\\hil.py:774\u001b[0m, in \u001b[0;36mHIL.open\u001b[1;34m(self, card_type, card_identifier)\u001b[0m\n\u001b[0;32m    771\u001b[0m     result \u001b[38;5;241m=\u001b[39m hil_lib\u001b[38;5;241m.\u001b[39mhil_open(card_type\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUTF-8\u001b[39m\u001b[38;5;124m'\u001b[39m), card_identifier\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUTF-8\u001b[39m\u001b[38;5;124m'\u001b[39m), card)\n\u001b[0;32m    773\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 774\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HILError(result)\n\u001b[0;32m    776\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_card \u001b[38;5;241m=\u001b[39m card[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mHILError\u001b[0m: -1410"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "\n",
    "# --- HIL/QUBE setup ---\n",
    "board.close()\n",
    "board = HIL(\"qube_servo3_usb\", \"0\")\n",
    "encoder_channels = np.array([0, 1], dtype=np.uint32)\n",
    "motor_channels = np.array([0], dtype=np.uint32)\n",
    "counts = np.zeros(2, dtype=np.int32)\n",
    "\n",
    "ENCODER_RES = 2048\n",
    "ARM_RAD_PER_COUNT = 2*np.pi / ENCODER_RES\n",
    "PEND_RAD_PER_COUNT = 2*np.pi / ENCODER_RES\n",
    "dt = 0.01  # 10 ms loop\n",
    "\n",
    "# --- Replay Buffer ---\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=100000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    def store(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    def sample(self, batch_size):\n",
    "        batch = np.array(random.sample(self.buffer, batch_size))\n",
    "        states, actions, rewards, next_states, dones = map(np.stack, zip(*batch))\n",
    "        return states, actions, rewards, next_states, dones\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "replay_buffer = ReplayBuffer()\n",
    "\n",
    "# --- Soft update ---\n",
    "def soft_update(target_weights, online_weights, tau=0.005):\n",
    "    for (target, online) in zip(target_weights, online_weights):\n",
    "        target.assign(target * (1 - tau) + online * tau)\n",
    "\n",
    "# --- TD3 models already defined: actor_model, critic_model1, critic_model2, \n",
    "# target_actor, target_critic1, target_critic2\n",
    "# optimizers: actor_optimizer, critic_optimizer1, critic_optimizer2\n",
    "\n",
    "state_size = 4\n",
    "action_size = 1\n",
    "gamma = 0.99\n",
    "batch_size = 32\n",
    "policy_delay = 2\n",
    "step = 0\n",
    "\n",
    "theta_arm_prev = 0.0\n",
    "theta_pend_prev = 0.0\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        step += 1\n",
    "\n",
    "        # --- 1. Read state ---\n",
    "        board.read_encoder(encoder_channels, len(encoder_channels), counts)\n",
    "        theta_arm = counts[0] * ARM_RAD_PER_COUNT\n",
    "        theta_pend = counts[1] * PEND_RAD_PER_COUNT\n",
    "\n",
    "        theta_arm_dot = (theta_arm - theta_arm_prev) / dt\n",
    "        theta_pend_dot = (theta_pend - theta_pend_prev) / dt\n",
    "        theta_arm_prev, theta_pend_prev = theta_arm, theta_pend\n",
    "\n",
    "        state = np.array([theta_arm, theta_pend, theta_arm_dot, theta_pend_dot], dtype=np.float32)\n",
    "\n",
    "        # --- 2. Compute action ---\n",
    "        action = actor_model(tf.convert_to_tensor([state], dtype=tf.float32)).numpy()[0]\n",
    "        u_array = np.array([float(action)], dtype=np.float64)\n",
    "\n",
    "        # --- 3. Apply action ---\n",
    "        board.write_analog(motor_channels, 1, u_array)\n",
    "\n",
    "        # --- 4. Read next state ---\n",
    "        board.read_encoder(encoder_channels, len(encoder_channels), counts)\n",
    "        next_theta_arm = counts[0] * ARM_RAD_PER_COUNT\n",
    "        next_theta_pend = counts[1] * PEND_RAD_PER_COUNT\n",
    "        next_theta_arm_dot = (next_theta_arm - theta_arm) / dt\n",
    "        next_theta_pend_dot = (next_theta_pend - theta_pend) / dt\n",
    "        next_state = np.array([next_theta_arm, next_theta_pend, next_theta_arm_dot, next_theta_pend_dot], dtype=np.float32)\n",
    "\n",
    "        # --- 5. Compute reward ---\n",
    "        reward = - (next_theta_pend**2 + 0.1 * next_theta_pend_dot**2)\n",
    "\n",
    "        # --- 6. Store transition ---\n",
    "        replay_buffer.store(state, action, reward, next_state, False)\n",
    "\n",
    "        # --- 7. Train TD3 ---\n",
    "        if replay_buffer.size() >= batch_size:\n",
    "            states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "            states = tf.convert_to_tensor(states, dtype=tf.float32)\n",
    "            actions = tf.convert_to_tensor(actions, dtype=tf.float32)\n",
    "            rewards = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
    "            next_states = tf.convert_to_tensor(next_states, dtype=tf.float32)\n",
    "            dones = tf.convert_to_tensor(dones, dtype=tf.float32)\n",
    "\n",
    "            # Target actions with clipped noise\n",
    "            noise = np.clip(np.random.normal(0, 0.2, size=actions.shape), -0.5, 0.5)\n",
    "            next_actions = tf.clip_by_value(target_actor(next_states) + noise, -2.0, 2.0)\n",
    "\n",
    "            # Target Q-values\n",
    "            target1 = tf.squeeze(target_critic1([next_states, next_actions]), axis=1)\n",
    "            target2 = tf.squeeze(target_critic2([next_states, next_actions]), axis=1)\n",
    "            target_q = rewards + gamma * (1 - dones) * tf.minimum(target1, target2)\n",
    "\n",
    "            # Critic updates\n",
    "            with tf.GradientTape() as tape1, tf.GradientTape() as tape2:\n",
    "                q1 = critic_model1([states, actions], training=True)\n",
    "                q2 = critic_model2([states, actions], training=True)\n",
    "                loss1 = tf.keras.losses.MSE(target_q, q1)\n",
    "                loss2 = tf.keras.losses.MSE(target_q, q2)\n",
    "\n",
    "            critic_grad1 = tape1.gradient(loss1, critic_model1.trainable_variables)\n",
    "            critic_grad2 = tape2.gradient(loss2, critic_model2.trainable_variables)\n",
    "            critic_optimizer1.apply_gradients(zip(critic_grad1, critic_model1.trainable_variables))\n",
    "            critic_optimizer2.apply_gradients(zip(critic_grad2, critic_model2.trainable_variables))\n",
    "\n",
    "            # Delayed actor update\n",
    "            if step % policy_delay == 0:\n",
    "                with tf.GradientTape() as tape_actor:\n",
    "                    act = actor_model(states)\n",
    "                    actor_loss = -tf.reduce_mean(critic_model1([states, act]))\n",
    "                actor_grad = tape_actor.gradient(actor_loss, actor_model.trainable_variables)\n",
    "                actor_optimizer.apply_gradients(zip(actor_grad, actor_model.trainable_variables))\n",
    "\n",
    "                soft_update(target_actor.variables, actor_model.variables)\n",
    "                soft_update(target_critic1.variables, critic_model1.variables)\n",
    "                soft_update(target_critic2.variables, critic_model2.variables)\n",
    "\n",
    "        # --- 8. Sleep to maintain loop ---\n",
    "        time.sleep(dt)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Stopping (Ctrl+C) and saving models...\")\n",
    "\n",
    "finally:\n",
    "    # Save models\n",
    "    actor_model.save(\"td3_actor.h5\")\n",
    "    critic_model1.save(\"td3_critic1.h5\")\n",
    "    critic_model2.save(\"td3_critic2.h5\")\n",
    "    board.close()\n",
    "    print(\"Training finished and models saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neural_network",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
