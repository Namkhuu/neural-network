{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "b8ceea56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Lambda\n",
    "from keras.layers import Concatenate\n",
    "from collections import deque\n",
    "import random\n",
    "from quanser.hardware import HIL \n",
    "from pal.products.qube import QubeServo3\n",
    "from pal.utilities.math import SignalGenerator, ddt_filter\n",
    "from threading import Thread\n",
    "import time\n",
    "import signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "8674aa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # board.close()\n",
    "# # Open connection to QUBE\n",
    "# # board = HIL(\"qube_servo3_usb\", \"0\")\n",
    "\n",
    "# encoder_channels = np.array([0, 1], dtype=np.uint32)\n",
    "# motor_channels = np.array([0], dtype=np.uint32)\n",
    "# counts = np.zeros(2, dtype=np.int32)\n",
    "\n",
    "# ENCODER_RES = 2048\n",
    "# ARM_RAD_PER_COUNT = 2*np.pi / ENCODER_RES\n",
    "# PEND_RAD_PER_COUNT = 2*np.pi / ENCODER_RES\n",
    "\n",
    "# dt = 0.01  # 10 ms\n",
    "# theta_arm_prev  = counts[0] * ARM_RAD_PER_COUNT\n",
    "# theta_pend_prev = counts[1] * PEND_RAD_PER_COUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "49e7c226",
   "metadata": {},
   "outputs": [],
   "source": [
    "global KILL_THREAD\n",
    "KILL_THREAD = False\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=100000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    def store(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = map(np.array, zip(*batch))\n",
    "        return (np.array(states, dtype=np.float32),\n",
    "            np.array(actions, dtype=np.float32),\n",
    "            np.array(rewards, dtype=np.float32),\n",
    "            np.array(next_states, dtype=np.float32),\n",
    "            np.array(dones, dtype=np.float32))\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "def soft_update(target_weights, online_weights, tau=0.005):\n",
    "    for (target, online) in zip(target_weights, online_weights):\n",
    "        target.assign(target * (1 - tau) + online * tau) \n",
    "\n",
    "def sig_handler(*args): \n",
    "    global KILL_THREAD\n",
    "    KILL_THREAD = True\n",
    "\n",
    "signal.signal(signal.SIGINT, sig_handler)\n",
    "\n",
    "replay_buffer = ReplayBuffer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "f8509c67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x127cb6f6cb0>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_size = 4\n",
    "action_size = 1\n",
    "gamma = 0.99 # discount rate\n",
    "learning_rate = 0.001 # learning rate\n",
    "\n",
    "# Define the actor model\n",
    "states_inputs = Input(shape=(state_size,))\n",
    "dense = Dense(128, activation='relu')(states_inputs)\n",
    "dense = Dense(128, activation='relu')(dense)\n",
    "outputs = Dense(action_size, activation='tanh')(dense)\n",
    "outputs = keras.layers.Lambda(lambda x: x * 1.75)(outputs)  # Scale action to [-12, 12] Voltage\n",
    "actor_model = Model(inputs=states_inputs, outputs=outputs)\n",
    "\n",
    "# Critic 1\n",
    "state_input1 = Input(shape=(state_size,))\n",
    "action_input1 = Input(shape=(action_size,))\n",
    "concat1 = Concatenate()([state_input1, action_input1])\n",
    "dense1 = Dense(128, activation='relu')(concat1)\n",
    "dense1 = Dense(128, activation='relu')(dense1)\n",
    "output1 = Dense(1)(dense1)\n",
    "critic_model1 = Model([state_input1, action_input1], output1)\n",
    "\n",
    "# Critic 2\n",
    "state_input2 = Input(shape=(state_size,))\n",
    "action_input2 = Input(shape=(action_size,))\n",
    "concat2 = Concatenate()([state_input2, action_input2])\n",
    "dense2 = Dense(128, activation='relu')(concat2)\n",
    "dense2 = Dense(128, activation='relu')(dense2)\n",
    "output2 = Dense(1)(dense2)\n",
    "critic_model2 = Model([state_input2, action_input2], output2)\n",
    "\n",
    "try:\n",
    "    actor_model.load_weights('saves/quanser/actor_model.weights.h5')\n",
    "    critic_model1.load_weights('saves/quanser/critic_model1.weights.h5')\n",
    "    critic_model2.load_weights('saves/quanser/critic_model2.weights.h5')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "actor_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "critic_optimizer1 = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "critic_optimizer2 = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "target_actor = keras.models.clone_model(actor_model)\n",
    "target_actor.set_weights(actor_model.get_weights())\n",
    "\n",
    "target_critic1 = keras.models.clone_model(critic_model1)\n",
    "target_critic1.set_weights(critic_model1.get_weights())\n",
    "target_critic2 = keras.models.clone_model(critic_model2)\n",
    "target_critic2.set_weights(critic_model2.get_weights())\n",
    "\n",
    "ckpt = tf.train.Checkpoint(actor_optimizer=actor_optimizer,\n",
    "                           critic_optimizer1=critic_optimizer1, \n",
    "                           critic_optimizer2=critic_optimizer2)\n",
    "\n",
    "# Restore the latest checkpoint with optimizer states\n",
    "ckpt.restore(tf.train.latest_checkpoint(\"saves/quanser/optimizers_ckpt\")).expect_partial()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "844cf3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency = 500  # Hz\n",
    "# state_theta_dot = np.array([0,0], dtype=np.float64)\n",
    "# state_alpha_dot = np.array([0,0], dtype=np.float64)\n",
    "# with QubeServo3(hardware = 1, pendulum = 1, frequency=frequency) as board:\n",
    "#     while True:\n",
    "#         # Have to initialize the board first before reading motorPosition or it won't read\n",
    "#         board.read_outputs()\n",
    "#         theta = board.motorPosition \n",
    "#         alpha = -board.pendulumPosition \n",
    "#         theta_dot, state_theta_dot = ddt_filter(theta, state_theta_dot, 50, 1/frequency)\n",
    "#         # u - input\n",
    "#         # state - previous state returned by this function -- initialize to np.array([0,0], dtype=np.float64)\n",
    "#         # Ts - sample time in seconds\n",
    "#         # A - filter bandwidth in rad/s\n",
    "#         alpha_dot, state_alpha_dot = ddt_filter(alpha, state_alpha_dot, 100, 1/frequency)\n",
    "#         alpha = np.mod(alpha, 2*np.pi) - np.pi\n",
    "#         alpha = np.cos(alpha)\n",
    "#         theta = np.clip(theta, (-5*np.pi)/8, (5*np.pi)/8)\n",
    "#         reward = -(alpha**2 + 0.0001*alpha_dot**2 )\n",
    "#         print(f\"Theta: {theta:.3f}, Theta dot: {theta_dot:.3f}, Alpha: {alpha:.3f}, Alpha dot: {alpha_dot:.3f}\", \n",
    "#               f\"Reward: {reward:.3f}\")\n",
    "#         time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7442b395",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ntk00\\AppData\\Local\\Temp\\ipykernel_22108\\3014037386.py:70: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  reward = float(reward)\n",
      "C:\\Users\\ntk00\\AppData\\Local\\Temp\\ipykernel_22108\\3014037386.py:131: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  f\"voltage: {float(np.array(action)):.2f}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Total Reward: -815.6732, Q1: 0.0000, Q2: 0.0000, TargetQ: 0.0000 alpha: -3.1416 alpha_dot: -285.5993 voltage: 1.75\n",
      "Epoch 2, Total Reward: -240.5498, Q1: 0.0000, Q2: 0.0000, TargetQ: 0.0000 alpha: -2.6569 alpha_dot: -151.1761 voltage: 1.63\n",
      "Epoch 3, Total Reward: -1877.9787, Q1: 0.0000, Q2: 0.0000, TargetQ: 0.0000 alpha: 3.1109 alpha_dot: 425.8297 voltage: 1.66\n",
      "Epoch 4, Total Reward: -501.1047, Q1: 0.0000, Q2: 0.0000, TargetQ: 0.0000 alpha: 2.2580 alpha_dot: 212.1891 voltage: 1.75\n",
      "Epoch 5, Total Reward: -1101.2294, Q1: 0.0000, Q2: 0.0000, TargetQ: 0.0000 alpha: -2.8409 alpha_dot: -326.4160 voltage: 1.75\n",
      "Epoch 6, Total Reward: -496.7432, Q1: 0.0000, Q2: 0.0000, TargetQ: 0.0000 alpha: -2.7397 alpha_dot: -217.1661 voltage: 1.75\n",
      "Epoch 7, Total Reward: -777.9796, Q1: 0.0000, Q2: 0.0000, TargetQ: 0.0000 alpha: 2.9084 alpha_dot: 275.6450 voltage: -1.75\n",
      "Epoch 8, Total Reward: -261.7164, Q1: 0.0000, Q2: 0.0000, TargetQ: 0.0000 alpha: 2.5679 alpha_dot: 160.2579 voltage: -1.75\n",
      "Epoch 9, Total Reward: -251.3726, Q1: 0.0000, Q2: 0.0000, TargetQ: 0.0000 alpha: 3.0250 alpha_dot: 143.0054 voltage: -1.75\n",
      "Epoch 10, Total Reward: -2256.4054, Q1: 0.0000, Q2: 0.0000, TargetQ: 0.0000 alpha: -3.0741 alpha_dot: -454.6259 voltage: 1.61\n",
      "Epoch 11, Total Reward: -625.9406, Q1: 0.0000, Q2: 0.0000, TargetQ: 0.0000 alpha: -2.1506 alpha_dot: -236.4102 voltage: 1.75\n",
      "Epoch 12, Total Reward: -783.5644, Q1: 0.0000, Q2: 0.0000, TargetQ: 0.0000 alpha: 2.4943 alpha_dot: 279.3195 voltage: -1.75\n",
      "Epoch 13, Total Reward: -410.8995, Q1: 0.0000, Q2: 0.0000, TargetQ: 0.0000 alpha: 2.5802 alpha_dot: 198.6458 voltage: 1.73\n",
      "Epoch 14, Total Reward: -519.4203, Q1: 0.0000, Q2: 0.0000, TargetQ: 0.0000 alpha: -2.1936 alpha_dot: -214.2344 voltage: 1.75\n",
      "Epoch 15, Total Reward: -859.9623, Q1: 0.0000, Q2: 0.0000, TargetQ: 0.0000 alpha: 2.4298 alpha_dot: 286.0255 voltage: 1.56\n",
      "Epoch 16, Total Reward: -514.9802, Q1: 0.0000, Q2: 0.0000, TargetQ: 0.0000 alpha: 2.6783 alpha_dot: 220.3510 voltage: 1.65\n",
      "Epoch 17, Total Reward: -467.4331, Q1: 0.0000, Q2: 0.0000, TargetQ: 0.0000 alpha: -2.2212 alpha_dot: -211.4438 voltage: 1.75\n",
      "Epoch 18, Total Reward: -1021.4328, Q1: 0.0000, Q2: 0.0000, TargetQ: 0.0000 alpha: 2.7151 alpha_dot: 317.3561 voltage: -1.75\n",
      "Epoch 19, Total Reward: -512.8098, Q1: 0.0000, Q2: 0.0000, TargetQ: 0.0000 alpha: 2.8317 alpha_dot: 224.2604 voltage: -1.75\n",
      "Epoch 20, Total Reward: -288.7923, Q1: 0.0000, Q2: 0.0000, TargetQ: 0.0000 alpha: 3.0066 alpha_dot: 163.4362 voltage: -1.75\n",
      "Epoch 21, Total Reward: -246.7217, Q1: 0.0000, Q2: 0.0000, TargetQ: 0.0000 alpha: 2.5525 alpha_dot: 72.6936 voltage: 1.75\n",
      "Epoch 22, Total Reward: -659.4306, Q1: 0.0000, Q2: 0.0000, TargetQ: 0.0000 alpha: -1.5616 alpha_dot: -235.9986 voltage: 1.71\n",
      "Epoch 23, Total Reward: -673.4322, Q1: 0.0000, Q2: 0.0000, TargetQ: 0.0000 alpha: -2.6139 alpha_dot: -254.6106 voltage: 1.75\n",
      "Epoch 24, Total Reward: -278.0828, Q1: 0.0000, Q2: 0.0000, TargetQ: 0.0000 alpha: 1.9788 alpha_dot: 165.2845 voltage: 1.15\n",
      "Epoch 25, Total Reward: -297.2254, Q1: 0.0000, Q2: 0.0000, TargetQ: 0.0000 alpha: 2.5311 alpha_dot: 152.0244 voltage: 1.75\n",
      "Epoch 26, Total Reward: -1660.9046, Q1: 0.0000, Q2: 0.0000, TargetQ: 0.0000 alpha: -2.9974 alpha_dot: -401.8844 voltage: 1.75\n",
      "Epoch 27, Total Reward: -557.0489, Q1: 0.0000, Q2: 0.0000, TargetQ: 0.0000 alpha: -2.4421 alpha_dot: -229.2480 voltage: 1.75\n",
      "Epoch 28, Total Reward: -1284.9761, Q1: 0.0000, Q2: 0.0000, TargetQ: 0.0000 alpha: 3.0342 alpha_dot: 355.2857 voltage: -1.75\n",
      "Epoch 29, Total Reward: -481.1811, Q1: 0.0000, Q2: 0.0000, TargetQ: 0.0000 alpha: 2.7305 alpha_dot: 217.0701 voltage: -1.74\n",
      "Epoch 30, Total Reward: -1498.9195, Q1: 0.0000, Q2: 0.0000, TargetQ: 0.0000 alpha: -3.0097 alpha_dot: -383.2655 voltage: 1.59\n",
      "Epoch 31, Total Reward: -434.1310, Q1: 0.0000, Q2: 0.0000, TargetQ: 0.0000 alpha: -2.1660 alpha_dot: -185.7495 voltage: 1.75\n",
      "Done.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only length-1 arrays can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[125], line 131\u001b[0m\n\u001b[0;32m    126\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    127\u001b[0m                 avg_q\u001b[38;5;241m.\u001b[39mappend(avg_target_q)\n\u001b[0;32m    128\u001b[0m                 \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Total Reward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mfloat\u001b[39m(reward)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    129\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQ1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_q1\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Q2: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_q2\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, TargetQ: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_target_q\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m    130\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malpha: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00malpha\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malpha_dot: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00malpha_dot\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m--> 131\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvoltage: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStopping (Ctrl+C). Saving…\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: only length-1 arrays can be converted to Python scalars"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "history = []\n",
    "policy_delay = 2  # Delayed policy updates\n",
    "step = 0\n",
    "total_reward = 0.0\n",
    "frequency = 500  # Hz\n",
    "state_theta_dot = np.array([0,0], dtype=np.float64)\n",
    "state_alpha_dot = np.array([0,0], dtype=np.float64)\n",
    "timestamps = 0 \n",
    "\n",
    "w_theta = 1.0\n",
    "w_phi = 10\n",
    "w_theta_dot = 0.01\n",
    "w_alpha_dot = 0.01\n",
    "w_u = 0.0001\n",
    "w_du = 0.001\n",
    "prev_action = np.zeros(action_size, dtype=np.float32)\n",
    "avg_q = []\n",
    "try: \n",
    "    with QubeServo3(hardware = 1, pendulum = 1, frequency=10) as board:\n",
    "        while True:\n",
    "            avg_q1, avg_q2, avg_target_q = 0.0, 0.0, 0.0\n",
    "            step += 1 \n",
    "            board.read_outputs()\n",
    "            theta = board.motorPosition * -1\n",
    "            alpha = board.pendulumPosition \n",
    "            theta = np.clip(theta, (-5*np.pi)/8, (5*np.pi)/8)\n",
    "            alpha = np.mod(alpha, 2*np.pi) - np.pi\n",
    "\n",
    "            theta_dot, state_theta_dot = ddt_filter(theta, state_theta_dot, 50, 1/frequency)\n",
    "            alpha_dot, state_alpha_dot = ddt_filter(alpha, state_alpha_dot, 100, 1/frequency)\n",
    "\n",
    "            state = np.array([theta, theta_dot, alpha, alpha_dot], dtype=np.float32)\n",
    "\n",
    "            action = actor_model(tf.convert_to_tensor([state], dtype=tf.float32)).numpy()[0]\n",
    "            action = action + np.random.normal(0, 0.1, size=action_size)  # Add exploration noise\n",
    "            action = np.clip(action, -1.75, 1.75) \n",
    "            board.write_voltage(action)\n",
    "\n",
    "            board.read_outputs()\n",
    "            next_theta = board.motorPosition * -1\n",
    "            next_alpha = board.pendulumPosition\n",
    "            next_alpha = np.mod(next_alpha, 2*np.pi) - np.pi\n",
    "            next_theta = np.clip(next_theta, (-5*np.pi)/8, (5*np.pi)/8)\n",
    "            next_theta_dot, state_theta_dot = ddt_filter(next_theta, state_theta_dot, 50, 1/frequency)\n",
    "            next_alpha_dot, state_alpha_dot = ddt_filter(next_alpha, state_alpha_dot, 100, 1/frequency)\n",
    "            next_state = np.array([next_theta, next_theta_dot, next_alpha, next_alpha_dot], dtype=np.float32)\n",
    "\n",
    "\n",
    "            # wrapped_alpha = ((alpha - np.pi + np.pi) % (2*np.pi)) - np.pi\n",
    "            # reward = -(alpha**2 + 0.0001*alpha_dot**2 + 0.001*action**2)\n",
    "            delta_u = action - prev_action\n",
    "            # reward = 1.0 - (\n",
    "            #     w_theta * (theta**2) +\n",
    "            #     w_phi * (alpha**2) +\n",
    "            #     w_theta_dot * (theta_dot**2) +\n",
    "            #     w_alpha_dot * (alpha_dot**2) +\n",
    "            #     w_u * (action**2) +\n",
    "            #     w_du * (delta_u**2)\n",
    "            # )\n",
    "            upright_reward = (np.cos(alpha) + 1.0) * 2.0  # max = 4 when alpha=0\n",
    "            reward = upright_reward - (\n",
    "                w_theta * (theta**2) +\n",
    "                w_theta_dot * (theta_dot**2) +\n",
    "                w_alpha_dot * (alpha_dot**2) +\n",
    "                w_u * (action**2) +\n",
    "                w_du * (delta_u**2)\n",
    "            )\n",
    "            prev_action = action.copy()\n",
    "            reward = float(reward)\n",
    "            total_reward += reward\n",
    "\n",
    "            replay_buffer.store(state, action, reward, next_state, False)\n",
    "            state = next_state\n",
    "\n",
    "            if replay_buffer.size() >= batch_size:\n",
    "                states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "                states = tf.convert_to_tensor(states, dtype=tf.float32)\n",
    "                actions = tf.convert_to_tensor(actions, dtype=tf.float32)\n",
    "                rewards = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
    "                next_states = tf.convert_to_tensor(next_states, dtype=tf.float32)\n",
    "                dones = tf.convert_to_tensor(dones, dtype=tf.float32) \n",
    "\n",
    "                # add clipped noise to target action\n",
    "                noise = np.clip(np.random.normal(0, 0.2, size=actions.shape), -0.5, 0.5)\n",
    "                next_actions = target_actor(next_states) + noise\n",
    "                next_actions = tf.clip_by_value(next_actions, -1.75, 1.75)  # Pendulum action bounds\n",
    "\n",
    "                # Compute target Q-values with both critics\n",
    "                target1 = tf.squeeze(target_critic1([next_states, next_actions]), axis=1)\n",
    "                target2 = tf.squeeze(target_critic2([next_states, next_actions]), axis=1)\n",
    "                target_q = rewards + gamma * (1 - dones) * tf.minimum(target1, target2)\n",
    "\n",
    "                with tf.GradientTape() as tape_critic1, tf.GradientTape() as tape_critic2:\n",
    "                    q1 = critic_model1([states, actions], training=True)\n",
    "                    q2 = critic_model2([states, actions], training=True)\n",
    "\n",
    "                    # Compute losses\n",
    "                    loss1 = tf.keras.losses.MSE(target_q, q1)\n",
    "                    loss2 = tf.keras.losses.MSE(target_q, q2)\n",
    "\n",
    "                avg_q1 = tf.reduce_mean(q1).numpy().item()\n",
    "                avg_q2 = tf.reduce_mean(q2).numpy().item()\n",
    "                avg_target_q = tf.reduce_mean(target_q).numpy().item()\n",
    "\n",
    "                # Get gradients for each critic once\n",
    "                critic_grad1 = tape_critic1.gradient(loss1, critic_model1.trainable_variables)\n",
    "                critic_grad2 = tape_critic2.gradient(loss2, critic_model2.trainable_variables)\n",
    "\n",
    "                # Apply gradients\n",
    "                critic_optimizer1.apply_gradients(zip(critic_grad1, critic_model1.trainable_variables))\n",
    "                critic_optimizer2.apply_gradients(zip(critic_grad2, critic_model2.trainable_variables))\n",
    "                if step % policy_delay == 0:  # Delayed policy updates\n",
    "                    with tf.GradientTape() as tape_actor: \n",
    "                        action = actor_model(states)\n",
    "                        actor_loss = -tf.reduce_mean(critic_model1([states, action]))\n",
    "\n",
    "                    actor_grad = tape_actor.gradient(actor_loss, actor_model.trainable_variables)\n",
    "                    actor_optimizer.apply_gradients(zip(actor_grad, actor_model.trainable_variables))\n",
    "\n",
    "                    soft_update(target_actor.variables, actor_model.variables, tau=0.005)\n",
    "                    soft_update(target_critic1.variables, critic_model1.variables, tau=0.005)\n",
    "                    soft_update(target_critic2.variables, critic_model2.variables, tau=0.005)\n",
    "\n",
    "            history.append(total_reward)\n",
    "            if step % 1 == 0:\n",
    "                avg_q.append(avg_target_q)\n",
    "                print(f\"Epoch {step}, Total Reward: {float(reward):.4f}, \"\n",
    "                f\"Q1: {avg_q1:.4f}, Q2: {avg_q2:.4f}, TargetQ: {avg_target_q:.4f}\", \n",
    "                f\"alpha: {alpha:.4f}\", f\"alpha_dot: {alpha_dot:.4f}\", \n",
    "                f\"voltage: {np.array(action).reshape(-1)[0]:.2f}\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nStopping (Ctrl+C). Saving…\")\n",
    "finally:\n",
    "    # save weights (use .save_weights if you prefer checkpoint style)\n",
    "    actor_model.save_weights(\"saves/quanser/actor_model.weights.h5\")\n",
    "    critic_model1.save_weights(\"saves/quanser/critic_model1.weights.h5\")\n",
    "    critic_model2.save_weights(\"saves/quanser/critic_model2.weights.h5\")\n",
    "    ckpt = tf.train.Checkpoint(actor_optimizer=actor_optimizer,\n",
    "                           critic_optimizer1=critic_optimizer1,\n",
    "                           critic_optimizer2=critic_optimizer2)\n",
    "    ckpt.save(\"saves/quanser/optimizers_ckpt/ckpt\")\n",
    "    plt.plot(avg_q, label='Target Q')\n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fd84a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 32\n",
    "# history = []\n",
    "# policy_delay = 2  # Delayed policy updates\n",
    "# step = 0\n",
    "# total_reward = 0.0\n",
    "\n",
    "# try:\n",
    "#     total_reward = 0.0\n",
    "#     while True:\n",
    "#         step += 1\n",
    "    \n",
    "#         # 1) read state\n",
    "#         board.read_encoder(encoder_channels, len(encoder_channels), counts)\n",
    "#         theta_arm  = counts[0] * ARM_RAD_PER_COUNT\n",
    "#         theta_pend = counts[1] * PEND_RAD_PER_COUNT\n",
    "#         theta_arm_dot  = (theta_arm  - theta_arm_prev)  / dt\n",
    "#         theta_pend_dot = (theta_pend - theta_pend_prev) / dt\n",
    "#         state = np.array([theta_arm, theta_pend, theta_arm_dot, theta_pend_dot], dtype=np.float32)\n",
    "\n",
    "#         # 2) select action\n",
    "#         action_vec = actor_model(tf.convert_to_tensor([state], dtype=tf.float32)).numpy()[0]\n",
    "#         action_val = float(np.clip(action_vec[0], -2.0, 2.0))  # scalar in [-2,2]; tune to your safe V range\n",
    "\n",
    "#         # 3) apply action (analog write wants numpy float64 buffer)\n",
    "#         voltages = np.array([action_val], dtype=np.float64)\n",
    "#         board.write_analog(motor_channels, len(motor_channels), voltages)\n",
    "\n",
    "#         # 4) get next_state after action\n",
    "#         time.sleep(dt)  # maintain loop timing around the actuation\n",
    "#         board.read_encoder(encoder_channels, len(encoder_channels), counts)\n",
    "#         next_theta_arm  = counts[0] * ARM_RAD_PER_COUNT\n",
    "#         next_theta_pend = counts[1] * PEND_RAD_PER_COUNT\n",
    "#         next_theta_arm_dot  = (next_theta_arm  - theta_arm)  / dt\n",
    "#         next_theta_pend_dot = (next_theta_pend - theta_pend) / dt\n",
    "#         next_state = np.array([next_theta_arm, next_theta_pend, next_theta_arm_dot, next_theta_pend_dot], dtype=np.float32)\n",
    "\n",
    "#         # 5) reward (example: upright pendulum, gentle motion)\n",
    "#         reward = - ( (np.angle(np.exp(1j*(next_theta_pend - np.pi))))**2\n",
    "#                      + 0.1*next_theta_pend_dot**2 + 0.01*action_val**2 )\n",
    "#         total_reward += reward\n",
    "\n",
    "#         # 6) store\n",
    "#         replay_buffer.store(state, action_val, reward, next_state, False)\n",
    "\n",
    "#         # 7) train if enough samples\n",
    "#         if replay_buffer.size() >= batch_size:\n",
    "#             states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "#             states      = tf.convert_to_tensor(states, dtype=tf.float32)\n",
    "#             actions     = tf.convert_to_tensor(actions.reshape(-1,1), dtype=tf.float32)\n",
    "#             rewards     = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
    "#             next_states = tf.convert_to_tensor(next_states, dtype=tf.float32)\n",
    "#             dones       = tf.convert_to_tensor(dones, dtype=tf.float32)\n",
    "\n",
    "#             # target policy smoothing\n",
    "#             noise = np.clip(np.random.normal(0, 0.2, size=(actions.shape[0], 1)), -0.5, 0.5)\n",
    "#             target_act = tf.clip_by_value(target_actor(next_states) + noise, -2.0, 2.0)\n",
    "\n",
    "#             # twin critics target\n",
    "#             t1 = tf.squeeze(target_critic1([next_states, target_act]), axis=1)\n",
    "#             t2 = tf.squeeze(target_critic2([next_states, target_act]), axis=1)\n",
    "#             target_q = rewards + gamma * (1.0 - dones) * tf.minimum(t1, t2)\n",
    "\n",
    "#             # critic updates\n",
    "#             with tf.GradientTape() as tape1, tf.GradientTape() as tape2:\n",
    "#                 q1 = tf.squeeze(critic_model1([states, actions]), axis=1)\n",
    "#                 q2 = tf.squeeze(critic_model2([states, actions]), axis=1)\n",
    "#                 loss1 = tf.keras.losses.MSE(target_q, q1)\n",
    "#                 loss2 = tf.keras.losses.MSE(target_q, q2)\n",
    "#             critic_optimizer1.apply_gradients(zip(tape1.gradient(loss1, critic_model1.trainable_variables),\n",
    "#                                                   critic_model1.trainable_variables))\n",
    "#             critic_optimizer2.apply_gradients(zip(tape2.gradient(loss2, critic_model2.trainable_variables),\n",
    "#                                                   critic_model2.trainable_variables))\n",
    "\n",
    "#             # delayed actor + target updates\n",
    "#             if step % policy_delay == 0:\n",
    "#                 with tf.GradientTape() as tape_actor:\n",
    "#                     pi = actor_model(states)\n",
    "#                     actor_loss = -tf.reduce_mean(critic_model1([states, pi]))\n",
    "#                 actor_optimizer.apply_gradients(zip(tape_actor.gradient(actor_loss, actor_model.trainable_variables),\n",
    "#                                                     actor_model.trainable_variables))\n",
    "#                 soft_update(target_actor.variables,   actor_model.variables,   tau=0.005)\n",
    "#                 soft_update(target_critic1.variables, critic_model1.variables, tau=0.005)\n",
    "#                 soft_update(target_critic2.variables, critic_model2.variables, tau=0.005)\n",
    "\n",
    "#         if step % 100 == 0:\n",
    "#             print(f\"Step {step}  reward_sum: {total_reward:.2f}\")\n",
    "\n",
    "#         # update prev angles for next derivative\n",
    "#         theta_arm_prev, theta_pend_prev = next_theta_arm, next_theta_pend\n",
    "\n",
    "# except KeyboardInterrupt:\n",
    "#     print(\"\\nStopping (Ctrl+C). Saving…\")\n",
    "# finally:\n",
    "#     # save weights (use .save_weights if you prefer checkpoint style)\n",
    "#     actor_model.save_weights(\"saves/quanser/actor_model.weights.h5\")\n",
    "#     critic_model1.save_weights(\"saves/quanser/critic_model1.weights.h5\")\n",
    "#     critic_model2.save_weights(\"saves/quanser/critic_model2.weights.h5\")\n",
    "#     # set motor to 0V and close safely\n",
    "#     board.write_analog(motor_channels, 1, np.array([0.0], dtype=np.float64))\n",
    "#     board.close()\n",
    "#     print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff966c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# while True:\n",
    "#     # read state\n",
    "#     # board.close()\n",
    "#     board.read_encoder(encoder_channels, len(encoder_channels), counts)\n",
    "#     theta_arm = counts[0] * ARM_RAD_PER_COUNT\n",
    "#     theta_pend = counts[1] * PEND_RAD_PER_COUNT\n",
    "\n",
    "#     # compute action from policy\n",
    "#     action = actor_model(tf.convert_to_tensor([state], dtype=tf.float32)).numpy()[0]\n",
    "#     u = float(action)\n",
    "\n",
    "#     # send to motor\n",
    "#     board.write_analog(motor_channels, 1, [u])\n",
    "\n",
    "#     time.sleep(dt)  # ~0.01s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e598aa56",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'QubeServo3' object has no attribute 'close'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcollections\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m deque\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# --- HIL/QUBE setup ---\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[43mboard\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m()\n\u001b[0;32m      8\u001b[0m board \u001b[38;5;241m=\u001b[39m HIL(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqube_servo3_usb\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m encoder_channels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39muint32)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'QubeServo3' object has no attribute 'close'"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# import time\n",
    "# import tensorflow as tf\n",
    "# from collections import deque\n",
    "\n",
    "# # --- HIL/QUBE setup ---\n",
    "# board.close()\n",
    "# board = HIL(\"qube_servo3_usb\", \"0\")\n",
    "# encoder_channels = np.array([0, 1], dtype=np.uint32)\n",
    "# motor_channels = np.array([0], dtype=np.uint32)\n",
    "# counts = np.zeros(2, dtype=np.int32)\n",
    "\n",
    "# ENCODER_RES = 2048\n",
    "# ARM_RAD_PER_COUNT = 2*np.pi / ENCODER_RES\n",
    "# PEND_RAD_PER_COUNT = 2*np.pi / ENCODER_RES\n",
    "# dt = 0.01  # 10 ms loop\n",
    "\n",
    "# # --- Replay Buffer ---\n",
    "# class ReplayBuffer:\n",
    "#     def __init__(self, capacity=100000):\n",
    "#         self.buffer = deque(maxlen=capacity)\n",
    "#     def store(self, state, action, reward, next_state, done):\n",
    "#         self.buffer.append((state, action, reward, next_state, done))\n",
    "#     def sample(self, batch_size):\n",
    "#         batch = np.array(random.sample(self.buffer, batch_size))\n",
    "#         states, actions, rewards, next_states, dones = map(np.stack, zip(*batch))\n",
    "#         return states, actions, rewards, next_states, dones\n",
    "#     def size(self):\n",
    "#         return len(self.buffer)\n",
    "\n",
    "# replay_buffer = ReplayBuffer()\n",
    "\n",
    "# # --- Soft update ---\n",
    "# def soft_update(target_weights, online_weights, tau=0.005):\n",
    "#     for (target, online) in zip(target_weights, online_weights):\n",
    "#         target.assign(target * (1 - tau) + online * tau)\n",
    "\n",
    "# # --- TD3 models already defined: actor_model, critic_model1, critic_model2, \n",
    "# # target_actor, target_critic1, target_critic2\n",
    "# # optimizers: actor_optimizer, critic_optimizer1, critic_optimizer2\n",
    "\n",
    "# state_size = 4\n",
    "# action_size = 1\n",
    "# gamma = 0.99\n",
    "# batch_size = 32\n",
    "# policy_delay = 2\n",
    "# step = 0\n",
    "\n",
    "# theta_arm_prev = 0.0\n",
    "# theta_pend_prev = 0.0\n",
    "\n",
    "# try:\n",
    "#     while True:\n",
    "#         step += 1\n",
    "\n",
    "#         # --- 1. Read state ---\n",
    "#         board.read_encoder(encoder_channels, len(encoder_channels), counts)\n",
    "#         theta_arm = counts[0] * ARM_RAD_PER_COUNT\n",
    "#         theta_pend = counts[1] * PEND_RAD_PER_COUNT\n",
    "\n",
    "#         theta_arm_dot = (theta_arm - theta_arm_prev) / dt\n",
    "#         theta_pend_dot = (theta_pend - theta_pend_prev) / dt\n",
    "#         theta_arm_prev, theta_pend_prev = theta_arm, theta_pend\n",
    "\n",
    "#         state = np.array([theta_arm, theta_pend, theta_arm_dot, theta_pend_dot], dtype=np.float32)\n",
    "\n",
    "#         # --- 2. Compute action ---\n",
    "#         action = actor_model(tf.convert_to_tensor([state], dtype=tf.float32)).numpy()[0]\n",
    "#         u_array = np.array([float(action)], dtype=np.float64)\n",
    "\n",
    "#         # --- 3. Apply action ---\n",
    "#         board.write_analog(motor_channels, 1, u_array)\n",
    "\n",
    "#         # --- 4. Read next state ---\n",
    "#         board.read_encoder(encoder_channels, len(encoder_channels), counts)\n",
    "#         next_theta_arm = counts[0] * ARM_RAD_PER_COUNT\n",
    "#         next_theta_pend = counts[1] * PEND_RAD_PER_COUNT\n",
    "#         next_theta_arm_dot = (next_theta_arm - theta_arm) / dt\n",
    "#         next_theta_pend_dot = (next_theta_pend - theta_pend) / dt\n",
    "#         next_state = np.array([next_theta_arm, next_theta_pend, next_theta_arm_dot, next_theta_pend_dot], dtype=np.float32)\n",
    "\n",
    "#         # --- 5. Compute reward ---\n",
    "#         reward = - (next_theta_pend**2 + 0.1 * next_theta_pend_dot**2)\n",
    "\n",
    "#         # --- 6. Store transition ---\n",
    "#         replay_buffer.store(state, action, reward, next_state, False)\n",
    "\n",
    "#         # --- 7. Train TD3 ---\n",
    "#         if replay_buffer.size() >= batch_size:\n",
    "#             states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "#             states = tf.convert_to_tensor(states, dtype=tf.float32)\n",
    "#             actions = tf.convert_to_tensor(actions, dtype=tf.float32)\n",
    "#             rewards = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
    "#             next_states = tf.convert_to_tensor(next_states, dtype=tf.float32)\n",
    "#             dones = tf.convert_to_tensor(dones, dtype=tf.float32)\n",
    "\n",
    "#             # Target actions with clipped noise\n",
    "#             noise = np.clip(np.random.normal(0, 0.2, size=actions.shape), -0.5, 0.5)\n",
    "#             next_actions = tf.clip_by_value(target_actor(next_states) + noise, -12.0, 12.0)\n",
    "\n",
    "#             # Target Q-values\n",
    "#             target1 = tf.squeeze(target_critic1([next_states, next_actions]), axis=1)\n",
    "#             target2 = tf.squeeze(target_critic2([next_states, next_actions]), axis=1)\n",
    "#             target_q = rewards + gamma * (1 - dones) * tf.minimum(target1, target2)\n",
    "\n",
    "#             # Critic updates\n",
    "#             with tf.GradientTape() as tape1, tf.GradientTape() as tape2:\n",
    "#                 q1 = critic_model1([states, actions], training=True)\n",
    "#                 q2 = critic_model2([states, actions], training=True)\n",
    "#                 loss1 = tf.keras.losses.MSE(target_q, q1)\n",
    "#                 loss2 = tf.keras.losses.MSE(target_q, q2)\n",
    "\n",
    "#             critic_grad1 = tape1.gradient(loss1, critic_model1.trainable_variables)\n",
    "#             critic_grad2 = tape2.gradient(loss2, critic_model2.trainable_variables)\n",
    "#             critic_optimizer1.apply_gradients(zip(critic_grad1, critic_model1.trainable_variables))\n",
    "#             critic_optimizer2.apply_gradients(zip(critic_grad2, critic_model2.trainable_variables))\n",
    "\n",
    "#             # Delayed actor update\n",
    "#             if step % policy_delay == 0:\n",
    "#                 with tf.GradientTape() as tape_actor:\n",
    "#                     act = actor_model(states)\n",
    "#                     actor_loss = -tf.reduce_mean(critic_model1([states, act]))\n",
    "#                 actor_grad = tape_actor.gradient(actor_loss, actor_model.trainable_variables)\n",
    "#                 actor_optimizer.apply_gradients(zip(actor_grad, actor_model.trainable_variables))\n",
    "\n",
    "#                 soft_update(target_actor.variables, actor_model.variables)\n",
    "#                 soft_update(target_critic1.variables, critic_model1.variables)\n",
    "#                 soft_update(target_critic2.variables, critic_model2.variables)\n",
    "\n",
    "#         # --- 8. Sleep to maintain loop ---\n",
    "#         time.sleep(dt)\n",
    "\n",
    "# except KeyboardInterrupt:\n",
    "#     print(\"Stopping (Ctrl+C) and saving models...\")\n",
    "\n",
    "# finally:\n",
    "#     # Save models\n",
    "#     actor_model.save(\"td3_actor.h5\")\n",
    "#     critic_model1.save(\"td3_critic1.h5\")\n",
    "#     critic_model2.save(\"td3_critic2.h5\")\n",
    "#     board.close()\n",
    "#     print(\"Training finished and models saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neural_network",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
