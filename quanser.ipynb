{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b8ceea56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Lambda\n",
    "from keras.layers import Concatenate\n",
    "from collections import deque\n",
    "import random\n",
    "from quanser.hardware import HIL \n",
    "from pal.products.qube import QubeServo3\n",
    "from pal.utilities.math import SignalGenerator, ddt_filter\n",
    "from threading import Thread\n",
    "import time\n",
    "import signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8674aa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # board.close()\n",
    "# # Open connection to QUBE\n",
    "# # board = HIL(\"qube_servo3_usb\", \"0\")\n",
    "\n",
    "# encoder_channels = np.array([0, 1], dtype=np.uint32)\n",
    "# motor_channels = np.array([0], dtype=np.uint32)\n",
    "# counts = np.zeros(2, dtype=np.int32)\n",
    "\n",
    "# ENCODER_RES = 2048\n",
    "# ARM_RAD_PER_COUNT = 2*np.pi / ENCODER_RES\n",
    "# PEND_RAD_PER_COUNT = 2*np.pi / ENCODER_RES\n",
    "\n",
    "# dt = 0.01  # 10 ms\n",
    "# theta_arm_prev  = counts[0] * ARM_RAD_PER_COUNT\n",
    "# theta_pend_prev = counts[1] * PEND_RAD_PER_COUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "49e7c226",
   "metadata": {},
   "outputs": [],
   "source": [
    "global KILL_THREAD\n",
    "KILL_THREAD = False\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=100000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    def store(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = map(np.array, zip(*batch))\n",
    "        return (np.array(states, dtype=np.float32),\n",
    "            np.array(actions, dtype=np.float32),\n",
    "            np.array(rewards, dtype=np.float32),\n",
    "            np.array(next_states, dtype=np.float32),\n",
    "            np.array(dones, dtype=np.float32))\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "def soft_update(target_weights, online_weights, tau=0.005):\n",
    "    for (target, online) in zip(target_weights, online_weights):\n",
    "        target.assign(target * (1 - tau) + online * tau) \n",
    "\n",
    "def sig_handler(*args): \n",
    "    global KILL_THREAD\n",
    "    KILL_THREAD = True\n",
    "\n",
    "signal.signal(signal.SIGINT, sig_handler)\n",
    "\n",
    "replay_buffer = ReplayBuffer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f8509c67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x232078c0730>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_size = 4\n",
    "action_size = 1\n",
    "gamma = 0.99 # discount rate\n",
    "learning_rate = 0.001 # learning rate\n",
    "\n",
    "# Define the actor model\n",
    "states_inputs = Input(shape=(state_size,))\n",
    "dense = Dense(128, activation='relu')(states_inputs)\n",
    "dense = Dense(128, activation='relu')(dense)\n",
    "outputs = Dense(action_size, activation='tanh')(dense)\n",
    "outputs = keras.layers.Lambda(lambda x: x * 1.75)(outputs)  # Scale action to [-12, 12] Voltage\n",
    "actor_model = Model(inputs=states_inputs, outputs=outputs)\n",
    "\n",
    "# Critic 1\n",
    "state_input1 = Input(shape=(state_size,))\n",
    "action_input1 = Input(shape=(action_size,))\n",
    "concat1 = Concatenate()([state_input1, action_input1])\n",
    "dense1 = Dense(128, activation='relu')(concat1)\n",
    "dense1 = Dense(128, activation='relu')(dense1)\n",
    "output1 = Dense(1)(dense1)\n",
    "critic_model1 = Model([state_input1, action_input1], output1)\n",
    "\n",
    "# Critic 2\n",
    "state_input2 = Input(shape=(state_size,))\n",
    "action_input2 = Input(shape=(action_size,))\n",
    "concat2 = Concatenate()([state_input2, action_input2])\n",
    "dense2 = Dense(128, activation='relu')(concat2)\n",
    "dense2 = Dense(128, activation='relu')(dense2)\n",
    "output2 = Dense(1)(dense2)\n",
    "critic_model2 = Model([state_input2, action_input2], output2)\n",
    "\n",
    "try:\n",
    "    actor_model.load_weights('saves/quanser/actor_model.weights.h5')\n",
    "    critic_model1.load_weights('saves/quanser/critic_model1.weights.h5')\n",
    "    critic_model2.load_weights('saves/quanser/critic_model2.weights.h5')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "actor_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "critic_optimizer1 = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "critic_optimizer2 = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "target_actor = keras.models.clone_model(actor_model)\n",
    "target_actor.set_weights(actor_model.get_weights())\n",
    "\n",
    "target_critic1 = keras.models.clone_model(critic_model1)\n",
    "target_critic1.set_weights(critic_model1.get_weights())\n",
    "target_critic2 = keras.models.clone_model(critic_model2)\n",
    "target_critic2.set_weights(critic_model2.get_weights())\n",
    "\n",
    "ckpt = tf.train.Checkpoint(actor_optimizer=actor_optimizer,\n",
    "                           critic_optimizer1=critic_optimizer1, \n",
    "                           critic_optimizer2=critic_optimizer2)\n",
    "\n",
    "# Restore the latest checkpoint with optimizer states\n",
    "ckpt.restore(tf.train.latest_checkpoint(\"saves/quanser/optimizers_ckpt\")).expect_partial()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "844cf3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency = 500  # Hz\n",
    "# state_theta_dot = np.array([0,0], dtype=np.float64)\n",
    "# state_alpha_dot = np.array([0,0], dtype=np.float64)\n",
    "# with QubeServo3(hardware = 1, pendulum = 1, frequency=frequency) as board:\n",
    "#     while True:\n",
    "#         # Have to initialize the board first before reading motorPosition or it won't read\n",
    "#         board.read_outputs()\n",
    "#         theta = board.motorPosition \n",
    "#         alpha = -board.pendulumPosition \n",
    "#         theta_dot, state_theta_dot = ddt_filter(theta, state_theta_dot, 50, 1/frequency)\n",
    "#         # u - input\n",
    "#         # state - previous state returned by this function -- initialize to np.array([0,0], dtype=np.float64)\n",
    "#         # Ts - sample time in seconds\n",
    "#         # A - filter bandwidth in rad/s\n",
    "#         alpha_dot, state_alpha_dot = ddt_filter(alpha, state_alpha_dot, 100, 1/frequency)\n",
    "#         alpha = np.mod(alpha, 2*np.pi) - np.pi\n",
    "#         alpha = np.cos(alpha)\n",
    "#         theta = np.clip(theta, (-5*np.pi)/8, (5*np.pi)/8)\n",
    "#         reward = -(alpha**2 + 0.0001*alpha_dot**2 )\n",
    "#         print(f\"Theta: {theta:.3f}, Theta dot: {theta_dot:.3f}, Alpha: {alpha:.3f}, Alpha dot: {alpha_dot:.3f}\", \n",
    "#               f\"Reward: {reward:.3f}\")\n",
    "#         time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7442b395",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ntk00\\AppData\\Local\\Temp\\ipykernel_18584\\880666586.py:68: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  reward = float(reward)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Total Reward: -815.5916, Q1: 0.0000, Q2: 0.0000, TargetQ: 0.0000 alpha: -3.1416 alpha_dot: -285.5993 voltage: -1.75 theta: -0.0000 theta_dot: 0.0000\n",
      "Epoch 2, Total Reward: -562.8499, Q1: 0.0000, Q2: 0.0000, TargetQ: 0.0000 alpha: 2.6415 alpha_dot: 234.7532 voltage: -1.73 theta: 0.7578 theta_dot: 34.9024\n",
      "Epoch 3, Total Reward: -458.1905, Q1: 0.0000, Q2: 0.0000, TargetQ: 0.0000 alpha: 3.0986 alpha_dot: 196.4238 voltage: -1.74 theta: 1.9482 theta_dot: 82.8063\n",
      "Epoch 4, Total Reward: -1187.7304, Q1: 0.0000, Q2: 0.0000, TargetQ: 0.0000 alpha: -3.1140 alpha_dot: -337.1978 voltage: -1.75 theta: 1.9635 theta_dot: 68.4457\n",
      "Epoch 5, Total Reward: -476.9560, Q1: 0.0000, Q2: 0.0000, TargetQ: 0.0000 alpha: 2.7489 alpha_dot: 210.4048 voltage: -1.64 theta: 1.9635 theta_dot: 56.0292\n",
      "Epoch 6, Total Reward: -1381.1331, Q1: 0.0000, Q2: 0.0000, TargetQ: 0.0000 alpha: -2.7826 alpha_dot: -368.4068 voltage: -1.75 theta: 1.9635 theta_dot: 45.8652\n",
      "Epoch 7, Total Reward: -843.2841, Q1: 0.0000, Q2: 0.0000, TargetQ: 0.0000 alpha: 3.0894 alpha_dot: 287.4595 voltage: -1.73 theta: 1.9635 theta_dot: 37.5450\n",
      "Epoch 8, Total Reward: -311.0137, Q1: 0.0000, Q2: 0.0000, TargetQ: 0.0000 alpha: 2.8103 alpha_dot: 172.8321 voltage: -1.75 theta: 1.9635 theta_dot: 30.7341\n",
      "Epoch 9, Total Reward: -863.8228, Q1: 0.0000, Q2: 0.0000, TargetQ: 0.0000 alpha: -2.7458 alpha_dot: -292.3414 voltage: -1.75 theta: 1.9635 theta_dot: 25.1588\n",
      "Epoch 10, Total Reward: -1081.8352, Q1: 0.0000, Q2: 0.0000, TargetQ: 0.0000 alpha: 2.9913 alpha_dot: 327.8316 voltage: -1.75 theta: 1.9635 theta_dot: 20.5948\n",
      "Epoch 11, Total Reward: -465.9687, Q1: 0.0000, Q2: 0.0000, TargetQ: 0.0000 alpha: 2.8870 alpha_dot: 214.5386 voltage: -1.75 theta: 1.9635 theta_dot: 16.8588\n",
      "Epoch 12, Total Reward: -739.9385, Q1: 0.0000, Q2: 0.0000, TargetQ: 0.0000 alpha: -2.7397 alpha_dot: -271.1412 voltage: -1.75 theta: 1.9635 theta_dot: 13.8005\n",
      "Epoch 13, Total Reward: -1122.6507, Q1: 0.0000, Q2: 0.0000, TargetQ: 0.0000 alpha: 2.8962 alpha_dot: 334.4422 voltage: -1.75 theta: 1.9635 theta_dot: 11.2970\n",
      "Epoch 14, Total Reward: -552.9355, Q1: 0.0000, Q2: 0.0000, TargetQ: 0.0000 alpha: 2.9790 alpha_dot: 234.3545 voltage: -1.67 theta: 1.9635 theta_dot: 9.2477\n",
      "Epoch 15, Total Reward: -718.9168, Q1: 0.0000, Q2: 0.0000, TargetQ: 0.0000 alpha: -2.7550 alpha_dot: -267.4856 voltage: -1.66 theta: 1.9635 theta_dot: 7.5701\n",
      "Epoch 16, Total Reward: -1111.8639, Q1: 0.0000, Q2: 0.0000, TargetQ: 0.0000 alpha: 2.8225 alpha_dot: 332.9593 voltage: -1.75 theta: 1.9635 theta_dot: 6.1968\n",
      "Epoch 17, Total Reward: -616.1038, Q1: 0.0000, Q2: 0.0000, TargetQ: 0.0000 alpha: 3.0802 alpha_dot: 247.5860 voltage: -1.75 theta: 1.9635 theta_dot: 5.0727\n",
      "Epoch 18, Total Reward: -736.6244, Q1: 0.0000, Q2: 0.0000, TargetQ: 0.0000 alpha: -2.7949 alpha_dot: -270.8493 voltage: -1.75 theta: 1.9635 theta_dot: 4.1525\n",
      "Epoch 19, Total Reward: -515.8740, Q1: 0.0000, Q2: 0.0000, TargetQ: 0.0000 alpha: 2.7673 alpha_dot: 226.4730 voltage: -1.75 theta: 1.9635 theta_dot: 3.3992\n",
      "Epoch 20, Total Reward: -1465.5977, Q1: 0.0000, Q2: 0.0000, TargetQ: 0.0000 alpha: -3.1017 alpha_dot: -382.4476 voltage: -1.75 theta: 1.9635 theta_dot: 2.7826\n",
      "Epoch 21, Total Reward: -575.8456, Q1: 0.0000, Q2: 0.0000, TargetQ: 0.0000 alpha: -2.8593 alpha_dot: -239.3604 voltage: -1.68 theta: 1.9635 theta_dot: 2.2778\n",
      "Epoch 22, Total Reward: -635.4562, Q1: 0.0000, Q2: 0.0000, TargetQ: 0.0000 alpha: 2.7397 alpha_dot: 251.5077 voltage: -1.75 theta: 1.9635 theta_dot: 1.8646\n",
      "Epoch 23, Total Reward: -1267.7383, Q1: 0.0000, Q2: 0.0000, TargetQ: 0.0000 alpha: -3.0005 alpha_dot: -355.6482 voltage: -1.75 theta: 1.9635 theta_dot: 1.5263\n",
      "Epoch 24, Total Reward: -563.5475, Q1: 0.0000, Q2: 0.0000, TargetQ: 0.0000 alpha: -2.9422 alpha_dot: -236.7855 voltage: -1.75 theta: 1.9635 theta_dot: 1.2494\n",
      "Epoch 25, Total Reward: -680.9876, Q1: 0.0000, Q2: 0.0000, TargetQ: 0.0000 alpha: 2.7336 alpha_dot: 260.4069 voltage: -1.68 theta: 1.9635 theta_dot: 1.0228\n",
      "Epoch 26, Total Reward: -1177.0773, Q1: 0.0000, Q2: 0.0000, TargetQ: 0.0000 alpha: -2.9115 alpha_dot: -342.6676 voltage: -1.75 theta: 1.9635 theta_dot: 0.8373\n",
      "Epoch 27, Total Reward: -593.2527, Q1: 0.0000, Q2: 0.0000, TargetQ: 0.0000 alpha: -3.0342 alpha_dot: -242.9793 voltage: -1.75 theta: 1.9635 theta_dot: 0.6854\n",
      "Epoch 28, Total Reward: -713.6508, Q1: 0.0000, Q2: 0.0000, TargetQ: 0.0000 alpha: 2.7581 alpha_dot: 266.6055 voltage: -1.62 theta: 1.9635 theta_dot: 0.5610\n",
      "Epoch 29, Total Reward: -1131.0271, Q1: 0.0000, Q2: 0.0000, TargetQ: 0.0000 alpha: -2.8440 alpha_dot: -335.8812 voltage: -1.75 theta: 1.9635 theta_dot: 0.4593\n",
      "Epoch 30, Total Reward: -638.1622, Q1: 0.0000, Q2: 0.0000, TargetQ: 0.0000 alpha: -3.1355 alpha_dot: -252.0521 voltage: -1.75 theta: 1.9635 theta_dot: 0.3759\n",
      "Epoch 31, Total Reward: -749.2938, Q1: 0.0000, Q2: 0.0000, TargetQ: 0.0000 alpha: 2.8041 alpha_dot: 273.2095 voltage: -1.74 theta: 1.9635 theta_dot: 0.3078\n",
      "Epoch 32, Total Reward: -524.8356, Q1: -6550.9619, Q2: -6559.2085, TargetQ: -7353.8452 alpha: -2.7980 alpha_dot: -228.4681 voltage: -1.75 theta: 1.9635 theta_dot: 0.2519\n",
      "Epoch 33, Total Reward: -1441.8265, Q1: -7050.2432, Q2: -7040.1094, TargetQ: -7356.8096 alpha: 3.0465 alpha_dot: 379.3372 voltage: -1.69 theta: 1.9635 theta_dot: 0.2062\n",
      "Epoch 34, Total Reward: -593.5148, Q1: -7462.7603, Q2: -7437.6147, TargetQ: -7413.1484 alpha: 2.8747 alpha_dot: 243.0338 voltage: -1.75 theta: 1.9635 theta_dot: 0.1688\n",
      "Epoch 35, Total Reward: -644.7727, Q1: -7784.2505, Q2: -7770.4170, TargetQ: -7368.3945 alpha: -2.7734 alpha_dot: -253.3590 voltage: -1.75 theta: 1.9635 theta_dot: 0.1382\n",
      "Epoch 36, Total Reward: -1254.6060, Q1: -7540.1572, Q2: -7544.5420, TargetQ: -7423.7520 alpha: 2.9544 alpha_dot: 353.8004 voltage: -1.75 theta: 1.9635 theta_dot: 0.1131\n",
      "Epoch 37, Total Reward: -580.8604, Q1: -7338.3740, Q2: -7360.2500, TargetQ: -7508.9897 alpha: 2.9575 alpha_dot: 240.4167 voltage: -1.75 theta: 1.9635 theta_dot: 0.0926\n",
      "Epoch 38, Total Reward: -695.8640, Q1: -7071.0967, Q2: -7094.3628, TargetQ: -7537.3359 alpha: -2.7796 alpha_dot: -263.2500 voltage: -1.75 theta: 1.9635 theta_dot: 0.0758\n",
      "Epoch 39, Total Reward: -1173.4120, Q1: -7058.0977, Q2: -7080.1543, TargetQ: -7364.6338 alpha: 2.8777 alpha_dot: 342.1335 voltage: -1.75 theta: 1.9635 theta_dot: 0.0621\n",
      "Epoch 40, Total Reward: -611.4877, Q1: -7016.9648, Q2: -7015.5366, TargetQ: -7476.3862 alpha: 3.0526 alpha_dot: 246.7040 voltage: -1.75 theta: 1.9635 theta_dot: 0.0508\n",
      "Epoch 41, Total Reward: -731.9656, Q1: -7089.2041, Q2: -7068.9609, TargetQ: -7429.0601 alpha: -2.8103 alpha_dot: -270.0198 voltage: -1.66 theta: 1.9635 theta_dot: 0.0416\n",
      "Epoch 42, Total Reward: -1129.1232, Q1: -7114.6631, Q2: -7080.0332, TargetQ: -7576.9736 alpha: 2.8133 alpha_dot: 335.5986 voltage: -1.75 theta: 1.9635 theta_dot: 0.0340\n",
      "Epoch 43, Total Reward: -997.3529, Q1: -7266.0967, Q2: -7225.0532, TargetQ: -7334.6104 alpha: -3.1263 alpha_dot: -315.3550 voltage: -1.75 theta: 1.9635 theta_dot: 0.0279\n",
      "Epoch 44, Total Reward: -373.6958, Q1: -7638.7520, Q2: -7620.3433, TargetQ: -7431.4014 alpha: -2.8655 alpha_dot: -192.5709 voltage: -1.75 theta: 1.9635 theta_dot: 0.0228\n",
      "Epoch 45, Total Reward: -822.8453, Q1: -7509.1255, Q2: -7504.9375, TargetQ: -7341.4658 alpha: 2.7796 alpha_dot: 286.3540 voltage: -1.72 theta: 1.9635 theta_dot: 0.0187\n",
      "Epoch 46, Total Reward: -1146.8481, Q1: -7615.2837, Q2: -7645.4053, TargetQ: -7459.8789 alpha: -3.0311 alpha_dot: -338.2291 voltage: -1.75 theta: 1.9635 theta_dot: 0.0153\n",
      "Epoch 47, Total Reward: -495.7269, Q1: -7176.6934, Q2: -7217.7310, TargetQ: -7344.0859 alpha: -2.9391 alpha_dot: -222.0061 voltage: -1.75 theta: 1.9635 theta_dot: 0.0125\n",
      "Epoch 48, Total Reward: -746.8387, Q1: -7119.7197, Q2: -7157.9023, TargetQ: -7495.4658 alpha: 2.7704 alpha_dot: 272.7600 voltage: -1.75 theta: 1.9635 theta_dot: 0.0102\n",
      "Epoch 49, Total Reward: -1161.8025, Q1: -6907.2896, Q2: -6942.6797, TargetQ: -7422.9648 alpha: -2.9483 alpha_dot: -340.4326 voltage: -1.71 theta: 1.9635 theta_dot: 0.0084\n",
      "Epoch 50, Total Reward: -566.4516, Q1: -6803.6445, Q2: -6819.2178, TargetQ: -7484.2422 alpha: -3.0250 alpha_dot: -237.4011 voltage: -1.75 theta: 1.9635 theta_dot: 0.0069\n",
      "Epoch 51, Total Reward: -740.0136, Q1: -7078.8418, Q2: -7063.5928, TargetQ: -7550.9316 alpha: 2.7857 alpha_dot: 271.5060 voltage: -1.75 theta: 1.9635 theta_dot: 0.0056\n",
      "Epoch 52, Total Reward: -1142.0528, Q1: -7283.5879, Q2: -7248.0244, TargetQ: -7550.4775 alpha: -2.8777 alpha_dot: -337.5195 voltage: -1.75 theta: 1.9635 theta_dot: 0.0046\n",
      "Epoch 53, Total Reward: -619.7951, Q1: -7699.0054, Q2: -7635.9385, TargetQ: -7566.4717 alpha: -3.1140 alpha_dot: -248.3821 voltage: -1.72 theta: 1.9635 theta_dot: 0.0038\n",
      "Epoch 54, Total Reward: -762.3695, Q1: -7565.6201, Q2: -7502.7979, TargetQ: -7463.8770 alpha: 2.8287 alpha_dot: 275.5902 voltage: -1.75 theta: 1.9635 theta_dot: 0.0031\n",
      "Epoch 55, Total Reward: -538.8637, Q1: -7523.1240, Q2: -7486.5859, TargetQ: -7669.0400 alpha: -2.8317 alpha_dot: -231.5143 voltage: -1.75 theta: 1.9635 theta_dot: 0.0025\n",
      "Epoch 56, Total Reward: -1466.0824, Q1: -7683.0903, Q2: -7671.1011, TargetQ: -7301.4854 alpha: 3.0741 alpha_dot: 382.5211 voltage: -1.75 theta: 1.9635 theta_dot: 0.0021\n",
      "Epoch 57, Total Reward: -596.1144, Q1: -7218.2588, Q2: -7237.8872, TargetQ: -7551.3242 alpha: 2.8870 alpha_dot: 243.5678 voltage: -1.64 theta: 1.9635 theta_dot: 0.0017\n",
      "Epoch 58, Total Reward: -660.0071, Q1: -7011.4180, Q2: -7050.2983, TargetQ: -7241.9180 alpha: -2.8041 alpha_dot: -256.3484 voltage: -1.75 theta: 1.9635 theta_dot: 0.0014\n",
      "Epoch 59, Total Reward: -1275.1557, Q1: -7009.7998, Q2: -7066.0693, TargetQ: -7724.4160 alpha: 2.9821 alpha_dot: 356.6928 voltage: -1.75 theta: 1.9635 theta_dot: 0.0011\n",
      "Epoch 60, Total Reward: -581.6362, Q1: -6984.8496, Q2: -7036.1230, TargetQ: -7407.8438 alpha: 2.9667 alpha_dot: 240.5780 voltage: -1.75 theta: 1.9635 theta_dot: 0.0009\n",
      "Epoch 61, Total Reward: -710.0569, Q1: -7171.7354, Q2: -7215.8594, TargetQ: -7432.9590 alpha: -2.8072 alpha_dot: -265.9310 voltage: -1.61 theta: 1.9635 theta_dot: 0.0008\n",
      "Epoch 62, Total Reward: -1190.5199, Q1: -7144.5962, Q2: -7159.9707, TargetQ: -7366.4609 alpha: 2.9023 alpha_dot: 344.6238 voltage: -1.75 theta: 1.9635 theta_dot: 0.0006\n",
      "Epoch 63, Total Reward: -612.0812, Q1: -7276.0391, Q2: -7267.8486, TargetQ: -7520.1694 alpha: 3.0618 alpha_dot: 246.8244 voltage: -1.75 theta: 1.9635 theta_dot: 0.0005\n",
      "Epoch 64, Total Reward: -745.4215, Q1: -7630.2627, Q2: -7605.6245, TargetQ: -7426.2129 alpha: -2.8348 alpha_dot: -272.5001 voltage: -1.75 theta: 1.9635 theta_dot: 0.0004\n",
      "Epoch 65, Total Reward: -1149.7265, Q1: -7668.1094, Q2: -7621.9668, TargetQ: -7410.8467 alpha: 2.8440 alpha_dot: 338.6543 voltage: -1.68 theta: 1.9635 theta_dot: 0.0003\n",
      "Epoch 66, Total Reward: -1000.2827, Q1: -7544.7754, Q2: -7502.8647, TargetQ: -7422.6426 alpha: -3.1232 alpha_dot: -315.8196 voltage: -1.75 theta: 1.9635 theta_dot: 0.0003\n",
      "Epoch 67, Total Reward: -382.6659, Q1: -7342.8545, Q2: -7313.7832, TargetQ: -7381.6050 alpha: -2.8900 alpha_dot: -194.8850 voltage: -1.70 theta: 1.9635 theta_dot: 0.0002\n",
      "Epoch 68, Total Reward: -840.2034, Q1: -7010.4473, Q2: -6993.4985, TargetQ: -7775.2285 alpha: 2.8103 alpha_dot: 289.3688 voltage: -1.75 theta: 1.9635 theta_dot: 0.0002\n",
      "Epoch 69, Total Reward: -1157.0619, Q1: -7409.4316, Q2: -7404.2568, TargetQ: -7357.7256 alpha: -3.0403 alpha_dot: -339.7353 voltage: -1.71 theta: 1.9635 theta_dot: 0.0002\n",
      "Epoch 70, Total Reward: -502.5904, Q1: -7321.7354, Q2: -7337.4067, TargetQ: -7388.4829 alpha: -2.9575 alpha_dot: -223.5468 voltage: -1.75 theta: 1.9635 theta_dot: 0.0001\n",
      "Epoch 71, Total Reward: -764.4193, Q1: -7322.1934, Q2: -7346.2188, TargetQ: -7399.1660 alpha: 2.8041 alpha_dot: 275.9629 voltage: -1.61 theta: 1.9635 theta_dot: 0.0001\n",
      "Epoch 72, Total Reward: -1172.4388, Q1: -7163.9473, Q2: -7197.1016, TargetQ: -7395.4639 alpha: -2.9575 alpha_dot: -341.9904 voltage: -1.75 theta: 1.9635 theta_dot: 0.0001\n",
      "Epoch 73, Total Reward: -572.6229, Q1: -7044.7734, Q2: -7077.3008, TargetQ: -7441.3975 alpha: -3.0403 alpha_dot: -238.6974 voltage: -1.75 theta: 1.9635 theta_dot: 0.0001\n",
      "Epoch 74, Total Reward: -755.6222, Q1: -6955.7441, Q2: -6976.7065, TargetQ: -7640.7793 alpha: 2.8164 alpha_dot: 274.3654 voltage: -1.75 theta: 1.9635 theta_dot: 0.0001\n",
      "Epoch 75, Total Reward: -1159.4926, Q1: -7029.8428, Q2: -7032.5249, TargetQ: -7726.6343 alpha: -2.8992 alpha_dot: -340.0932 voltage: -1.74 theta: 1.9635 theta_dot: 0.0000\n",
      "Epoch 76, Total Reward: -625.9852, Q1: -7452.7163, Q2: -7443.9146, TargetQ: -7326.0854 alpha: -3.1324 alpha_dot: -249.6232 voltage: -1.75 theta: 1.9635 theta_dot: 0.0000\n",
      "Epoch 77, Total Reward: -777.8521, Q1: -7393.9072, Q2: -7377.7441, TargetQ: -7616.7822 alpha: 2.8563 alpha_dot: 278.3852 voltage: -1.75 theta: 1.9635 theta_dot: 0.0000\n",
      "Epoch 78, Total Reward: -550.7952, Q1: -7621.1895, Q2: -7586.0884, TargetQ: -7494.3154 alpha: -2.8563 alpha_dot: -234.0805 voltage: -1.75 theta: 1.9635 theta_dot: 0.0000\n",
      "Epoch 79, Total Reward: -1463.0298, Q1: -7736.5093, Q2: -7700.3579, TargetQ: -7542.1787 alpha: 3.0618 alpha_dot: 382.1217 voltage: -1.68 theta: 1.9635 theta_dot: 0.0000\n",
      "Epoch 80, Total Reward: -609.8329, Q1: -7527.5615, Q2: -7503.3257, TargetQ: -7762.7734 alpha: 2.9146 alpha_dot: 246.3684 voltage: -1.75 theta: 1.9635 theta_dot: 0.0000\n",
      "Epoch 81, Total Reward: -676.7561, Q1: -7416.9331, Q2: -7398.0005, TargetQ: -7435.0264 alpha: -2.8379 alpha_dot: -259.5954 voltage: -1.75 theta: 1.9635 theta_dot: 0.0000\n",
      "Epoch 82, Total Reward: -1277.1959, Q1: -7501.4990, Q2: -7496.4980, TargetQ: -7421.1885 alpha: 2.9759 alpha_dot: 356.9786 voltage: -1.75 theta: 1.9635 theta_dot: 0.0000\n",
      "\n",
      "Stopping (Ctrl+C). Savingâ€¦\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "history = []\n",
    "policy_delay = 2  # Delayed policy updates\n",
    "step = 0\n",
    "total_reward = 0.0\n",
    "frequency = 500  # Hz\n",
    "state_theta_dot = np.array([0,0], dtype=np.float64)\n",
    "state_alpha_dot = np.array([0,0], dtype=np.float64)\n",
    "timestamps = 0 \n",
    "\n",
    "w_theta = 1.0\n",
    "w_alpha = 0.1\n",
    "w_theta_dot = 0.01\n",
    "w_alpha_dot = 0.01\n",
    "w_u = 0.001\n",
    "w_du = 0.3\n",
    "prev_action = np.zeros(action_size, dtype=np.float32)\n",
    "avg_q = []\n",
    "episode_length = 200   # number of control steps per episode\n",
    "step_in_episode = 0\n",
    "try: \n",
    "    with QubeServo3(hardware = 1, pendulum = 1, frequency=10) as board:\n",
    "        while True:\n",
    "            avg_q1, avg_q2, avg_target_q = 0.0, 0.0, 0.0\n",
    "            step += 1 \n",
    "            board.read_outputs()\n",
    "            theta = board.motorPosition * -1\n",
    "            alpha = board.pendulumPosition \n",
    "            theta = np.clip(theta, (-5*np.pi)/8, (5*np.pi)/8)\n",
    "            alpha = np.mod(alpha, 2*np.pi) - np.pi\n",
    "\n",
    "            theta_dot, state_theta_dot = ddt_filter(theta, state_theta_dot, 50, 1/frequency)\n",
    "            alpha_dot, state_alpha_dot = ddt_filter(alpha, state_alpha_dot, 100, 1/frequency)\n",
    "\n",
    "            state = np.array([theta, theta_dot, alpha, alpha_dot], dtype=np.float32)\n",
    "\n",
    "            action = actor_model(tf.convert_to_tensor([state], dtype=tf.float32)).numpy()[0]\n",
    "            action = action + np.random.normal(0, 0.1, size=action_size)  # Add exploration noise\n",
    "            action = np.clip(action, -1.75, 1.75) \n",
    "            board.write_voltage(action)\n",
    "\n",
    "            board.read_outputs()\n",
    "            next_theta = board.motorPosition * -1\n",
    "            next_alpha = board.pendulumPosition\n",
    "            next_alpha = np.mod(next_alpha, 2*np.pi) - np.pi\n",
    "            next_theta = np.clip(next_theta, (-5*np.pi)/8, (5*np.pi)/8)\n",
    "            next_theta_dot, state_theta_dot = ddt_filter(next_theta, state_theta_dot, 50, 1/frequency)\n",
    "            next_alpha_dot, state_alpha_dot = ddt_filter(next_alpha, state_alpha_dot, 100, 1/frequency)\n",
    "            next_state = np.array([next_theta, next_theta_dot, next_alpha, next_alpha_dot], dtype=np.float32)\n",
    "\n",
    "\n",
    "            # wrapped_alpha = ((alpha - np.pi + np.pi) % (2*np.pi)) - np.pi\n",
    "            # reward = -(alpha**2 + 0.0001*alpha_dot**2 + 0.001*action**2)\n",
    "            if abs(theta) <= (5*np.pi/8) and abs(theta_dot) <= 60:\n",
    "                Fk = 1\n",
    "            else:\n",
    "                Fk = 0\n",
    "\n",
    "            delta_u = action - prev_action\n",
    "            reward = Fk - (\n",
    "                w_theta * (theta**2) +\n",
    "                w_theta_dot * (theta_dot**2) +\n",
    "                w_alpha_dot * (alpha_dot**2) +\n",
    "                w_u * (action**2) +\n",
    "                w_du * (delta_u**2)\n",
    "            )\n",
    "            prev_action = action.copy()\n",
    "            reward = float(reward)\n",
    "            total_reward += reward\n",
    "\n",
    "            replay_buffer.store(state, action, reward, next_state, False)\n",
    "            state = next_state\n",
    "\n",
    "            if replay_buffer.size() >= batch_size:\n",
    "                states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "                states = tf.convert_to_tensor(states, dtype=tf.float32)\n",
    "                actions = tf.convert_to_tensor(actions, dtype=tf.float32)\n",
    "                rewards = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
    "                next_states = tf.convert_to_tensor(next_states, dtype=tf.float32)\n",
    "                dones = tf.convert_to_tensor(dones, dtype=tf.float32) \n",
    "\n",
    "                # add clipped noise to target action\n",
    "                noise = np.clip(np.random.normal(0, 0.2, size=actions.shape), -0.5, 0.5)\n",
    "                next_actions = target_actor(next_states) + noise\n",
    "                next_actions = tf.clip_by_value(next_actions, -1.75, 1.75)  # Pendulum action bounds\n",
    "\n",
    "                # Compute target Q-values with both critics\n",
    "                target1 = tf.squeeze(target_critic1([next_states, next_actions]), axis=1)\n",
    "                target2 = tf.squeeze(target_critic2([next_states, next_actions]), axis=1)\n",
    "                target_q = rewards + gamma * (1 - dones) * tf.minimum(target1, target2)\n",
    "\n",
    "                with tf.GradientTape() as tape_critic1, tf.GradientTape() as tape_critic2:\n",
    "                    q1 = critic_model1([states, actions], training=True)\n",
    "                    q2 = critic_model2([states, actions], training=True)\n",
    "\n",
    "                    # Compute losses\n",
    "                    loss1 = tf.keras.losses.MSE(target_q, q1)\n",
    "                    loss2 = tf.keras.losses.MSE(target_q, q2)\n",
    "\n",
    "                avg_q1 = tf.reduce_mean(q1).numpy().item()\n",
    "                avg_q2 = tf.reduce_mean(q2).numpy().item()\n",
    "                avg_target_q = tf.reduce_mean(target_q).numpy().item()\n",
    "\n",
    "                # Get gradients for each critic once\n",
    "                critic_grad1 = tape_critic1.gradient(loss1, critic_model1.trainable_variables)\n",
    "                critic_grad2 = tape_critic2.gradient(loss2, critic_model2.trainable_variables)\n",
    "\n",
    "                # Apply gradients\n",
    "                critic_optimizer1.apply_gradients(zip(critic_grad1, critic_model1.trainable_variables))\n",
    "                critic_optimizer2.apply_gradients(zip(critic_grad2, critic_model2.trainable_variables))\n",
    "                if step % policy_delay == 0:  # Delayed policy updates\n",
    "                    with tf.GradientTape() as tape_actor: \n",
    "                        action = actor_model(states)\n",
    "                        actor_loss = -tf.reduce_mean(critic_model1([states, action]))\n",
    "\n",
    "                    actor_grad = tape_actor.gradient(actor_loss, actor_model.trainable_variables)\n",
    "                    actor_optimizer.apply_gradients(zip(actor_grad, actor_model.trainable_variables))\n",
    "\n",
    "                    soft_update(target_actor.variables, actor_model.variables, tau=0.005)\n",
    "                    soft_update(target_critic1.variables, critic_model1.variables, tau=0.005)\n",
    "                    soft_update(target_critic2.variables, critic_model2.variables, tau=0.005)\n",
    "\n",
    "            history.append(total_reward)\n",
    "            if step % 1 == 0:\n",
    "                avg_q.append(avg_target_q)\n",
    "                print(f\"Epoch {step}, Total Reward: {float(reward):.4f}, \"\n",
    "                f\"Q1: {avg_q1:.4f}, Q2: {avg_q2:.4f}, TargetQ: {avg_target_q:.4f}\", \n",
    "                f\"alpha: {alpha:.4f}\", f\"alpha_dot: {alpha_dot:.4f}\", \n",
    "                f\"voltage: {np.array(action).reshape(-1)[0]:.2f}\", \n",
    "                f\"theta: {theta:.4f}\", f\"theta_dot: {theta_dot:.4f}\")\n",
    "\n",
    "            if step_in_episode >= episode_length:\n",
    "                print(\"Resetting episode...\")\n",
    "                board.write_voltage(0.0)   # stop motor\n",
    "                time.sleep(5.0)            # wait for pendulum to settle down\n",
    "                step_in_episode = 0        # reset counter\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nStopping (Ctrl+C). Savingâ€¦\")\n",
    "finally:\n",
    "    # save weights (use .save_weights if you prefer checkpoint style)\n",
    "    actor_model.save_weights(\"saves/quanser/actor_model.weights.h5\")\n",
    "    critic_model1.save_weights(\"saves/quanser/critic_model1.weights.h5\")\n",
    "    critic_model2.save_weights(\"saves/quanser/critic_model2.weights.h5\")\n",
    "    ckpt = tf.train.Checkpoint(actor_optimizer=actor_optimizer,\n",
    "                           critic_optimizer1=critic_optimizer1,\n",
    "                           critic_optimizer2=critic_optimizer2)\n",
    "    ckpt.save(\"saves/quanser/optimizers_ckpt/ckpt\")\n",
    "    plt.plot(avg_q, label='Target Q')\n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fd84a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 32\n",
    "# history = []\n",
    "# policy_delay = 2  # Delayed policy updates\n",
    "# step = 0\n",
    "# total_reward = 0.0\n",
    "\n",
    "# try:\n",
    "#     total_reward = 0.0\n",
    "#     while True:\n",
    "#         step += 1\n",
    "    \n",
    "#         # 1) read state\n",
    "#         board.read_encoder(encoder_channels, len(encoder_channels), counts)\n",
    "#         theta_arm  = counts[0] * ARM_RAD_PER_COUNT\n",
    "#         theta_pend = counts[1] * PEND_RAD_PER_COUNT\n",
    "#         theta_arm_dot  = (theta_arm  - theta_arm_prev)  / dt\n",
    "#         theta_pend_dot = (theta_pend - theta_pend_prev) / dt\n",
    "#         state = np.array([theta_arm, theta_pend, theta_arm_dot, theta_pend_dot], dtype=np.float32)\n",
    "\n",
    "#         # 2) select action\n",
    "#         action_vec = actor_model(tf.convert_to_tensor([state], dtype=tf.float32)).numpy()[0]\n",
    "#         action_val = float(np.clip(action_vec[0], -2.0, 2.0))  # scalar in [-2,2]; tune to your safe V range\n",
    "\n",
    "#         # 3) apply action (analog write wants numpy float64 buffer)\n",
    "#         voltages = np.array([action_val], dtype=np.float64)\n",
    "#         board.write_analog(motor_channels, len(motor_channels), voltages)\n",
    "\n",
    "#         # 4) get next_state after action\n",
    "#         time.sleep(dt)  # maintain loop timing around the actuation\n",
    "#         board.read_encoder(encoder_channels, len(encoder_channels), counts)\n",
    "#         next_theta_arm  = counts[0] * ARM_RAD_PER_COUNT\n",
    "#         next_theta_pend = counts[1] * PEND_RAD_PER_COUNT\n",
    "#         next_theta_arm_dot  = (next_theta_arm  - theta_arm)  / dt\n",
    "#         next_theta_pend_dot = (next_theta_pend - theta_pend) / dt\n",
    "#         next_state = np.array([next_theta_arm, next_theta_pend, next_theta_arm_dot, next_theta_pend_dot], dtype=np.float32)\n",
    "\n",
    "#         # 5) reward (example: upright pendulum, gentle motion)\n",
    "#         reward = - ( (np.angle(np.exp(1j*(next_theta_pend - np.pi))))**2\n",
    "#                      + 0.1*next_theta_pend_dot**2 + 0.01*action_val**2 )\n",
    "#         total_reward += reward\n",
    "\n",
    "#         # 6) store\n",
    "#         replay_buffer.store(state, action_val, reward, next_state, False)\n",
    "\n",
    "#         # 7) train if enough samples\n",
    "#         if replay_buffer.size() >= batch_size:\n",
    "#             states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "#             states      = tf.convert_to_tensor(states, dtype=tf.float32)\n",
    "#             actions     = tf.convert_to_tensor(actions.reshape(-1,1), dtype=tf.float32)\n",
    "#             rewards     = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
    "#             next_states = tf.convert_to_tensor(next_states, dtype=tf.float32)\n",
    "#             dones       = tf.convert_to_tensor(dones, dtype=tf.float32)\n",
    "\n",
    "#             # target policy smoothing\n",
    "#             noise = np.clip(np.random.normal(0, 0.2, size=(actions.shape[0], 1)), -0.5, 0.5)\n",
    "#             target_act = tf.clip_by_value(target_actor(next_states) + noise, -2.0, 2.0)\n",
    "\n",
    "#             # twin critics target\n",
    "#             t1 = tf.squeeze(target_critic1([next_states, target_act]), axis=1)\n",
    "#             t2 = tf.squeeze(target_critic2([next_states, target_act]), axis=1)\n",
    "#             target_q = rewards + gamma * (1.0 - dones) * tf.minimum(t1, t2)\n",
    "\n",
    "#             # critic updates\n",
    "#             with tf.GradientTape() as tape1, tf.GradientTape() as tape2:\n",
    "#                 q1 = tf.squeeze(critic_model1([states, actions]), axis=1)\n",
    "#                 q2 = tf.squeeze(critic_model2([states, actions]), axis=1)\n",
    "#                 loss1 = tf.keras.losses.MSE(target_q, q1)\n",
    "#                 loss2 = tf.keras.losses.MSE(target_q, q2)\n",
    "#             critic_optimizer1.apply_gradients(zip(tape1.gradient(loss1, critic_model1.trainable_variables),\n",
    "#                                                   critic_model1.trainable_variables))\n",
    "#             critic_optimizer2.apply_gradients(zip(tape2.gradient(loss2, critic_model2.trainable_variables),\n",
    "#                                                   critic_model2.trainable_variables))\n",
    "\n",
    "#             # delayed actor + target updates\n",
    "#             if step % policy_delay == 0:\n",
    "#                 with tf.GradientTape() as tape_actor:\n",
    "#                     pi = actor_model(states)\n",
    "#                     actor_loss = -tf.reduce_mean(critic_model1([states, pi]))\n",
    "#                 actor_optimizer.apply_gradients(zip(tape_actor.gradient(actor_loss, actor_model.trainable_variables),\n",
    "#                                                     actor_model.trainable_variables))\n",
    "#                 soft_update(target_actor.variables,   actor_model.variables,   tau=0.005)\n",
    "#                 soft_update(target_critic1.variables, critic_model1.variables, tau=0.005)\n",
    "#                 soft_update(target_critic2.variables, critic_model2.variables, tau=0.005)\n",
    "\n",
    "#         if step % 100 == 0:\n",
    "#             print(f\"Step {step}  reward_sum: {total_reward:.2f}\")\n",
    "\n",
    "#         # update prev angles for next derivative\n",
    "#         theta_arm_prev, theta_pend_prev = next_theta_arm, next_theta_pend\n",
    "\n",
    "# except KeyboardInterrupt:\n",
    "#     print(\"\\nStopping (Ctrl+C). Savingâ€¦\")\n",
    "# finally:\n",
    "#     # save weights (use .save_weights if you prefer checkpoint style)\n",
    "#     actor_model.save_weights(\"saves/quanser/actor_model.weights.h5\")\n",
    "#     critic_model1.save_weights(\"saves/quanser/critic_model1.weights.h5\")\n",
    "#     critic_model2.save_weights(\"saves/quanser/critic_model2.weights.h5\")\n",
    "#     # set motor to 0V and close safely\n",
    "#     board.write_analog(motor_channels, 1, np.array([0.0], dtype=np.float64))\n",
    "#     board.close()\n",
    "#     print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff966c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# while True:\n",
    "#     # read state\n",
    "#     # board.close()\n",
    "#     board.read_encoder(encoder_channels, len(encoder_channels), counts)\n",
    "#     theta_arm = counts[0] * ARM_RAD_PER_COUNT\n",
    "#     theta_pend = counts[1] * PEND_RAD_PER_COUNT\n",
    "\n",
    "#     # compute action from policy\n",
    "#     action = actor_model(tf.convert_to_tensor([state], dtype=tf.float32)).numpy()[0]\n",
    "#     u = float(action)\n",
    "\n",
    "#     # send to motor\n",
    "#     board.write_analog(motor_channels, 1, [u])\n",
    "\n",
    "#     time.sleep(dt)  # ~0.01s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e598aa56",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'QubeServo3' object has no attribute 'close'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcollections\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m deque\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# --- HIL/QUBE setup ---\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[43mboard\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m()\n\u001b[0;32m      8\u001b[0m board \u001b[38;5;241m=\u001b[39m HIL(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqube_servo3_usb\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m encoder_channels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39muint32)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'QubeServo3' object has no attribute 'close'"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# import time\n",
    "# import tensorflow as tf\n",
    "# from collections import deque\n",
    "\n",
    "# # --- HIL/QUBE setup ---\n",
    "# board.close()\n",
    "# board = HIL(\"qube_servo3_usb\", \"0\")\n",
    "# encoder_channels = np.array([0, 1], dtype=np.uint32)\n",
    "# motor_channels = np.array([0], dtype=np.uint32)\n",
    "# counts = np.zeros(2, dtype=np.int32)\n",
    "\n",
    "# ENCODER_RES = 2048\n",
    "# ARM_RAD_PER_COUNT = 2*np.pi / ENCODER_RES\n",
    "# PEND_RAD_PER_COUNT = 2*np.pi / ENCODER_RES\n",
    "# dt = 0.01  # 10 ms loop\n",
    "\n",
    "# # --- Replay Buffer ---\n",
    "# class ReplayBuffer:\n",
    "#     def __init__(self, capacity=100000):\n",
    "#         self.buffer = deque(maxlen=capacity)\n",
    "#     def store(self, state, action, reward, next_state, done):\n",
    "#         self.buffer.append((state, action, reward, next_state, done))\n",
    "#     def sample(self, batch_size):\n",
    "#         batch = np.array(random.sample(self.buffer, batch_size))\n",
    "#         states, actions, rewards, next_states, dones = map(np.stack, zip(*batch))\n",
    "#         return states, actions, rewards, next_states, dones\n",
    "#     def size(self):\n",
    "#         return len(self.buffer)\n",
    "\n",
    "# replay_buffer = ReplayBuffer()\n",
    "\n",
    "# # --- Soft update ---\n",
    "# def soft_update(target_weights, online_weights, tau=0.005):\n",
    "#     for (target, online) in zip(target_weights, online_weights):\n",
    "#         target.assign(target * (1 - tau) + online * tau)\n",
    "\n",
    "# # --- TD3 models already defined: actor_model, critic_model1, critic_model2, \n",
    "# # target_actor, target_critic1, target_critic2\n",
    "# # optimizers: actor_optimizer, critic_optimizer1, critic_optimizer2\n",
    "\n",
    "# state_size = 4\n",
    "# action_size = 1\n",
    "# gamma = 0.99\n",
    "# batch_size = 32\n",
    "# policy_delay = 2\n",
    "# step = 0\n",
    "\n",
    "# theta_arm_prev = 0.0\n",
    "# theta_pend_prev = 0.0\n",
    "\n",
    "# try:\n",
    "#     while True:\n",
    "#         step += 1\n",
    "\n",
    "#         # --- 1. Read state ---\n",
    "#         board.read_encoder(encoder_channels, len(encoder_channels), counts)\n",
    "#         theta_arm = counts[0] * ARM_RAD_PER_COUNT\n",
    "#         theta_pend = counts[1] * PEND_RAD_PER_COUNT\n",
    "\n",
    "#         theta_arm_dot = (theta_arm - theta_arm_prev) / dt\n",
    "#         theta_pend_dot = (theta_pend - theta_pend_prev) / dt\n",
    "#         theta_arm_prev, theta_pend_prev = theta_arm, theta_pend\n",
    "\n",
    "#         state = np.array([theta_arm, theta_pend, theta_arm_dot, theta_pend_dot], dtype=np.float32)\n",
    "\n",
    "#         # --- 2. Compute action ---\n",
    "#         action = actor_model(tf.convert_to_tensor([state], dtype=tf.float32)).numpy()[0]\n",
    "#         u_array = np.array([float(action)], dtype=np.float64)\n",
    "\n",
    "#         # --- 3. Apply action ---\n",
    "#         board.write_analog(motor_channels, 1, u_array)\n",
    "\n",
    "#         # --- 4. Read next state ---\n",
    "#         board.read_encoder(encoder_channels, len(encoder_channels), counts)\n",
    "#         next_theta_arm = counts[0] * ARM_RAD_PER_COUNT\n",
    "#         next_theta_pend = counts[1] * PEND_RAD_PER_COUNT\n",
    "#         next_theta_arm_dot = (next_theta_arm - theta_arm) / dt\n",
    "#         next_theta_pend_dot = (next_theta_pend - theta_pend) / dt\n",
    "#         next_state = np.array([next_theta_arm, next_theta_pend, next_theta_arm_dot, next_theta_pend_dot], dtype=np.float32)\n",
    "\n",
    "#         # --- 5. Compute reward ---\n",
    "#         reward = - (next_theta_pend**2 + 0.1 * next_theta_pend_dot**2)\n",
    "\n",
    "#         # --- 6. Store transition ---\n",
    "#         replay_buffer.store(state, action, reward, next_state, False)\n",
    "\n",
    "#         # --- 7. Train TD3 ---\n",
    "#         if replay_buffer.size() >= batch_size:\n",
    "#             states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "#             states = tf.convert_to_tensor(states, dtype=tf.float32)\n",
    "#             actions = tf.convert_to_tensor(actions, dtype=tf.float32)\n",
    "#             rewards = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
    "#             next_states = tf.convert_to_tensor(next_states, dtype=tf.float32)\n",
    "#             dones = tf.convert_to_tensor(dones, dtype=tf.float32)\n",
    "\n",
    "#             # Target actions with clipped noise\n",
    "#             noise = np.clip(np.random.normal(0, 0.2, size=actions.shape), -0.5, 0.5)\n",
    "#             next_actions = tf.clip_by_value(target_actor(next_states) + noise, -12.0, 12.0)\n",
    "\n",
    "#             # Target Q-values\n",
    "#             target1 = tf.squeeze(target_critic1([next_states, next_actions]), axis=1)\n",
    "#             target2 = tf.squeeze(target_critic2([next_states, next_actions]), axis=1)\n",
    "#             target_q = rewards + gamma * (1 - dones) * tf.minimum(target1, target2)\n",
    "\n",
    "#             # Critic updates\n",
    "#             with tf.GradientTape() as tape1, tf.GradientTape() as tape2:\n",
    "#                 q1 = critic_model1([states, actions], training=True)\n",
    "#                 q2 = critic_model2([states, actions], training=True)\n",
    "#                 loss1 = tf.keras.losses.MSE(target_q, q1)\n",
    "#                 loss2 = tf.keras.losses.MSE(target_q, q2)\n",
    "\n",
    "#             critic_grad1 = tape1.gradient(loss1, critic_model1.trainable_variables)\n",
    "#             critic_grad2 = tape2.gradient(loss2, critic_model2.trainable_variables)\n",
    "#             critic_optimizer1.apply_gradients(zip(critic_grad1, critic_model1.trainable_variables))\n",
    "#             critic_optimizer2.apply_gradients(zip(critic_grad2, critic_model2.trainable_variables))\n",
    "\n",
    "#             # Delayed actor update\n",
    "#             if step % policy_delay == 0:\n",
    "#                 with tf.GradientTape() as tape_actor:\n",
    "#                     act = actor_model(states)\n",
    "#                     actor_loss = -tf.reduce_mean(critic_model1([states, act]))\n",
    "#                 actor_grad = tape_actor.gradient(actor_loss, actor_model.trainable_variables)\n",
    "#                 actor_optimizer.apply_gradients(zip(actor_grad, actor_model.trainable_variables))\n",
    "\n",
    "#                 soft_update(target_actor.variables, actor_model.variables)\n",
    "#                 soft_update(target_critic1.variables, critic_model1.variables)\n",
    "#                 soft_update(target_critic2.variables, critic_model2.variables)\n",
    "\n",
    "#         # --- 8. Sleep to maintain loop ---\n",
    "#         time.sleep(dt)\n",
    "\n",
    "# except KeyboardInterrupt:\n",
    "#     print(\"Stopping (Ctrl+C) and saving models...\")\n",
    "\n",
    "# finally:\n",
    "#     # Save models\n",
    "#     actor_model.save(\"td3_actor.h5\")\n",
    "#     critic_model1.save(\"td3_critic1.h5\")\n",
    "#     critic_model2.save(\"td3_critic2.h5\")\n",
    "#     board.close()\n",
    "#     print(\"Training finished and models saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neural_network",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
